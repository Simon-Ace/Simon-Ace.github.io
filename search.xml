<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux扫描磁盘</title>
      <link href="/2021/04/12/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E6%89%AB%E6%8F%8F%E7%A3%81%E7%9B%98/"/>
      <url>/2021/04/12/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E6%89%AB%E6%8F%8F%E7%A3%81%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.jianshu.com/p/c75ce4964062" target="_blank" rel="noopener">https://www.jianshu.com/p/c75ce4964062</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 入门到实战</title>
      <link href="/2021/03/17/Hadoop/Flink%20%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/"/>
      <url>/2021/03/17/Hadoop/Flink%20%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="一、基础概念"><a href="#一、基础概念" class="headerlink" title="一、基础概念"></a>一、基础概念</h2><h2 id="二、Flink-流处理-API"><a href="#二、Flink-流处理-API" class="headerlink" title="二、Flink 流处理 API"></a>二、Flink 流处理 API</h2><h3 id="2-1-Environment"><a href="#2-1-Environment" class="headerlink" title="2.1 Environment"></a>2.1 Environment</h3><p>获取执行环境：本地？集群？</p><h3 id="2-2-Source"><a href="#2-2-Source" class="headerlink" title="2.2 Source"></a>2.2 Source</h3><p>从不同的数据源获取数据</p><ul><li>集合</li><li>文件</li><li>Kafka</li><li>自定义 Source</li></ul><h3 id="2-3-Transform-转换算子"><a href="#2-3-Transform-转换算子" class="headerlink" title="2.3 Transform 转换算子"></a>2.3 Transform 转换算子</h3><h4 id="2-3-1-基本转换算子"><a href="#2-3-1-基本转换算子" class="headerlink" title="2.3.1 基本转换算子"></a>2.3.1 基本转换算子</h4><ul><li>map</li><li>flatMap</li><li>filter</li></ul><h4 id="2-3-2-聚合算子"><a href="#2-3-2-聚合算子" class="headerlink" title="2.3.2 聚合算子"></a>2.3.2 聚合算子</h4><p>DataStream 需要现分组才能做聚合操作</p><p>先 keyBy 得到 KeyedStream，再调用 reduce、sum 方法</p><ul><li>keyBy<ul><li>根据 key 进行分组</li></ul></li><li>Rolling Aggregation<ul><li>sum()、min()、max()、minBy()、maxBy()</li><li>滚动聚合，来一个数据更新一次结果</li></ul></li><li>reduce<ul><li>自定义聚合</li></ul></li></ul><h4 id="2-3-3-多流转换算子"><a href="#2-3-3-多流转换算子" class="headerlink" title="2.3.3 多流转换算子"></a>2.3.3 多流转换算子</h4><ul><li><p>Split 和 Select（已废弃）</p><ul><li>split 将 DataStream 中的数据拆到一个 SplitStream 中的两部分</li><li>Select 将 SplitStream 中的数据，提取成多个 DataStream</li></ul></li><li><p>Connect 和 CoMap</p><ul><li>与 Split、Select 相对，将两条Stream合并为一个Stream</li><li>connect 将两个 stream 合并成 ConnectedStreams 中的两部分</li><li>CoMap 将ConnectedStreams 中的两部分，合并为一个 DataSteam</li></ul></li><li><p>Union</p><ul><li>对两个或者两个以上的DataStream进行Union操作</li><li>与 Connect 区别<ul><li>Connect 的数据类型可以不同，Connect 只能合并两个流</li><li>Union可以合并多条流，Union的数据结构必须是一样的</li></ul></li></ul></li></ul><h4 id="2-3-4-算子转换关系"><a href="#2-3-4-算子转换关系" class="headerlink" title="2.3.4 算子转换关系"></a>2.3.4 算子转换关系</h4><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/watermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FfZHJqaWFvZGE%3D%2Csize_16%2Ccolor_FFFFFF%2Ct_70.png" alt="img"></p><h3 id="2-4-支持的数据类型"><a href="#2-4-支持的数据类型" class="headerlink" title="2.4 支持的数据类型"></a>2.4 支持的数据类型</h3><h3 id="2-5-实现-UDF-函数"><a href="#2-5-实现-UDF-函数" class="headerlink" title="2.5 实现 UDF 函数"></a>2.5 实现 UDF 函数</h3><h3 id="2-6-数据重分区"><a href="#2-6-数据重分区" class="headerlink" title="2.6 数据重分区"></a>2.6 数据重分区</h3><h3 id="2-7-Sink"><a href="#2-7-Sink" class="headerlink" title="2.7 Sink"></a>2.7 Sink</h3><h4 id="2-7-1-Kafka-Sink"><a href="#2-7-1-Kafka-Sink" class="headerlink" title="2.7.1 Kafka Sink"></a>2.7.1 Kafka Sink</h4><ol><li>pom 依赖</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.shuofxz<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>FlinkTutorial<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>Java 代码</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shuofxz.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.shuofxz.beans.SensorReading;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">kafkaSink</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从文件读取数据</span></span><br><span class="line"><span class="comment">//        DataStream&lt;String&gt; inputStream = env.addSource( new FlinkKafkaConsumer011&lt;String&gt;("sensor", new SimpleStringSchema(), properties));</span></span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop102:9092"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从kafka读取数据</span></span><br><span class="line">        DataStream&lt;String&gt; inputStream = env.addSource( <span class="keyword">new</span> FlinkKafkaConsumer011&lt;String&gt;(<span class="string">"sensor"</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换成SensorReading类型</span></span><br><span class="line">        DataStream&lt;String&gt; dataStream = inputStream.map(line -&gt; &#123;</span><br><span class="line">            String[] fields = line.split(<span class="string">","</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> SensorReading(fields[<span class="number">0</span>], <span class="keyword">new</span> Long(fields[<span class="number">1</span>]), <span class="keyword">new</span> Double(fields[<span class="number">2</span>])).toString();</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        dataStream.addSink( <span class="keyword">new</span> FlinkKafkaProducer011&lt;String&gt;(<span class="string">"hadoop102:9092"</span>, <span class="string">"sinktest"</span>, <span class="keyword">new</span> SimpleStringSchema()));</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>启动 zookeeper、kafka 服务</li><li>启动 kafka 生产者 &amp; 消费者</li></ol><p>新建kafka生产者console</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-console-producer.sh --broker-list localhost:9092  --topic sensor</span><br></pre></td></tr></table></figure><p>新建kafka消费者console</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sinktest</span><br></pre></td></tr></table></figure><ol start="5"><li>运行Flink程序，在kafka生产者console输入数据，查看kafka消费者console的输出结果</li></ol><p>输入(kafka生产者console)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">sensor_1,1547718199,35.8</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sensor_6,1547718201,15.4</span></span><br></pre></td></tr></table></figure><p>输出(kafka消费者console)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SensorReading&#123;id='sensor_1', timestamp=1547718199, temperature=35.8&#125;</span><br><span class="line">SensorReading&#123;id='sensor_6', timestamp=1547718201, temperature=15.4&#125;</span><br></pre></td></tr></table></figure><p>这里Flink的作用相当于pipeline了。</p><h4 id="2-7-2-Redis-Sink"><a href="#2-7-2-Redis-Sink" class="headerlink" title="2.7.2 Redis Sink"></a>2.7.2 Redis Sink</h4><h4 id="2-7-3-ES-Sink"><a href="#2-7-3-ES-Sink" class="headerlink" title="2.7.3 ES Sink"></a>2.7.3 ES Sink</h4><h4 id="2-7-4-JDBC-Sink"><a href="#2-7-4-JDBC-Sink" class="headerlink" title="2.7.4 JDBC Sink"></a>2.7.4 JDBC Sink</h4><h2 id="三、Flink-Window"><a href="#三、Flink-Window" class="headerlink" title="三、Flink Window"></a>三、Flink Window</h2><h3 id="3-1-Window-概述"><a href="#3-1-Window-概述" class="headerlink" title="3.1 Window 概述"></a>3.1 Window 概述</h3><p>将无限数据流切割为有限数据块的操作。<strong>Window将一个无限的stream拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作</strong>。</p><h4 id="3-1-1-类型"><a href="#3-1-1-类型" class="headerlink" title="3.1.1 类型"></a>3.1.1 类型</h4><ul><li>时间窗口（Time Window）<ul><li>滚动时间窗口</li><li>滑动时间窗口</li><li>会话窗口</li></ul></li><li>计数窗口（Count Window）<ul><li>滚动计数窗口</li><li>滑动计数窗口</li></ul></li></ul><p>TimeWindow：按照时间生成Window</p><p>CountWindow：按照指定的数据条数生成一个Window，与时间无关</p><h5 id="滚动窗口-Tumbling-Windows"><a href="#滚动窗口-Tumbling-Windows" class="headerlink" title="滚动窗口(Tumbling Windows)"></a>滚动窗口(Tumbling Windows)</h5><ul><li>依据<strong>固定的窗口长度</strong>对数据进行切分</li><li>时间对齐，窗口长度固定，没有重叠</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210319104716644.png" alt="image-20210319104716644" style="zoom:50%;"><h5 id="滑动窗口-Sliding-Windows"><a href="#滑动窗口-Sliding-Windows" class="headerlink" title="滑动窗口(Sliding Windows)"></a>滑动窗口(Sliding Windows)</h5><ul><li>可以按照固定的长度向后滑动固定的距离</li><li>滑动窗口由<strong>固定的窗口长度</strong>和<strong>滑动间隔</strong>组成</li><li>可以有重叠(是否重叠和滑动距离有关系)</li><li>滑动窗口是固定窗口的更广义的一种形式，滚动窗口可以看做是滑动窗口的一种特殊情况（即窗口大小和滑动间隔相等）</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210319104808234.png" alt="image-20210319104808234" style="zoom:50%;"><h5 id="会话窗口-Session-Windows"><a href="#会话窗口-Session-Windows" class="headerlink" title="会话窗口(Session Windows)"></a>会话窗口(Session Windows)</h5><ul><li>由一系列事件组合一个指定时间长度的timeout间隙组成，也就是一段时间没有接收到新数据就会生成新的窗口</li><li>特点：时间无对齐</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210319104829248.png" alt="image-20210319104829248" style="zoom:50%;"><h3 id="3-2-Window-API"><a href="#3-2-Window-API" class="headerlink" title="3.2 Window API"></a>3.2 Window API</h3><h2 id="四、时间语义-amp-Watermark"><a href="#四、时间语义-amp-Watermark" class="headerlink" title="四、时间语义 &amp; Watermark"></a>四、时间语义 &amp; Watermark</h2><h3 id="4-1-Flink中的时间语义"><a href="#4-1-Flink中的时间语义" class="headerlink" title="4.1 Flink中的时间语义"></a>4.1 Flink中的时间语义</h3><ul><li><strong>Event Time：事件创建时间</strong></li><li>Ingestion Time：数据进入Flink的时间</li><li>Processing Time：执行操作算子的本地系统时间，与机器相关</li></ul><p>Event Time是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。</p><h3 id="4-2-Watermark"><a href="#4-2-Watermark" class="headerlink" title="4.2 Watermark"></a>4.2 Watermark</h3><h4 id="4-2-1-概念及作用"><a href="#4-2-1-概念及作用" class="headerlink" title="4.2.1 概念及作用"></a>4.2.1 概念及作用</h4><p><strong>出现原因：</strong>用于处理网络延迟、分布式延迟等造成的数据乱序问题</p><p><strong>工作方式：</strong></p><ul><li>伪装成一个普通数据插入到数据流中，基本只包含时间信息</li><li>Flink 读到 watermark 证明该 watermark 中时间点前的数据已全部到达，可以关闭对应的 bucket 进行处理</li><li>为了能够尽量包容延迟数据，会将 watermark 的时间比实际收到的数据时间慢一些（如2-3s），这样就可以把几秒内的延迟数据包容进来了</li><li>猜测：如果延迟时间过长，超过了 watermark 设置的时间，就会被丢弃</li></ul><h3 id><a href="#" class="headerlink" title></a></h3><h2 id="五、Flink-状态管理"><a href="#五、Flink-状态管理" class="headerlink" title="五、Flink 状态管理"></a>五、Flink 状态管理</h2><p>状态：相当于是之前task留下来的数据，用于和新的数据流数据进行计算用的？</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matplotlib 图例中文乱码</title>
      <link href="/2021/03/05/Python/matplotlib%20%E5%9B%BE%E4%BE%8B%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"/>
      <url>/2021/03/05/Python/matplotlib%20%E5%9B%BE%E4%BE%8B%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>1、下载中文字体（黑体，看准系统版本）</p><p><a href="https://www.fontpalace.com/font-details/SimHei/" target="_blank" rel="noopener">https://www.fontpalace.com/font-details/SimHei/</a></p><p>2、解压之后在系统当中安装好，我的是Mac，打开字体册就可以安装了，Windows的在网上搜一下吧</p><p>3、找到<code>matplotlib</code>字体文件夹，例如：<code>matplotlib/mpl-data/fonts/ttf</code>，将SimHei.ttf拷贝到ttf文件夹下面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行下面的找到文件夹</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(matplotlib.matplotlib_fname())</span><br><span class="line">/Users/xxx/Library/Python/<span class="number">3.8</span>/lib/python/site-packages/matplotlib/mpl-data/matplotlibrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 即前缀就是 /Users/xxx/Library/Python/3.8/lib/python/site-packages/matplotlib</span></span><br></pre></td></tr></table></figure><p>4、修改配置文件<code>matplotlibrc</code> （上一步看到的位置），修改下面三项配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">font.family         : sans-serif        </span><br><span class="line"></span><br><span class="line">font.sans-serif     : SimHei, Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif   </span><br><span class="line"></span><br><span class="line">axes.unicode_minus  : False <span class="comment">#作用就是解决负号'-'显示为方块的问题</span></span><br></pre></td></tr></table></figure><p>5、重新加载字体，在Python中运行如下代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> _rebuild</span><br><span class="line">_rebuild() <span class="comment">#reload一下</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vue 工程创建</title>
      <link href="/2021/03/01/vue/Vue%20%E5%B7%A5%E7%A8%8B%E5%88%9B%E5%BB%BA/"/>
      <url>/2021/03/01/vue/Vue%20%E5%B7%A5%E7%A8%8B%E5%88%9B%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="一、初始化项目"><a href="#一、初始化项目" class="headerlink" title="一、初始化项目"></a>一、初始化项目</h2><h3 id="1-1-Vue-UI-创建"><a href="#1-1-Vue-UI-创建" class="headerlink" title="1.1 Vue UI 创建"></a>1.1 Vue UI 创建</h3><ul><li>命令行输入，会跳出一个浏览器页面：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vue ui</span><br></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210301172457715.png" alt="image-20210301172457715" style="zoom: 25%;"><ul><li>选择一个存放路径</li><li>输入项目名称</li><li>进入「创建新项目」界面<ul><li>如果之前有保存配置，这里可以选择之前的</li><li>没有保存过，就选手动</li></ul></li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210301173027563.png" alt="image-20210301173027563" style="zoom:25%;"><ul><li>进入「功能」界面<ul><li>添加一些常用的功能</li><li>一般会需要：Babel、Router、Vuex、Linter/Formatter、使用配置文件</li></ul></li><li>进入「配置」页面<ul><li>Linter/formatter 选择 standard</li></ul></li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210301173626015.png" alt="image-20210301173626015" style="zoom:25%;"><ul><li><p>最后可以把配置保存</p></li><li><p>生成项目大概需要5-10min</p></li></ul><h2 id="二、安装插件-amp-依赖"><a href="#二、安装插件-amp-依赖" class="headerlink" title="二、安装插件 &amp; 依赖"></a>二、安装插件 &amp; 依赖</h2><h3 id="2-1-插件"><a href="#2-1-插件" class="headerlink" title="2.1 插件"></a>2.1 插件</h3><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210301174646908.png" alt="image-20210301174646908" style="zoom:33%;"><p><strong>推荐安装的插件：</strong></p><ul><li>vue-cli-plugin-element<ul><li>想要生成的包小，就用「import on demand」；如果想开发省事，就「Fully import」</li></ul></li></ul><h3 id="2-2-依赖"><a href="#2-2-依赖" class="headerlink" title="2.2 依赖"></a>2.2 依赖</h3><img src="Vue 工程创建.assets/image-20210301175324844.png" alt="image-20210301175324844" style="zoom: 33%;"><ul><li>运行依赖<ul><li>axios</li></ul></li><li>开发依赖<ul><li>less-loader</li><li>less</li></ul></li></ul><h2 id="三、生产优化"><a href="#三、生产优化" class="headerlink" title="三、生产优化"></a>三、生产优化</h2><ul><li><p>移除 console 输出</p></li><li><p>查看打包报告</p></li></ul><p>能够看到各种环境的占比；</p><p>或者使用命令行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vue-cli-service build --report</span><br></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210310101101647.png" alt="image-20210310101101647" style="zoom: 25%;"><ul><li>分开 prod 和 dev 的配置文件</li><li>使用 CDN 加载</li></ul><p>减小包的大小</p><ul><li>路由懒加载</li></ul><p><a href="https://neveryu.github.io/2019/07/01/vue-element-change-theme/" target="_blank" rel="noopener">基于Vue、ElementUI的换肤解决方案</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注解和反射</title>
      <link href="/2021/02/23/JAVA/%E6%B3%A8%E8%A7%A3%E5%92%8C%E5%8F%8D%E5%B0%84/"/>
      <url>/2021/02/23/JAVA/%E6%B3%A8%E8%A7%A3%E5%92%8C%E5%8F%8D%E5%B0%84/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="一、注解"><a href="#一、注解" class="headerlink" title="一、注解"></a>一、注解</h2><p><strong>学前疑惑：</strong></p><ul><li>干啥用的？</li><li>类似于注释，给编译器看的？看的什么东西呢。加载的时候会有什么不同？</li><li>虽然语法上有点像 python 的装饰器，但是作用似乎不一样</li></ul><h3 id="1-1-什么是注解"><a href="#1-1-什么是注解" class="headerlink" title="1.1 什么是注解"></a>1.1 什么是注解</h3><ul><li>可以对程序作出解释（类似于注释，是给编译器看的）</li><li>可以被其他程序（如编译器等）读取</li></ul><h3 id="1-2-元注解"><a href="#1-2-元注解" class="headerlink" title="1.2 元注解"></a>1.2 元注解</h3><p>用于注解其他注解的注解，即对注解进行一些说明。</p><p>共有4个 —— <code>@Target, @Retention, @Document, @Inherited</code></p><ul><li><code>@Target</code><ul><li>描述注解的使用范围，作用于方法、类等</li></ul></li><li><code>@Retention</code><ul><li>表示注解在什么地方还有效</li><li>SOURCE &lt; CLASS &lt; RUNTIME</li><li>默认用 RUNTIME</li></ul></li><li><code>@Document</code><ul><li>表示是否将注解生成在 javadoc 中</li></ul></li><li><code>@Inherited</code><ul><li>说明子类可以继承父类注解</li></ul></li></ul><h3 id="1-3-自定义注解"><a href="#1-3-自定义注解" class="headerlink" title="1.3 自定义注解"></a>1.3 自定义注解</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Target</span>(value = ElementType.METHOD)</span><br><span class="line"><span class="meta">@interface</span> MyAnnotation &#123;</span><br><span class="line">    <span class="function">String <span class="title">name</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">age</span><span class="params">()</span> <span class="keyword">default</span> 0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用 <code>@interface</code> 声明注解</li><li>其中的每个方法，实际为一个配置参数</li><li>如果只有一个参数成员，一般参数名为 <code>value</code></li><li>默认值用 <code>default</code></li></ul><h2 id="二、反射机制"><a href="#二、反射机制" class="headerlink" title="二、反射机制"></a>二、反射机制</h2><p><strong>学前疑惑：</strong></p><ul><li>什么是反射，有什么作用？</li><li>为什么不直接new出来对象调用呢？</li></ul><h3 id="2-1-基础概念"><a href="#2-1-基础概念" class="headerlink" title="2.1 基础概念"></a>2.1 基础概念</h3><h4 id="2-1-1-相关知识"><a href="#2-1-1-相关知识" class="headerlink" title="2.1.1 相关知识"></a>2.1.1 相关知识</h4><p><strong>静态语言 &amp; 动态语言：</strong></p><ul><li><p>动态语言</p><ul><li>【在运行时检查】</li><li>在运行时可以改变结构的语言，即变量、函数等可以在运行时改变类型（JS、Python 等）</li></ul></li><li><p>静态语言</p><ul><li>【在编译期检查】</li><li>相对应的，不能改变类型，声明了类型就不许改变（Java、C++ 等）</li></ul></li></ul><p><strong>强类型 &amp; 弱类型：</strong></p><ul><li>强类型<ul><li>【不会隐式做语言类型转换】</li></ul></li><li>弱类型<ul><li>【做隐式类型转换】</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/b0aeb7ffd1667b9162e5329154d43777_r.jpg" alt="preview"></p><h4 id="2-1-2-反射概念"><a href="#2-1-2-反射概念" class="headerlink" title="2.1.2 反射概念"></a>2.1.2 反射概念</h4><ul><li>反射机制允许程序在执行期间获取类本身的各种定义，如属性、方法等</li><li>与new对象的方式相反：<ul><li>new 对象：import 包 -&gt; new 对象 -&gt; 取得实例化对象</li><li>反射：实例化对象 -&gt; getCalss() 方法 -&gt; 得到完整的”包类“名称</li><li>由于和new对象的过程相反，所以有「反射」的叫法</li></ul></li></ul><h4 id="2-1-3-反射的作用"><a href="#2-1-3-反射的作用" class="headerlink" title="2.1.3 反射的作用"></a>2.1.3 反射的作用</h4><ul><li>在运行时判断对象所属的类</li><li>在运行时构造对象</li><li>在运行时调用对象的成员变量和方法</li><li>。。。</li></ul><h4 id="2-1-4-反射优点和缺点"><a href="#2-1-4-反射优点和缺点" class="headerlink" title="2.1.4 反射优点和缺点"></a>2.1.4 反射优点和缺点</h4><p><strong>优点：</strong></p><ul><li>动态创建对象和编译</li></ul><p><strong>缺点：</strong></p><ul><li>性能低</li><li>反射类似于解释操作，告诉JVM我需要什么，然后JVM来做，会比我们直接操作要慢</li></ul><h3 id="2-2-反射"><a href="#2-2-反射" class="headerlink" title="2.2 反射"></a>2.2 反射</h3><h4 id="2-2-1-获取反射对象"><a href="#2-2-1-获取反射对象" class="headerlink" title="2.2.1 获取反射对象"></a>2.2.1 获取反射对象</h4><ul><li>一个类在内存中只有一个Class对象</li><li>一个类被加载后，整个类结构都会封装在 Class 对象中</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Class c1 = Class.forName(<span class="string">"com.shuofxz.reflection.User"</span>);</span><br></pre></td></tr></table></figure><h4 id="2-2-2-Class"><a href="#2-2-2-Class" class="headerlink" title="2.2.2 Class"></a>2.2.2 Class</h4><ul><li><p>是什么</p><ul><li>Class 本身也是一个类</li><li>Class 对象只能由系统（JVM）创建</li></ul></li><li><p>特征</p><ul><li>一个类只会在 JVM 中有一个 Class 实例</li><li>每个类的实例都知道自己是由哪个 Class 实例生成</li></ul></li><li><p>用处</p><ul><li>通过 Class 实例可以获得一个类中所有被加载的结构</li><li>通过 Class 可以动态加载运行类（即反射）</li></ul></li><li><p>获取 Class 类的实例</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1) 从类获得</span></span><br><span class="line">Class c1 = Person.class;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) 从实例获得</span></span><br><span class="line">Class c2 = person.getClass();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3) 通过 包名+类名 获得</span></span><br><span class="line">Class c3 = Class.forName(<span class="string">"com.shuofxz.Person"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4) ClassLoader</span></span><br></pre></td></tr></table></figure><ul><li>类加载 与 ClassLoader<ul><li>详细请看 JVM 篇</li><li><ol><li>方法区：从 .class 文件中加载类数据到方法区（每个类数据包括静态变量、静态方法、常量池、代码等）</li><li>堆：以方法区类数据作为模板，生成出类对象；再由「类对象」生成出「对象」</li><li>栈：程序执行，按照代码赋值对象，执行方法</li></ol></li></ul></li></ul><h2 id="三、注解与反射"><a href="#三、注解与反射" class="headerlink" title="三、注解与反射"></a>三、注解与反射</h2><ul><li><p>注解可以通过反射机制起作用</p><ul><li>获取到一个类实例后，通过反射的方式拿到所写的注解信息</li><li>再通过类本身信息 + 注解的信息 完成一些操作<ul><li>如操作数据库的时候，会给类属性对应一个数据库列的名字，就可以用注解来对应</li></ul></li><li>相当于一个语法糖？<ul><li>将一些本该由更多代码实现的对应信息，通过注解简单的方式实现了</li></ul></li></ul></li><li><p>作用</p><ul><li><p>简化代码</p></li><li><p>结偶</p><ul><li>比方原来需要通过继承或者实现方式获得的属性和方法，通过注解方式结偶</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java, 注解, 反射 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IDEA 常用快捷键</title>
      <link href="/2021/02/08/IDEA%20%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
      <url>/2021/02/08/IDEA%20%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
      
        <content type="html"><![CDATA[<ul><li>清除无效 import</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctrl + alt + o</span><br><span class="line">ctrl + option + o</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> idea, 快捷键 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git 使用看这一篇就够了</title>
      <link href="/2021/02/08/Linux/git%20%E4%BD%BF%E7%94%A8%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/"/>
      <url>/2021/02/08/Linux/git%20%E4%BD%BF%E7%94%A8%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<h2 id="一、基础概念"><a href="#一、基础概念" class="headerlink" title="一、基础概念"></a>一、基础概念</h2><p>区域</p><ul><li>工作区</li><li>暂存区</li><li>版本库</li></ul><p>git 对象</p><h2 id="二、基本操作"><a href="#二、基本操作" class="headerlink" title="二、基本操作"></a>二、基本操作</h2><h3 id="2-1-初始化仓库"><a href="#2-1-初始化仓库" class="headerlink" title="2.1 初始化仓库"></a>2.1 初始化仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><h2 id="三、进阶操作"><a href="#三、进阶操作" class="headerlink" title="三、进阶操作"></a>三、进阶操作</h2><ul><li>同步另一个分支的修改</li></ul><p>在 A、B 分支之前处于一个位置，之后A修改了，变成A’，想把B同步到A’的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先切换到 B 分支</span></span><br><span class="line">git checkout B</span><br><span class="line"><span class="comment"># 合并 A1 的修改</span></span><br><span class="line">git merge A</span><br></pre></td></tr></table></figure><ul><li>删除某次 commit</li></ul><h3 id="3-1-git-reset"><a href="#3-1-git-reset" class="headerlink" title="3.1 git reset"></a>3.1 git reset</h3><ul><li><code>git reset</code> ：回滚到某次提交。</li><li><code>git reset --soft</code>：此次提交之后的修改会被退回到暂存区。</li><li><code>git reset --hard</code>：此次提交之后的修改不做任何保留，<code>git status</code> 查看工作区是没有记录的。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> <span class="comment">// 查询要回滚的 commit_id</span></span><br><span class="line">git reset --hard commit_id <span class="comment">// HEAD 就会指向此次的提交记录</span></span><br><span class="line">git push origin HEAD --force <span class="comment">// 强制推送到远端</span></span><br></pre></td></tr></table></figure><h3 id="3-2-rebase"><a href="#3-2-rebase" class="headerlink" title="3.2 rebase"></a>3.2 rebase</h3><ul><li>修改之前的commit信息（只允许在推到远程之前做）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rebase -i &lt;要改的commit 之前的一次commit id&gt;</span><br></pre></td></tr></table></figure><p>【原理】</p><p>在要做变基的节点上，创建出了新的一条支，然后在新的这个支上作修改。</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot 入门</title>
      <link href="/2021/02/05/Spring/SpringBoot%E5%85%A5%E9%97%A8/"/>
      <url>/2021/02/05/Spring/SpringBoot%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="一、基础入门"><a href="#一、基础入门" class="headerlink" title="一、基础入门"></a>一、基础入门</h2><h3 id="1-1-Spring-与-SpringBoot"><a href="#1-1-Spring-与-SpringBoot" class="headerlink" title="1.1 Spring 与 SpringBoot"></a>1.1 Spring 与 SpringBoot</h3><p>SpringBoot 出现原因：整合各种 Spring 框架，简化配置，快速上手开发。【约定大于配置】</p><h3 id="1-2-SpringBoot-第一个程序"><a href="#1-2-SpringBoot-第一个程序" class="headerlink" title="1.2 SpringBoot 第一个程序"></a>1.2 SpringBoot 第一个程序</h3><h4 id="1-2-1-maven-设置"><a href="#1-2-1-maven-设置" class="headerlink" title="1.2.1 maven 设置"></a>1.2.1 maven 设置</h4><p><code>pom.xml</code>中添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.4.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="1-2-2-主类"><a href="#1-2-2-主类" class="headerlink" title="1.2.2 主类"></a>1.2.2 主类</h4><p><code>MainApplication.java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainApplication</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(MainApplication.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>HelloController.java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/hello"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Hello springboot !"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动即可</p><p>完全参考官方文档即可，写的很清楚</p><p><a href="https://docs.spring.io/spring-boot/docs/2.3.4.RELEASE/reference/html/getting-started.html#getting-started-first-application-dependencies" target="_blank" rel="noopener">官方文档</a></p><h3 id="1-3-了解自动配置原理（重点）"><a href="#1-3-了解自动配置原理（重点）" class="headerlink" title="1.3 了解自动配置原理（重点）"></a>1.3 了解自动配置原理（重点）</h3><ul><li>SpringBoot 底层整合了 Sping, SpringMVC等</li></ul><h2 id="二、核心功能"><a href="#二、核心功能" class="headerlink" title="二、核心功能"></a>二、核心功能</h2><h3 id="2-1-配置文件"><a href="#2-1-配置文件" class="headerlink" title="2.1 配置文件"></a>2.1 配置文件</h3><h3 id="2-2-Web-开发"><a href="#2-2-Web-开发" class="headerlink" title="2.2 Web 开发"></a>2.2 Web 开发</h3><h4 id="2-2-1-请求参数处理"><a href="#2-2-1-请求参数处理" class="headerlink" title="2.2.1 请求参数处理"></a>2.2.1 请求参数处理</h4><ul><li>REST 风格<ul><li>GET-查询    DELETE-删除     PUT-修改      POST-保存</li></ul></li></ul><ul><li><strong>请求方法注解</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 原始写法</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/hello"</span>, method = RequestMethod.POST)</span><br><span class="line"><span class="comment">// 简略写法</span></span><br><span class="line"><span class="meta">@PostMapping</span>(<span class="string">"/hello"</span>)</span><br></pre></td></tr></table></figure><h4 id="2-2-2-响应参数处理"><a href="#2-2-2-响应参数处理" class="headerlink" title="2.2.2 响应参数处理"></a>2.2.2 响应参数处理</h4>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto</title>
      <link href="/2021/02/04/Hadoop/Presto/"/>
      <url>/2021/02/04/Hadoop/Presto/</url>
      
        <content type="html"><![CDATA[<h1 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h1><h2 id="一、-简介"><a href="#一、-简介" class="headerlink" title="一、 简介"></a>一、 简介</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h3><ul><li>是什么：即席查询框架</li><li>为什么出现：<ul><li>hive等大数据仓库查数据太慢</li><li>原来的数据库（仓库）无法做到跨源联合查询</li></ul></li><li>presto特征：<ul><li>基于内存<ul><li>数据量支持 GB~PB 查询，查询在几秒到几分钟</li></ul></li><li>可联合多个数据源进行join查询</li><li>只是一个查询引擎，不存储数据</li></ul></li></ul><h3 id="1-2-架构"><a href="#1-2-架构" class="headerlink" title="1.2 架构"></a>1.2 架构</h3><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210204102243401.png" alt="image-20210204102243401"></p><h3 id="1-3-优缺点"><a href="#1-3-优缺点" class="headerlink" title="1.3 优缺点"></a>1.3 优缺点</h3><p><strong>优点：</strong></p><ul><li>基于内存，减少落盘次数，计算快</li><li>能够连接多个数据源，进行跨数据源join</li></ul><p><strong>缺点：</strong></p><ul><li>因为是放在内存计算，当连表查询时，会产生一堆的临时表（落盘？），反而比hive还慢</li></ul>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto, 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>raw.githubusercontent.com无法连接</title>
      <link href="/2021/01/24/raw.githubusercontent.com%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5/"/>
      <url>/2021/01/24/raw.githubusercontent.com%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5/</url>
      
        <content type="html"><![CDATA[<p>由于 DNS 污染导致 raw.githubusercontent.com 无法正常访问，可通过在 hosts 中添加下面一行解决：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">199.232.96.133 raw.githubusercontent.com</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、解决方法"><a href="#一、解决方法" class="headerlink" title="一、解决方法"></a>一、解决方法</h2><p><strong>查询真实IP</strong></p><p>通过<a href="https://www.ipaddress.com/" target="_blank" rel="noopener"><code>IPAddress.com</code></a>首页，输入<code>raw.githubusercontent.com</code>查询到真实IP地址</p><p>如查询到的ip为：<code>199.232.96.133</code></p><p><strong>修改hosts</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hosts</span><br></pre></td></tr></table></figure><p>添加以下内容保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">199.232.96.133 raw.githubusercontent.com</span><br></pre></td></tr></table></figure><h2 id="二、其他代理"><a href="#二、其他代理" class="headerlink" title="二、其他代理"></a>二、其他代理</h2><p>可参考 <a href="https://ghproxy.com/" target="_blank" rel="noopener"><code>https://ghproxy.com</code></a> <a href="https://www.ioiox.com/archives/102.html" target="_blank" rel="noopener">更详细使用方法</a></p><p>主要用于 clone github 的项目</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数仓 V2.0 （03 电商数据仓库系统）</title>
      <link href="/2021/01/24/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0%20%EF%BC%8803%20%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%B3%BB%E7%BB%9F%EF%BC%89/"/>
      <url>/2021/01/24/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0%20%EF%BC%8803%20%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%B3%BB%E7%BB%9F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="电商数仓-V2-0-（03-电商数据仓库系统）"><a href="#电商数仓-V2-0-（03-电商数据仓库系统）" class="headerlink" title="电商数仓 V2.0 （03 电商数据仓库系统）"></a>电商数仓 V2.0 （03 电商数据仓库系统）</h1><h2 id="一、数仓分层"><a href="#一、数仓分层" class="headerlink" title="一、数仓分层"></a>一、数仓分层</h2><h3 id="1-1-为什么要分层"><a href="#1-1-为什么要分层" class="headerlink" title="1.1 为什么要分层"></a>1.1 为什么要分层</h3><ul><li>把复杂问题简单化<ul><li>将复杂的任务分解成多层来完成，每一层只处理简单的任务，方便定位问题</li></ul></li><li>减少重复开发<ul><li>规范数据分层，通过的中间层数据，能够减少极大的重复计算，增加一次计算结果的复用性</li></ul></li><li>隔离原始数据<ul><li>不论是数据的异常还是数据的敏感性，使真实数据与统计数据解耦开</li></ul></li></ul><p><strong>常见分层方式：</strong></p><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210124094731145.png" alt="image-20210124094731145"></p><h3 id="1-2-数据集市与数据仓库"><a href="#1-2-数据集市与数据仓库" class="headerlink" title="1.2 数据集市与数据仓库"></a>1.2 数据集市与数据仓库</h3><p><strong>数据仓库：</strong>企业级别的，包含企业内所有的数据</p><p><strong>数据集市：</strong>可以理解为部门级别的，归属于数据仓库</p><h2 id="二、数仓理论简介"><a href="#二、数仓理论简介" class="headerlink" title="二、数仓理论简介"></a>二、数仓理论简介</h2><h3 id="2-1-范式理论"><a href="#2-1-范式理论" class="headerlink" title="2.1 范式理论"></a>2.1 范式理论</h3><p><strong>定义</strong></p><p>范式可以理解为设计一张数据表的表结构，符合的标准级别，规范和要求</p><p><strong>优点</strong></p><ul><li>减少数据冗余</li><li>保持数据一致性（只改其中一张表就可以了）</li></ul><p><strong>缺点</strong></p><ul><li>难以应对大数据量的计算，因为会频繁涉及表之间的 join 操作</li></ul><h3 id="2-2-关系建模和维度建模"><a href="#2-2-关系建模和维度建模" class="headerlink" title="2.2 关系建模和维度建模"></a>2.2 关系建模和维度建模</h3><p><strong>数据处理主要分类：</strong></p><ul><li>联机事务处理 OLTP（on-line transaction processing）<ul><li>基于事务型的数据增删改查，如银行交易</li></ul></li><li>联机分析处理 OLAP（On-Line Analytical Processing）<ul><li>基于分析型的数据处理，如计算pv uv</li></ul></li></ul><table><thead><tr><th>对比属性</th><th>OLTP（如 MySQL）</th><th>OLAP（如 Hive）</th></tr></thead><tbody><tr><td>读特性</td><td>每次查询只返回少量记录</td><td>对大量记录进行汇总</td></tr><tr><td>写特性</td><td>随机、低延时写入用户的输入</td><td>批量导入</td></tr><tr><td>使用场景</td><td>用户，Java EE项目</td><td>内部分析师，为决策提供支持</td></tr><tr><td>数据表征</td><td>最新数据状态</td><td>随时间变化的历史状态</td></tr><tr><td>数据规模</td><td>GB</td><td>TB到PB</td></tr></tbody></table><h4 id="2-2-1-关系建模"><a href="#2-2-1-关系建模" class="headerlink" title="2.2.1 关系建模"></a>2.2.1 关系建模</h4><ul><li>基本严格遵循三范式</li><li>表较为零碎</li><li>数据冗余低</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210124113305858.png" alt="image-20210124113305858" style="zoom:67%;"><h4 id="2-2-2-维度建模"><a href="#2-2-2-维度建模" class="headerlink" title="2.2.2 维度建模"></a>2.2.2 维度建模</h4><ul><li>主要应用于OLAP系统中</li><li>通常以某一个事实表为中心进行表的组织，主要面向业务</li><li>存在数据的冗余，但是能方便的得到数据。</li><li>在大规模数据，跨表分析统计查询过程中，不需要多表关联，提升效率</li><li>把相关各种表整理成两种：事实表和维度表</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210124113451765.png" alt="image-20210124113451765" style="zoom:50%;"><p><strong>分类</strong></p><ul><li>星型模型<ul><li>在一个事实表周围只会围绕一圈维度表</li><li>冗余可能会多一点，但是减少了 join 能提升计算效率</li><li><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210124114249379.png" alt="image-20210124114249379" style="zoom:33%;"></li></ul></li><li>雪花模型<ul><li>相对于星型模型，事实表周围会有多层维度表</li><li>会更灵活一些</li><li><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210124114305334.png" alt="image-20210124114305334" style="zoom:33%;"></li></ul></li><li>星座模型<ul><li>在前两种上的扩展，如果多于一个事实表，就是星座模型</li><li><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210124114351218.png" alt="image-20210124114351218" style="zoom:33%;"></li></ul></li></ul><h3 id="2-3-维度表和事实表"><a href="#2-3-维度表和事实表" class="headerlink" title="2.3 维度表和事实表"></a>2.3 维度表和事实表</h3><h4 id="2-3-1-维度表"><a href="#2-3-1-维度表" class="headerlink" title="2.3.1 维度表"></a>2.3.1 维度表</h4><p><strong>定义：</strong></p><p>一般是对事实的<strong><em>描述信息</em></strong>。每一张维表对应现实世界中的一个对象或者概念。   例如：用户、商品、日期、地区等</p><p><strong>特征：</strong></p><ul><li><p>维表的范围很宽（具有多个属性、列比较多）</p></li><li><p>跟事实表相比，行数相对较小：通常&lt; 10万条</p></li><li><p>内容相对固定：编码表</p></li></ul><h4 id="2-3-2-事实表"><a href="#2-3-2-事实表" class="headerlink" title="2.3.2 事实表"></a>2.3.2 事实表</h4><p><strong>定义：</strong></p><p>描述一个业务过程，如下单、支付、退款、评价等。通常包含着可统计的列，如金额、数量等。</p><p><strong>特征：</strong></p><ul><li><p>非常的大</p></li><li><p>内容相对的窄：列数较少</p></li><li><p>经常发生变化，每天会新增加很多</p></li></ul><p><strong>事实表分类：</strong></p><p><strong>1）事务型事实表</strong></p><p>以<em>每个事务或事件为单位</em>，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。一旦事务被提交，事实表数据被插入，数据就不再进行更改，其更新方式为增量更新。  </p><p><strong>2）周期型快照事实表</strong></p><p>周期型快照事实表中<em>不会保留所有数据，只保留固定时间间隔的数据</em>，例如每天或者每月的销售额。对中间过程不敏感，只关心到一个时间点时的状态。</p><p><strong>3）累积型快照事实表</strong></p><p><em>累计快照事实表用于跟踪业务事实的变化。</em>例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。</p><h3 id="2-4-数据仓库建模"><a href="#2-4-数据仓库建模" class="headerlink" title="2.4 数据仓库建模"></a>2.4 数据仓库建模</h3><h4 id="2-4-1-ODS层"><a href="#2-4-1-ODS层" class="headerlink" title="2.4.1 ODS层"></a>2.4.1 ODS层</h4><ul><li>保存着最原始的数据，起到数据备份的作用</li><li>采用数据压缩，减少磁盘占用（压缩到 1/10 ~ 1/20）</li><li>创建分区表，避免之后全表扫描</li></ul><p><strong>原始表之间关系：</strong></p><ul><li>绿色：维度表</li><li>深色：事实表</li><li>白色：会进行维度退化的表</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210128111435826.png" alt="image-20210128111435826"><h4 id="2-4-2-DWD层"><a href="#2-4-2-DWD层" class="headerlink" title="2.4.2 DWD层"></a>2.4.2 DWD层</h4><p>1、在这里使用<strong>维度建模</strong>，一般采用星型模型，展现出来为星座模型</p><p>2、<strong>维度建模大致分为以下四个步骤：</strong></p><p>选择业务过程 -&gt; 声明粒度 -&gt; 确认维度 -&gt; 确认事实</p><ul><li><p>选择业务过程</p><ul><li>挑选感兴趣的业务线，一条业务线对应一张事实表</li></ul></li><li><p>声明最小粒度</p><ul><li>无论统计啥都能从这个粒度进行计算</li></ul></li><li><p>确认维度</p><ul><li>谁、何处、何时</li><li>维度表：会根据需求进行维度退化</li></ul></li><li><p>确认事实</p><ul><li>即确定「度量值」，个数、次数、件数等信息</li></ul></li></ul><p>3、维度建模到这里已完毕</p><p>4、DWD 层以业务过程为驱动，后面DWS、DWT则以需求为驱动</p><table><thead><tr><th></th><th>时间</th><th>用户</th><th>地区</th><th>商品</th><th>优惠券</th><th>活动</th><th>编码</th><th>度量值</th></tr></thead><tbody><tr><td>订单</td><td>√</td><td>√</td><td>√</td><td></td><td></td><td>√</td><td></td><td>件数/金额</td></tr><tr><td>订单详情</td><td>√</td><td>√</td><td>√</td><td>√</td><td></td><td></td><td></td><td>件数/金额</td></tr><tr><td>支付</td><td>√</td><td>√</td><td>√</td><td></td><td></td><td></td><td></td><td>金额</td></tr><tr><td>加购</td><td>√</td><td>√</td><td></td><td>√</td><td></td><td></td><td></td><td>件数/金额</td></tr><tr><td>收藏</td><td>√</td><td>√</td><td></td><td>√</td><td></td><td></td><td></td><td>个数</td></tr><tr><td>评价</td><td>√</td><td>√</td><td></td><td>√</td><td></td><td></td><td></td><td>个数</td></tr><tr><td>退款</td><td>√</td><td>√</td><td></td><td>√</td><td></td><td></td><td></td><td>件数/金额</td></tr><tr><td>优惠券领用</td><td>√</td><td>√</td><td></td><td></td><td>√</td><td></td><td></td><td>个数</td></tr></tbody></table><h4 id="2-4-3-DWS层"><a href="#2-4-3-DWS层" class="headerlink" title="2.4.3 DWS层"></a>2.4.3 DWS层</h4><p><strong>主要作用：</strong>将维度表和关联的事实表进行组合（join），每个维度表生成一个大宽表，避免后续重复join计算</p><p><strong>粒度：</strong>统计各主题对象当天的行为</p><p><strong>每个表包含哪些字段：</strong>以维度表为基准，关联相关事实表的度量值（如下图）</p><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210128113346588.png" alt="image-20210128113346588"></p><p>以用户表为例：</p><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210128113813789.png" alt="image-20210128113813789"></p><h4 id="2-4-4-DWT层"><a href="#2-4-4-DWT层" class="headerlink" title="2.4.4 DWT层"></a>2.4.4 DWT层</h4><p><strong>作用：</strong>统计事情从第一次发生至今的累积度量值</p><p>例：</p><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210128114038540.png" alt="image-20210128114038540"></p><h4 id="2-4-5-ADS层"><a href="#2-4-5-ADS层" class="headerlink" title="2.4.5 ADS层"></a>2.4.5 ADS层</h4><p>根据实际业务需要在前面几张表中进行统计，如 pv，uv，新增，留存等。</p><h2 id="三、ODS-层搭建"><a href="#三、ODS-层搭建" class="headerlink" title="三、ODS 层搭建"></a>三、ODS 层搭建</h2><ol><li>保持数据原貌不做任何修改，起到备份数据的作用（即直接以一串string的原始格式存进来，不作任何解析）</li><li>数据采用LZO压缩，减少磁盘存储空间。100G数据可以压缩到10G以内。</li><li>创建分区表，防止后续的全表扫描，在企业开发中大量使用分区表。</li><li>创建外部表。在企业开发中，除了自己用的临时表，创建内部表外，绝大多数场景都是创建外部表。</li></ol><h3 id="3-1-创建数据库"><a href="#3-1-创建数据库" class="headerlink" title="3.1 创建数据库"></a>3.1 创建数据库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; create database gmall;</span><br><span class="line">&gt; use gmall;</span><br></pre></td></tr></table></figure><h3 id="3-2-用户行为数据"><a href="#3-2-用户行为数据" class="headerlink" title="3.2 用户行为数据"></a>3.2 用户行为数据</h3><h4 id="3-2-1-启动日志表"><a href="#3-2-1-启动日志表" class="headerlink" title="3.2.1 启动日志表"></a>3.2.1 启动日志表</h4><ul><li>创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USE</span> gmall;</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> ods_start_log;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> ods_start_log (<span class="string">`line`</span> <span class="keyword">string</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="string">`dt`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span></span><br><span class="line">  INPUTFORMAT <span class="string">'com.hadoop.mapred.DeprecatedLzoTextInputFormat'</span></span><br><span class="line">  OUTPUTFORMAT <span class="string">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span></span><br><span class="line">LOCATION <span class="string">'/warehouse/gmall/ods/ods_start_log'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载数据</li></ul><p><code>load</code> 操作是剪切操作，相当于从 hdfs 的一个路径，移动到另一个路径</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/origin_data/gmall/log/topic_start/2020-03-10'</span> <span class="keyword">into</span> <span class="keyword">table</span> gmall.ods_start_log <span class="keyword">partition</span>(dt=<span class="string">'2020-03-10'</span>);</span><br><span class="line"><span class="comment">-- 注意：时间格式都配置成YYYY-MM-DD格式，这是Hive默认支持的时间格式</span></span><br></pre></td></tr></table></figure><ul><li>查看数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ods_start_log <span class="keyword">where</span> dt=<span class="string">'2020-03-10'</span> <span class="keyword">limit</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure><ul><li>创建 lzo 索引</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=2020-03-10</span><br></pre></td></tr></table></figure><h4 id="3-2-2-事件日志表"><a href="#3-2-2-事件日志表" class="headerlink" title="3.2.2 事件日志表"></a>3.2.2 事件日志表</h4><ul><li>创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">USE</span> gmall;</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> ods_event_log;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> ods_event_log(<span class="string">`line`</span> <span class="keyword">string</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="string">`dt`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span></span><br><span class="line">  INPUTFORMAT <span class="string">'com.hadoop.mapred.DeprecatedLzoTextInputFormat'</span></span><br><span class="line">  OUTPUTFORMAT <span class="string">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span></span><br><span class="line">LOCATION <span class="string">'/warehouse/gmall/ods/ods_event_log'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/origin_data/gmall/log/topic_event/2020-03-10'</span> <span class="keyword">into</span> <span class="keyword">table</span> gmall.ods_event_log <span class="keyword">partition</span>(dt=<span class="string">'2020-03-10'</span>);</span><br></pre></td></tr></table></figure><ul><li>查看数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ods_event_log <span class="keyword">where</span> dt=<span class="string">"2020-03-10"</span> <span class="keyword">limit</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure><ul><li>创建索引</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2020-03-10</span><br></pre></td></tr></table></figure><h4 id="3-2-3-数据导入脚本"><a href="#3-2-3-数据导入脚本" class="headerlink" title="3.2.3 数据导入脚本"></a>3.2.3 数据导入脚本</h4><p><code>hdfs_to_ods_log.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量方便修改</span></span><br><span class="line">APP=gmall</span><br><span class="line">HIVE=/opt/module/hive-2.3.0-bin/bin/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$1</span>"</span> ] ;<span class="keyword">then</span></span><br><span class="line">   do_date=<span class="variable">$1</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">   do_date=`date -d <span class="string">"-1 day"</span> +%F`</span><br><span class="line"><span class="keyword">fi</span> </span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"=== 日志日期为 <span class="variable">$do_date</span> ==="</span></span><br><span class="line">sql=<span class="string">"</span></span><br><span class="line"><span class="string">load data inpath '/origin_data/gmall/log/topic_start/<span class="variable">$do_date</span>' overwrite into table <span class="variable">$&#123;APP&#125;</span>.ods_start_log partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/gmall/log/topic_event/<span class="variable">$do_date</span>' overwrite into table <span class="variable">$&#123;APP&#125;</span>.ods_event_log partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$HIVE</span> -e <span class="string">"<span class="variable">$sql</span>"</span></span><br><span class="line"></span><br><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=<span class="variable">$do_date</span></span><br><span class="line"></span><br><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=<span class="variable">$do_date</span></span><br></pre></td></tr></table></figure><h3 id="3-3-业务数据"><a href="#3-3-业务数据" class="headerlink" title="3.3 业务数据"></a>3.3 业务数据</h3><p>从 MySQL 导入 HDFS 的数据</p><h4 id="3-3-1-创建表"><a href="#3-3-1-创建表" class="headerlink" title="3.3.1 创建表"></a>3.3.1 创建表</h4><p>一堆表，以 <strong>订单表</strong> 为例</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 增量及更新</span></span><br><span class="line">hive (gmall)&gt;</span><br><span class="line"><span class="keyword">use</span> gmall;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ods_order_info;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> ods_order_info (</span><br><span class="line">    <span class="string">`id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'订单号'</span>,</span><br><span class="line">    <span class="string">`final_total_amount`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">COMMENT</span> <span class="string">'订单金额'</span>,</span><br><span class="line">    <span class="string">`order_status`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'订单状态'</span>,</span><br><span class="line">    <span class="string">`user_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户id'</span>,</span><br><span class="line">    <span class="string">`out_trade_no`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'支付流水号'</span>,</span><br><span class="line">    <span class="string">`create_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line">    <span class="string">`operate_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'操作时间'</span>,</span><br><span class="line">    <span class="string">`province_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省份ID'</span>,</span><br><span class="line">    <span class="string">`benefit_reduce_amount`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">COMMENT</span> <span class="string">'优惠金额'</span>,</span><br><span class="line">    <span class="string">`original_total_amount`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>)  <span class="keyword">COMMENT</span> <span class="string">'原价金额'</span>,</span><br><span class="line">    <span class="string">`feight_fee`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>)  <span class="keyword">COMMENT</span> <span class="string">'运费'</span></span><br><span class="line">) <span class="keyword">COMMENT</span> <span class="string">'订单表'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="string">`dt`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span></span><br><span class="line">  INPUTFORMAT <span class="string">'com.hadoop.mapred.DeprecatedLzoTextInputFormat'</span></span><br><span class="line">  OUTPUTFORMAT <span class="string">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ods/ods_order_info/'</span>;</span><br></pre></td></tr></table></figure><p>其他表的创建略。</p><h4 id="3-3-2-数据导入脚本"><a href="#3-3-2-数据导入脚本" class="headerlink" title="3.3.2 数据导入脚本"></a>3.3.2 数据导入脚本</h4><p><code>hdfs_to_ods_db.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">APP=gmall</span><br><span class="line">HIVE=/opt/module/hive-2.3.0-bin/bin/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$2</span>"</span> ] ;<span class="keyword">then</span></span><br><span class="line">    do_date=<span class="variable">$2</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">    do_date=`date -d <span class="string">"-1 day"</span> +%F`</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sql1=<span class="string">" </span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/order_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_order_info partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/order_detail/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_order_detail partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/sku_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_sku_info partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/user_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_user_info partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/payment_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_payment_info partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_category1/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_category1 partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_category2/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_category2 partition(dt='<span class="variable">$do_date</span>');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_category3/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_category3 partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_trademark/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_trademark partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/activity_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_activity_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/activity_order/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_activity_order partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/cart_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_cart_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/comment_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_comment_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/coupon_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_coupon_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/coupon_use/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_coupon_use partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/favor_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_favor_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/order_refund_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_order_refund_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/order_status_log/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_order_status_log partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/spu_info/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_spu_info partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/activity_rule/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_activity_rule partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_dic/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_dic partition(dt='<span class="variable">$do_date</span>'); </span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line">sql2=<span class="string">" </span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_province/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_province;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">load data inpath '/origin_data/<span class="variable">$APP</span>/db/base_region/<span class="variable">$do_date</span>' OVERWRITE into table <span class="variable">$&#123;APP&#125;</span>.ods_base_region;</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"first"</span>)&#123;</span><br><span class="line">    <span class="variable">$HIVE</span> -e <span class="string">"<span class="variable">$sql1</span>"</span></span><br><span class="line">    <span class="variable">$HIVE</span> -e <span class="string">"<span class="variable">$sql2</span>"</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"all"</span>)&#123;</span><br><span class="line">    <span class="variable">$HIVE</span> -e <span class="string">"<span class="variable">$sql1</span>"</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs_to_ods_db.sh first 2020-03-10</span><br><span class="line">hdfs_to_ods_db.sh all 2020-03-11</span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ods_order_detail <span class="keyword">where</span> dt=<span class="string">'2020-03-11'</span>;</span><br></pre></td></tr></table></figure><h2 id="四、DWD-层搭建"><a href="#四、DWD-层搭建" class="headerlink" title="四、DWD 层搭建"></a>四、DWD 层搭建</h2><h3 id="4-1-用户行为数据"><a href="#4-1-用户行为数据" class="headerlink" title="4.1 用户行为数据"></a>4.1 用户行为数据</h3><ul><li><p>将 JSON 数据解析为标准的列数据</p></li><li><p>创建 UDF 函数（解析json列）</p></li><li><p>创建 UDTF 函数（一进多出，将一行数据拆分成多行）</p></li></ul><h3 id="4-2-业务数据"><a href="#4-2-业务数据" class="headerlink" title="4.2 业务数据"></a>4.2 业务数据</h3><ul><li>进行维度建模</li><li>join多张表，形成一个大宽表</li><li>拉链表<ul><li>用于处理缓慢变化的数据</li><li>如果每日全量记录，太浪费</li><li>拉链表，有变化的行才会记录</li><li>两个特殊的列  start_time, end_time 分别记录这条的生效和结束时间，有变化时就会产生一个新的记录行</li></ul></li></ul><h2 id="五、DWS-层搭建"><a href="#五、DWS-层搭建" class="headerlink" title="五、DWS 层搭建"></a>五、DWS 层搭建</h2>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS6 yum 404</title>
      <link href="/2021/01/21/Linux/CentOS6%20yum%20404/"/>
      <url>/2021/01/21/Linux/CentOS6%20yum%20404/</url>
      
        <content type="html"><![CDATA[<h2 id="一、问题出现原因"><a href="#一、问题出现原因" class="headerlink" title="一、问题出现原因"></a>一、问题出现原因</h2><p>突然发现 yum不可用了，错误信息如下：</p><blockquote><p>Determining fastest mirrors<br>YumRepo Error: All mirror URLs are not using ftp, http[s] or file.<br>Eg. Invalid release/repo/arch combination/<br>removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt<br>错误：Cannot find a valid baseurl for repo: base</p></blockquote><p>前面怀疑是服务器的网络问题，经排查网络无异常。拿yum源中的地址确认问题，发现404了，地址已经发发生了改变，原因是CentOS 6已经随着2020年11月的结束进入了EOL（Reaches End of Life），官方便在12月2日正式将CentOS 6相关的软件源移出了官方源，随之而来逐级镜像也会陆续将其删除。</p><h2 id="二、解决问题"><a href="#二、解决问题" class="headerlink" title="二、解决问题"></a>二、解决问题</h2><p><strong>备份文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># by root</span></span><br><span class="line">cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak</span><br></pre></td></tr></table></figure><p><strong>替换CentOS-Base.repo文件内容</strong></p><p>下面提供了官方Vault源和阿里云Vault镜像，选择其一即可，国内建议使用阿里云Vault镜像，速度会更快。<br><code>vi /etc/yum.repos.d/CentOS-Base.repo</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">#阿里云Vault镜像，本例使用的6.10版本，注意修改为你当前的操作系统版本号</span><br><span class="line">[base]</span><br><span class="line">name=CentOS-6.10 - Base - mirrors.aliyun.com</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/os/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#released updates </span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-6.10 - Updates - mirrors.aliyun.com</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/updates/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#additional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-6.10 - Extras - mirrors.aliyun.com</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/extras/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#additional packages that extend functionality of existing packages</span><br><span class="line">[centosplus]</span><br><span class="line">name=CentOS-6.10 - Plus - mirrors.aliyun.com</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/centosplus/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#contrib - packages by Centos Users</span><br><span class="line">[contrib]</span><br><span class="line">name=CentOS-6.10 - Contrib - mirrors.aliyun.com</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://mirrors.aliyun.com/centos-vault/6.10/contrib/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--官方Vault源，本例使用的6.10版本，注意修改为你当前的操作系统版本号</span><br><span class="line">[base]</span><br><span class="line">name=CentOS-6.10 - Base - vault.centos.org</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://vault.centos.org/6.10/os/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#released updates </span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-6.10 - Updates - vault.centos.org</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://vault.centos.org/6.10/updates/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#additional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-6.10 - Extras - vault.centos.org</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://vault.centos.org/6.10/extras/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#additional packages that extend functionality of existing packages</span><br><span class="line">[centosplus]</span><br><span class="line">name=CentOS-6.10 - Plus - vault.centos.org</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://vault.centos.org/6.10/centosplus/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=0</span><br><span class="line">gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6</span><br><span class="line"> </span><br><span class="line">#contrib - packages by Centos Users</span><br><span class="line">[contrib]</span><br><span class="line">name=CentOS-6.10 - Contrib - vault.centos.org</span><br><span class="line">failovermethod=priority</span><br><span class="line">baseurl=http://vault.centos.org/6.10/contrib/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=0</span><br><span class="line">gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6</span><br></pre></td></tr></table></figure><p><strong>清除YUM缓存</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br></pre></td></tr></table></figure><p><strong>重新构建缓存</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> yum </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数仓 V2.0 （02 业务数据采集模块）</title>
      <link href="/2021/01/19/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0%20%EF%BC%8802%20%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A8%A1%E5%9D%97%EF%BC%89/"/>
      <url>/2021/01/19/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0%20%EF%BC%8802%20%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A8%A1%E5%9D%97%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="电商数仓-V2-0-（02-业务数据采集模块）"><a href="#电商数仓-V2-0-（02-业务数据采集模块）" class="headerlink" title="电商数仓 V2.0 （02 业务数据采集模块）"></a>电商数仓 V2.0 （02 业务数据采集模块）</h1><h2 id="一、电商业务简介"><a href="#一、电商业务简介" class="headerlink" title="一、电商业务简介"></a>一、电商业务简介</h2><h2 id="二、业务数据采集模块"><a href="#二、业务数据采集模块" class="headerlink" title="二、业务数据采集模块"></a>二、业务数据采集模块</h2><h3 id="2-1-MySQL"><a href="#2-1-MySQL" class="headerlink" title="2.1 MySQL"></a>2.1 MySQL</h3><p>装一台机器就行 hadoop102</p><h3 id="2-2-Sqoop"><a href="#2-2-Sqoop" class="headerlink" title="2.2 Sqoop"></a>2.2 Sqoop</h3><ul><li>修改配置<code>sqoop-env.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前用于将数据从 MySQL 导入 HDFS 配置下面这俩就可以</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入 Hive 要配置 Hive_HOME；导入 HBase 要配置 HBASE_HOME 和 ZOOCFGDIR</span></span><br><span class="line"><span class="comment">#export HBASE_HOME=</span></span><br><span class="line"><span class="comment">#export HIVE_HOME=</span></span><br><span class="line"><span class="comment">#export ZOOCFGDIR=</span></span><br></pre></td></tr></table></figure><ul><li>拷贝 mysql jar 包</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-5.1.27-bin.jar <span class="variable">$SQOOP_HOME</span>/lib</span><br></pre></td></tr></table></figure><ul><li>验证安装情况</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ bin/sqoop <span class="built_in">help</span></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  <span class="built_in">eval</span>               Evaluate a SQL statement and display the results</span><br><span class="line">  <span class="built_in">export</span>             Export an HDFS directory to a database table</span><br><span class="line">  <span class="built_in">help</span>               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved <span class="built_in">jobs</span></span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables <span class="keyword">in</span> a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See <span class="string">'sqoop help COMMAND'</span> <span class="keyword">for</span> information on a specific <span class="built_in">command</span>.</span><br></pre></td></tr></table></figure><ul><li>测试连接 mysql</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 123456</span><br><span class="line"><span class="comment"># 能出现表名就是正确的了</span></span><br></pre></td></tr></table></figure><h3 id="2-3-业务数据生成"><a href="#2-3-业务数据生成" class="headerlink" title="2.3 业务数据生成"></a>2.3 业务数据生成</h3><ul><li>通过建表语句导入数据</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmall2020-03-16.sql</span><br></pre></td></tr></table></figure><ul><li>java 生成业务数据</li></ul><p>拷贝 <code>application.properties</code> 和 <code>gmall-mock-db-2020-03-16-SNAPSHOT.jar</code></p><p>修改 <code>application.properties</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先生成 2020-03-10 clear=1（清空之前sql语句生成的数据），再生成 03-11 clear=0</span></span><br><span class="line"><span class="comment"># 业务日期</span></span><br><span class="line">mock.date=2020-03-11</span><br><span class="line"><span class="comment"># 是否重置</span></span><br><span class="line">mock.clear=1</span><br></pre></td></tr></table></figure><p>生成数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h3 id="2-4-同步策略"><a href="#2-4-同步策略" class="headerlink" title="2.4 同步策略"></a>2.4 同步策略</h3><p>数据同步策略的类型包括：<strong>全量表、增量表、新增及变化表</strong></p><ul><li><p>全量表：存储完整的数据。</p></li><li><p>增量表：存储新增加的数据。</p></li><li><p>新增及变化表：存储新增加的数据和变化的数据。</p></li><li><p>特殊表：只需要存储一次。</p></li></ul><h4 id="2-4-1-全量同步策略"><a href="#2-4-1-全量同步策略" class="headerlink" title="2.4.1 全量同步策略"></a>2.4.1 全量同步策略</h4><p>每日存入一份完整数据，作为一个分区</p><p>适用场景：表数据量不大（如小于10万），每天既有新增，也有修改的场景</p><p>例子：品牌表、优惠规则、活动、商品分类等</p><h4 id="2-4-2-增量同步策略"><a href="#2-4-2-增量同步策略" class="headerlink" title="2.4.2 增量同步策略"></a>2.4.2 增量同步策略</h4><p>每天存储一份增量数据作为一个分区</p><p>适用场景：表数据量大（十万、百万以上），且每天只有新增数据，不会修改原来的数据</p><p>例子：退单表、订单状态表、商品评论表等</p><h4 id="2-4-3-新增及变化策略"><a href="#2-4-3-新增及变化策略" class="headerlink" title="2.4.3 新增及变化策略"></a>2.4.3 新增及变化策略</h4><p>每日新增及变化，就是存储创建时间和操作时间都是今天的数据</p><p>适用场景：表的数据量大，既会有新增，又会有变化</p><p>例子：用户表、订单表、优惠卷领用表</p><h4 id="2-4-4-特殊策略"><a href="#2-4-4-特殊策略" class="headerlink" title="2.4.4 特殊策略"></a>2.4.4 特殊策略</h4><p>某些特殊的维度表，可不必遵循上述同步策略，如一共只存储一次。</p><p>1）客观世界维度</p><p>没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。</p><p>2）日期维度</p><p>日期维度可以一次性导入一年或若干年的数据。</p><p>3）地区维度</p><p>省份表、地区表</p><h4 id="2-5-业务数据导入-HDFS"><a href="#2-5-业务数据导入-HDFS" class="headerlink" title="2.5 业务数据导入 HDFS"></a>2.5 业务数据导入 HDFS</h4><ul><li>使用 sqoop 工具</li><li>时间处理：T + 1模式，即后一天导入前一天的数据</li><li>导表类型：<ul><li>全量：<code>select * from table where 1=1</code>（1=1 是为了适应 Sqoop 中 <code>and  $CONDITIONS</code>的语法）</li><li>增量：<code>select * from table where createtime paytime = $cur_date</code></li><li>新增和变化：``select * from table where createtime or operatetime = $cur_date`</li></ul></li></ul><p><strong>执行脚本：</strong></p><ul><li>内容略</li><li>执行：<ul><li>参数：<ul><li>参数1：first or all，表示第一次或每日，first会导入两个地区表，all就不导入了</li><li>参数2：日期</li></ul></li><li><code>mysql_to_hdfs.sh first 2020-03-10</code></li><li><code>mysql_to_hdfs.sh all 2020-03-11</code></li><li>一天的数据导入大概20分钟</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数仓 V2.0 （01 用户行为采集平台）</title>
      <link href="/2021/01/15/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0%20%EF%BC%8801%20%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E9%87%87%E9%9B%86%E5%B9%B3%E5%8F%B0%EF%BC%89/"/>
      <url>/2021/01/15/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0/%E7%94%B5%E5%95%86%E6%95%B0%E4%BB%93%20V2.0%20%EF%BC%8801%20%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E9%87%87%E9%9B%86%E5%B9%B3%E5%8F%B0%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="电商数仓-V2-0-（01-用户行为采集平台）"><a href="#电商数仓-V2-0-（01-用户行为采集平台）" class="headerlink" title="电商数仓 V2.0 （01 用户行为采集平台）"></a>电商数仓 V2.0 （01 用户行为采集平台）</h1><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210115095854491.png" alt="image-20210115095854491"></p><h2 id="一、数据仓库概念"><a href="#一、数据仓库概念" class="headerlink" title="一、数据仓库概念"></a>一、数据仓库概念</h2><h2 id="二、项目需求及架构设计"><a href="#二、项目需求及架构设计" class="headerlink" title="二、项目需求及架构设计"></a>二、项目需求及架构设计</h2><h2 id="三、数据生成模块"><a href="#三、数据生成模块" class="headerlink" title="三、数据生成模块"></a>三、数据生成模块</h2><p>执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java -classpath logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar com.shuofxz.appclient.AppMain &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"><span class="comment"># 或者（需要打包后的主类是 AppMain）</span></span><br><span class="line">java -jar logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>更改系统时间，用于生成不同日期的日志</p><p>（这个应该放到 java 代码中改吧）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更改系统时间</span></span><br><span class="line">sudo date -s 2021-01-11</span><br></pre></td></tr></table></figure><h2 id="四、数据采集模块"><a href="#四、数据采集模块" class="headerlink" title="四、数据采集模块"></a>四、数据采集模块</h2><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><ul><li>配置支持 lzo 压缩（先别搞，需要 lzo 编译的 hadoop 才能用）</li></ul><p>需要先安装 lzo 库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install lzo-devel lzop</span><br></pre></td></tr></table></figure><p>将编译好后的<code>hadoop-lzo-0.4.20.jar</code> 放入<code>hadoop-2.7.2/share/hadoop/common/</code></p><p>修改 <code>etc/hadaoop/core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>将 jar 包 和 core-site.xml 同步到其他机器上</p><ul><li>压力测试</li></ul><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><h3 id="flume-采集模块"><a href="#flume-采集模块" class="headerlink" title="flume 采集模块"></a>flume 采集模块</h3><ul><li>配置 <code>conf/flume-env.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line"><span class="comment"># HADOOP_HOME 必须写，否则启动时会报错，找不到 lib</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><ul><li>flume 中添加一个配置文件，用于指定 interceptor, channel 等</li></ul><p><code>conf/file-flume-kafka.conf</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1 c2</span><br><span class="line"></span><br><span class="line"># configure source</span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume-1.7.0-bin/test/log_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /tmp/logs/app.+</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">#interceptor</span><br><span class="line">a1.sources.r1.interceptors =  i1 i2</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.shuofxz.flume.interceptor.LogETLInterceptor$Builder</span><br><span class="line">a1.sources.r1.interceptors.i2.type = com.shuofxz.flume.interceptor.LogTypeInterceptor$Builder</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = topic</span><br><span class="line">a1.sources.r1.selector.mapping.topic_start = c1</span><br><span class="line">a1.sources.r1.selector.mapping.topic_event = c2</span><br><span class="line"></span><br><span class="line"># configure channel</span><br><span class="line">a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.channels.c1.kafka.topic = topic_start</span><br><span class="line">a1.channels.c1.parseAsFlumeEvent = false</span><br><span class="line">a1.channels.c1.kafka.consumer.group.id = flume-consumer</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.channels.c2.kafka.topic = topic_event</span><br><span class="line">a1.channels.c2.parseAsFlumeEvent = false</span><br><span class="line">a1.channels.c2.kafka.consumer.group.id = flume-consumer</span><br></pre></td></tr></table></figure><ul><li>写一个 java，编写 ETL 和 类型拦截器</li><li>修改 <code>conf/log4j.properties</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写相对路径貌似有问题，这个路径的logs 文件夹貌似也要提前创建</span></span><br><span class="line"><span class="comment"># 疑问：这个为啥要改呢？？？</span></span><br><span class="line">flume.log.dir=/opt/module/flume-1.7.0-bin/logs</span><br></pre></td></tr></table></figure><ul><li>flume 群起群停脚本</li></ul><p><code>flume-f1.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">" --------启动 <span class="variable">$i</span> 采集flume-------"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"nohup /opt/module/flume-1.7.0-bin/bin/flume-ng agent --conf /opt/module/flume-1.7.0-bin/conf --conf-file /opt/module/flume-1.7.0-bin/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/flume.log 2&gt;&amp;1  &amp;"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">" --------停止 <span class="variable">$i</span> 采集flume-------"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"ps -ef | grep file-flume-kafka | grep -v grep |awk  '&#123;print \$2&#125;' | xargs kill"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>注意：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个参数要加，否则 flume 运行不正常</span></span><br><span class="line">-Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/flume.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>注意：</p><p><code>flume-env.sh</code> 中需要配置 flume_opts 到 hadoop common 目录，否则启动时报错</p><h3 id="Kafka-日志收集"><a href="#Kafka-日志收集" class="headerlink" title="Kafka 日志收集"></a>Kafka 日志收集</h3><ul><li>flume 启动后就会往 kafka 里写数据，topic 也会自己创建</li><li>测试：执行之前创建的生成日志脚本 <code>gen_logs.sh</code>，如果可以在kafka中接收到新的数据，证明从日志生成 -&gt; flume -&gt; kafka 链路打通了</li><li>压力测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Producer</span></span><br><span class="line">bin/kafka-producer-perf-test.sh  --topic <span class="built_in">test</span> --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line"></span><br><span class="line"><span class="comment"># Consumer</span></span><br><span class="line">bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092 --topic <span class="built_in">test</span> --fetch-size 10000 --messages 10000000 --threads 1</span><br></pre></td></tr></table></figure><ul><li>查看列表</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper hadoop102:2181 --list</span><br></pre></td></tr></table></figure><ul><li>消费消息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop102:9092 --from-beginning --topic topic_start</span><br><span class="line"></span><br><span class="line">bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop102:9092 --topic topic_start</span><br></pre></td></tr></table></figure><h3 id="Kafka-数据存储到-HDFS"><a href="#Kafka-数据存储到-HDFS" class="headerlink" title="Kafka 数据存储到 HDFS"></a>Kafka 数据存储到 HDFS</h3><ul><li>配置文件</li></ul><p><code>conf/kafka-flume-hdfs.conf</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 组件</span></span><br><span class="line">a1.sources=r1 r2</span><br><span class="line">a1.channels=c1 c2</span><br><span class="line">a1.sinks=k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">## source1</span></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.batchSize = 5000</span><br><span class="line">a1.sources.r1.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sources.r1.kafka.topics=topic_start</span><br><span class="line"><span class="comment"># 重新读取数据需要添加，用于重新消费 kafka 的数据</span></span><br><span class="line"><span class="comment">#a1.sources.r1.groupId=f2_1</span></span><br><span class="line"><span class="comment">#a1.sources.r1.kafka.consumer.auto.offset.reset = earliest</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## source2</span></span><br><span class="line">a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r2.batchSize = 5000</span><br><span class="line">a1.sources.r2.batchDurationMillis = 2000</span><br><span class="line">a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line">a1.sources.r2.kafka.topics=topic_event</span><br><span class="line"><span class="comment">#a1.sources.r2.groupId=f2_1</span></span><br><span class="line"><span class="comment">#a1.sources.r2.kafka.consumer.auto.offset.reset = earliest</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## channel1</span></span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0-bin/checkpoint/behavior1</span><br><span class="line">a1.channels.c1.dataDirs = /opt/module/flume-1.7.0-bin/data/behavior1/</span><br><span class="line">a1.channels.c1.keep-alive = 6</span><br><span class="line"></span><br><span class="line"><span class="comment">## channel2</span></span><br><span class="line">a1.channels.c2.type = file</span><br><span class="line">a1.channels.c2.checkpointDir = /opt/module/flume-1.7.0-bin/checkpoint/behavior2</span><br><span class="line">a1.channels.c2.dataDirs = /opt/module/flume-1.7.0-bin/data/behavior2/</span><br><span class="line">a1.channels.c2.keep-alive = 6</span><br><span class="line"></span><br><span class="line"><span class="comment">## sink1</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /origin_data/gmall/<span class="built_in">log</span>/topic_start/%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = logstart-</span><br><span class="line"></span><br><span class="line"><span class="comment">##sink2</span></span><br><span class="line">a1.sinks.k2.type = hdfs</span><br><span class="line">a1.sinks.k2.hdfs.path = /origin_data/gmall/<span class="built_in">log</span>/topic_event/%Y-%m-%d</span><br><span class="line">a1.sinks.k2.hdfs.filePrefix = logevent-</span><br><span class="line"></span><br><span class="line"><span class="comment">## 不要产生大量小文件,生产环境rollInterval配置为3600</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.hdfs.rollInterval = 10</span><br><span class="line">a1.sinks.k2.hdfs.rollSize = 134217728</span><br><span class="line">a1.sinks.k2.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"><span class="comment">## 控制输出文件是原生文件(hdfs未配置lzo压缩，这里全都注释掉)</span></span><br><span class="line"><span class="comment">#a1.sinks.k1.hdfs.fileType = CompressedStream</span></span><br><span class="line"><span class="comment">#a1.sinks.k2.hdfs.fileType = CompressedStream</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#a1.sinks.k1.hdfs.codeC = lzop</span></span><br><span class="line"><span class="comment">#a1.sinks.k2.hdfs.codeC = lzop</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 拼装</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel= c1</span><br><span class="line"></span><br><span class="line">a1.sources.r2.channels = c2</span><br><span class="line">a1.sinks.k2.channel= c2</span><br></pre></td></tr></table></figure><ul><li>脚本 <code>flume-f2.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">" --------启动 <span class="variable">$i</span> 消费flume-------"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"nohup /opt/module/flume-1.7.0-bin/bin/flume-ng agent --conf-file /opt/module/flume-1.7.0-bin/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/logs/f2_log.txt  2&gt;&amp;1 &amp;"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">" --------停止 <span class="variable">$i</span> 消费flume-------"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '&#123;print \$2&#125;' | xargs kill"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><ul><li>集群启动/停止脚本<code>cluster.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" -------- 启动 集群 -------"</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" -------- 启动 hadoop集群 -------"</span></span><br><span class="line">        /opt/module/hadoop-2.7.2/sbin/start-dfs.sh</span><br><span class="line">        ssh hadoop103 <span class="string">"/opt/module/hadoop-2.7.2/sbin/start-yarn.sh"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#启动 Zookeeper集群</span></span><br><span class="line">        /opt/module/zookeeper-3.4.10/bin/zk-all.sh start</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (( i=0; i&lt;6; i++ ));<span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"wait... <span class="variable">$i</span>"</span></span><br><span class="line">                sleep 1s;</span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#启动 Flume采集集群</span></span><br><span class="line">        /opt/module/flume-1.7.0-bin/bin/flume-f1.sh start</span><br><span class="line"></span><br><span class="line">        <span class="comment">#启动 Kafka采集集群</span></span><br><span class="line">        /opt/module/kafka_2.12-2.3.1/bin/kafka-all.sh start</span><br><span class="line"></span><br><span class="line">    sleep 6s;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#启动 Flume消费集群</span></span><br><span class="line">        /opt/module/flume-1.7.0-bin/bin/flume-f2.sh start</span><br><span class="line"></span><br><span class="line">        &#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">" -------- 停止 集群 -------"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#停止 Flume消费集群</span></span><br><span class="line">        /opt/module/flume-1.7.0-bin/bin/flume-f2.sh stop</span><br><span class="line"></span><br><span class="line">        <span class="comment">#停止 Kafka采集集群</span></span><br><span class="line">        /opt/module/kafka_2.12-2.3.1/bin/kafka-all.sh stop</span><br><span class="line"></span><br><span class="line">    sleep 10s;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#停止 Flume采集集群</span></span><br><span class="line">        /opt/module/flume-1.7.0-bin/bin/flume-f1.sh stop</span><br><span class="line"></span><br><span class="line">        <span class="comment">#停止 Zookeeper集群</span></span><br><span class="line">        /opt/module/zookeeper-3.4.10/bin/zk-all.sh stop</span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">" -------- 停止 hadoop集群 -------"</span></span><br><span class="line">        ssh hadoop103 <span class="string">"/opt/module/hadoop-2.7.2/sbin/stop-yarn.sh"</span></span><br><span class="line">        /opt/module/hadoop-2.7.2/sbin/stop-dfs.sh</span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><ul><li>停止集群</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cluster.sh stop</span><br></pre></td></tr></table></figure><ul><li>改系统时间 <code>change_date.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">change_date.sh 2020-03-10</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"========== Change Date <span class="variable">$i</span> =========="</span></span><br><span class="line">        ssh -t <span class="variable">$i</span> <span class="string">"sudo date -s <span class="variable">$1</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><ul><li>启动集群</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cluster.sh start</span><br></pre></td></tr></table></figure><ul><li>生成日志</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gen_logs.sh</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103;<span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"====== generte <span class="variable">$i</span> log ======="</span></span><br><span class="line">    ssh <span class="variable">$i</span> <span class="string">"java -jar /opt/project/data_warehouse_v2/logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar <span class="variable">$1</span> <span class="variable">$2</span> &gt; /dev/null 2&gt;&amp;1 &amp;"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><ul><li>查看 hdfs 中的结果</li></ul><p>启动后，日志经历  file -&gt; flume -&gt; kafka -&gt; flume -&gt; hdfs，最终会输出到配置的<code>hdfs://origin_data/gmall/log/topic_event/%Y-%m-%d</code>中</p><ul><li>注意</li></ul><p>如果要删掉原来的重新采集的话，1）kafka 注意删除的时候把 topic 删除完全，去 Zookeeper 中看是否还有该 topic 记录；2）flume 要重新消费</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python 解析xml配置文件</title>
      <link href="/2020/12/21/Python/Python%20%E8%A7%A3%E6%9E%90xml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
      <url>/2020/12/21/Python/Python%20%E8%A7%A3%E6%9E%90xml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<hr><p><a href="https://www.cnblogs.com/hupeng1234/p/7262371.html" target="_blank" rel="noopener">https://www.cnblogs.com/hupeng1234/p/7262371.html</a></p><p><a href="https://www.cnblogs.com/yyds/p/6627208.html" target="_blank" rel="noopener">https://www.cnblogs.com/yyds/p/6627208.html</a></p><p><a href="https://blog.csdn.net/liangpingguo/article/details/105164967" target="_blank" rel="noopener">https://blog.csdn.net/liangpingguo/article/details/105164967</a></p><p><a href="https://blog.csdn.net/youZhengChuan/article/details/52996524" target="_blank" rel="noopener">https://blog.csdn.net/youZhengChuan/article/details/52996524</a></p><p><a href="https://www.pythonf.cn/read/79523" target="_blank" rel="noopener">https://www.pythonf.cn/read/79523</a></p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM 资料</title>
      <link href="/2020/12/16/JVM/JVM%20%E8%B5%84%E6%96%99/"/>
      <url>/2020/12/16/JVM/JVM%20%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<h2 id="1-JVM-资料整理"><a href="#1-JVM-资料整理" class="headerlink" title="1 JVM 资料整理"></a>1 JVM 资料整理</h2><p>Java堆分析器 - Eclipse Memory Analyzer Tool(MAT)</p><p><a href="https://www.jianshu.com/p/de989b94ca3a" target="_blank" rel="noopener">https://www.jianshu.com/p/de989b94ca3a</a></p><p>内存分析工具MAT的使用入门<br><a href="https://cloud.tencent.com/developer/article/1676945" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1676945</a></p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux文本编辑命令 sed</title>
      <link href="/2020/12/16/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%91%BD%E4%BB%A4%20sed/"/>
      <url>/2020/12/16/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%91%BD%E4%BB%A4%20sed/</url>
      
        <content type="html"><![CDATA[<h2 id="一、基本介绍"><a href="#一、基本介绍" class="headerlink" title="一、基本介绍"></a>一、基本介绍</h2><h2 id="二、常用操作"><a href="#二、常用操作" class="headerlink" title="二、常用操作"></a>二、常用操作</h2><h3 id="2-1-替换"><a href="#2-1-替换" class="headerlink" title="2.1 替换"></a>2.1 替换</h3><p>用<code>s</code>命令替换</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">"s/替换前字符/替换后字符/"</span> xxx.txt  <span class="comment"># 仅替换每行第一个匹配的字符</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ cat NewFile.txt</span><br><span class="line">hello tom</span><br><span class="line">1234</span><br><span class="line">hhh                                                                                                                                                                                     ➜ sed <span class="string">"s/h/p/"</span> NewFile.txt</span><br><span class="line">pello tom</span><br><span class="line">1234</span><br><span class="line">phh</span><br></pre></td></tr></table></figure><p>可以在后面添加<code>g</code>，作用于行内所有匹配的字符</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ sed <span class="string">'s/h/p/g'</span> NewFile.txt</span><br><span class="line">pello tom</span><br><span class="line">1234</span><br><span class="line">ppp</span><br></pre></td></tr></table></figure><p><strong>输出到文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出到新文件</span></span><br><span class="line">sed <span class="string">'s/h/p/g'</span> NewFile.txt &gt; nn.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原地替换（-i）</span></span><br><span class="line">sed -i <span class="string">'s/h/p/g'</span> NewFile.txt</span><br></pre></td></tr></table></figure><h3 id="2-2-其他"><a href="#2-2-其他" class="headerlink" title="2.2 其他"></a>2.2 其他</h3><p><code>d</code>命令：删除匹配行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &apos;2,$d&apos; my.txt</span><br></pre></td></tr></table></figure><p><code>p</code>命令：打印命令，类似grep功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n &apos;1,/fish/p&apos; my.txt</span><br></pre></td></tr></table></figure><h2 id="三、正则匹配"><a href="#三、正则匹配" class="headerlink" title="三、正则匹配"></a>三、正则匹配</h2><h3 id="3-1-单个匹配"><a href="#3-1-单个匹配" class="headerlink" title="3.1 单个匹配"></a>3.1 单个匹配</h3><p>在每一行最前面(^)加点东西：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/^/#/g&quot; pets.txt</span><br></pre></td></tr></table></figure><p>在每一行最后面($)加点东西：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/$/ --- /g&quot; pets.txt</span><br></pre></td></tr></table></figure><p>正则表达式的一些最基本的东西：</p><ul><li><code>^</code> 表示一行的开头。如：<code>/^#/</code> 以<code>#</code>开头的匹配。</li><li><code>$</code> 表示一行的结尾。如：<code>/}$/</code> 以<code>}</code>结尾的匹配。</li><li><code>\&lt;</code> 表示词首。 如 <code>\&lt;abc</code> 表示以 abc 为首的詞。</li><li><code>\&gt;</code> 表示词尾。 如 <code>abc\&gt;</code> 表示以 abc 結尾的詞。</li><li><code>.</code> 表示任何单个字符。</li><li><code>*</code>表示某个字符出现了0次或多次。</li><li><code>[]</code> 空格或字符集合。 如：<code>[abc]</code>表示匹配a或b或c，还有<code>[a-zA-Z]</code>表示匹配所有的26个字符。如果其中有<code>^</code>表示反，如<code>[^a]</code>表示非a的字符。</li></ul><p>正规则表达式是一些很牛的事，比如我们要去掉某html中的tags：</p><p>如果你这样搞的话，就会有问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/&lt;.*&gt;//g&quot; html.txt</span><br></pre></td></tr></table></figure><p>要解决上面的那个问题，就得像下面这样，其中的”[^&gt;]”指定了除了&gt;的字符重复0次或多次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/&lt;[^&gt;]*&gt;//g&quot; html.txt</span><br></pre></td></tr></table></figure><p>我们再来看看指定需要替换第3行的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;3s/my/your/g&quot; pets.txt</span><br></pre></td></tr></table></figure><p>下面的命令只替换第3到第6行的文本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;3,6s/my/your/g&quot; pets.txt</span><br></pre></td></tr></table></figure><p>只替换每一行的第一个s：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/s/S/1&quot; my.txt</span><br></pre></td></tr></table></figure><p>只替换每一行的第二个s：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/s/S/2&quot; my.txt</span><br></pre></td></tr></table></figure><p>只替换第一行的第3个以后的s：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/s/S/3g&quot; my.txt</span><br></pre></td></tr></table></figure><h3 id="3-2-多个匹配"><a href="#3-2-多个匹配" class="headerlink" title="3.2 多个匹配"></a>3.2 多个匹配</h3><p>如果我们需要一次替换多个模式，可参看下面的示例：（第一个模式把第一行到第三行的my替换成your，第二个则把第3行以后的This替换成了That）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;1,3s/my/your/g; 3,$s/This/That/g&quot; my.txt</span><br></pre></td></tr></table></figure><p>上面的命令等价于：（注：下面使用的是sed的-e命令行参数）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -e &quot;1,3s/my/your/g&quot; -e &quot;3,$s/This/That/g&quot; my.txt</span><br></pre></td></tr></table></figure><p>我们可以使用&amp;来当做被匹配的变量，然后可以在基本左右加点东西。如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/my/[&amp;]/g&quot; my.txt</span><br></pre></td></tr></table></figure><h3 id="3-3-圆括号匹配"><a href="#3-3-圆括号匹配" class="headerlink" title="3.3 圆括号匹配"></a>3.3 圆括号匹配</h3><p>使用圆括号匹配的示例：（圆括号括起来的正则表达式所匹配的字符串会可以当成变量来使用，sed中使用的是\1,\2…）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed &quot;s/This is my \([^,]*\),.*is \(.*\)/\1:\2/g&quot; my.txt</span><br></pre></td></tr></table></figure><p>上面这个例子中的正则表达式有点复杂，解开如下（去掉转义字符）：</p><p>正则为：This is my ([^,]<em>),.*is (.</em>)</p><p>匹配为：This is my (cat),……….is (betty)</p><p>然后：\1就是cat，\2就是betty</p><hr><h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h3><p><a href="https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html" target="_blank" rel="noopener">https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html</a></p><p><a href="https://man.linuxde.net/sed" target="_blank" rel="noopener">https://man.linuxde.net/sed</a></p><p><a href="https://coolshell.cn/articles/9104.html" target="_blank" rel="noopener">https://coolshell.cn/articles/9104.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/145661854" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/145661854</a></p><p><a href="https://www.cnblogs.com/along21/p/10366886.html" target="_blank" rel="noopener">https://www.cnblogs.com/along21/p/10366886.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, sed </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2020/11/27/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/11/27/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="一、基础指令"><a href="#一、基础指令" class="headerlink" title="一、基础指令"></a>一、基础指令</h2><ul><li>xargs</li></ul><p>[Linux输出转换命令 xargs](./常用命令/Linux输出转换命令 xargs.md)</p><p><code>xargs</code>命令的作用，是将标准输入转为命令行参数。</p><p>原因：大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"hello world"</span> | xargs <span class="built_in">echo</span></span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h2 id="二、指令组合"><a href="#二、指令组合" class="headerlink" title="二、指令组合"></a>二、指令组合</h2><p><strong>输出中间结果</strong></p><p>使用 tee 指令，将中间结果进行输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find ./ -i <span class="string">"*.java"</span> | tee JavaList | grep Spring</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux文本文件处理程序 awk</title>
      <link href="/2020/11/27/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%20awk/"/>
      <url>/2020/11/27/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%20awk/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、基础用法"><a href="#一、基础用法" class="headerlink" title="一、基础用法"></a>一、基础用法</h2><p><code>awk</code>是处理文本文件的一个应用程序，几乎所有 Linux 系统都自带这个程序。</p><p>它依次处理文件的每一行，并读取里面的每一个字段。对于日志、CSV 那样的每行格式相同的文本文件，<code>awk</code>可能是最方便的工具。</p><p><code>awk</code>的基本用法就是下面的形式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 格式</span></span><br><span class="line">$ awk 动作 文件名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">$ awk <span class="string">'&#123;print $0&#125;'</span> demo.txt</span><br></pre></td></tr></table></figure><p>上面示例中，<code>demo.txt</code>是<code>awk</code>所要处理的文本文件。前面单引号内部有一个大括号，里面就是每一行的处理动作<code>print $0</code>。其中，<code>print</code>是打印命令，<code>$0</code>代表当前行。因此上面命令是把每一行原样打印出来。</p><p><code>awk</code>会根据空格和制表符，将每一行分成若干字段，依次用<code>$1</code>、<code>$2</code>、<code>$3</code>代表第一个字段、第二个字段、第三个字段等等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'this is a test'</span> | awk <span class="string">'&#123;print $3&#125;'</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>下面，为了便于举例，我们把<code>/etc/passwd</code>文件保存成<code>demo.txt</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root:x:0:0:root:/root:/usr/bin/zsh</span><br><span class="line">daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin</span><br><span class="line">bin:x:2:2:bin:/bin:/usr/sbin/nologin</span><br><span class="line">sys:x:3:3:sys:/dev:/usr/sbin/nologin</span><br><span class="line">sync:x:4:65534:sync:/bin:/bin/sync</span><br></pre></td></tr></table></figure><p>这个文件的字段分隔符是冒号（<code>:</code>），所以要用<code>-F</code>参数指定分隔符为冒号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'&#123; print $1 &#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line">daemon</span><br><span class="line">bin</span><br><span class="line">sys</span><br><span class="line">sync</span><br></pre></td></tr></table></figure><h2 id="二、变量"><a href="#二、变量" class="headerlink" title="二、变量"></a>二、变量</h2><ul><li><code>数字</code>：第几个字段（从1开始）</li><li><code>NF</code>：当前行有多少个字段，因此<code>$NF</code>就代表最后一个字段</li><li><code>NR</code>：表示当前处理的是第几行。</li><li><code>FILENAME</code>：当前文件名</li><li><code>FS</code>：字段分隔符，默认是空格和制表符。</li><li><code>RS</code>：行分隔符，用于分割每一行，默认是换行符。</li><li><code>OFS</code>：输出字段的分隔符，用于打印时分隔字段，默认为空格。</li><li><code>ORS</code>：输出记录的分隔符，用于打印时分隔记录，默认为换行符。</li><li><code>OFMT</code>：数字输出的格式，默认为<code>％.6g</code>。</li></ul><p>变量<code>NF</code>表示当前行有多少个字段，因此<code>$NF</code>就代表最后一个字段；<code>$(NF-1)</code>代表倒数第二个字段。</p><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ <span class="built_in">echo</span> <span class="string">'this is a test'</span> | awk <span class="string">'&#123;print $NF, $(NF-1)&#125;'</span>   <span class="comment"># print中的逗号是以空格分隔的意思</span></span><br><span class="line">&gt; <span class="built_in">test</span> a</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>变量<code>NR</code>表示当前处理的是第几行。</p><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ awk -F <span class="string">':'</span> <span class="string">'&#123;print NR ") " $1&#125;'</span> demo.txt</span><br><span class="line">&gt; 1) root</span><br><span class="line">&gt; 2) daemon</span><br><span class="line">&gt; 3) bin</span><br><span class="line">&gt; 4) sys</span><br><span class="line">&gt; 5) sync</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>上面代码中，<code>print</code>命令里面，如果原样输出字符，要放在双引号里面</p><h2 id="三、函数"><a href="#三、函数" class="headerlink" title="三、函数"></a>三、函数</h2><p>常用函数如下</p><ul><li><code>toupper()</code>：字符转为大写</li><li><code>tolower()</code>：字符转为小写</li><li><code>length()</code>：返回字符串长度</li><li><code>substr()</code>：返回子字符串</li><li><code>sin()</code>：正弦</li><li><code>cos()</code>：余弦</li><li><code>sqrt()</code>：平方根</li><li><code>rand()</code>：随机数</li></ul><p><code>awk</code>内置函数的完整列表，可以查看<a href="https://www.gnu.org/software/gawk/manual/html_node/Built_002din.html#Built_002din" target="_blank" rel="noopener">手册</a>。</p><p>例：<code>toupper</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'&#123; print toupper($1) &#125;'</span> demo.txt</span><br><span class="line">ROOT</span><br><span class="line">DAEMON</span><br><span class="line">BIN</span><br><span class="line">SYS</span><br><span class="line">SYNC</span><br></pre></td></tr></table></figure><h2 id="四、条件"><a href="#四、条件" class="headerlink" title="四、条件"></a>四、条件</h2><p><code>awk</code>允许指定输出条件，只输出符合条件的行。</p><p>输出条件要写在动作的前面。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ awk <span class="string">'条件 动作'</span> 文件名</span><br></pre></td></tr></table></figure><p>请看下面的例子。<code>print</code>命令前面是一个正则表达式，只输出包含<code>usr</code>的行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'/usr/ &#123;print $1&#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line">daemon</span><br><span class="line">bin</span><br><span class="line">sys</span><br></pre></td></tr></table></figure><p>下面的例子只输出奇数行，以及输出第三行以后的行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出奇数行</span></span><br><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'NR % 2 == 1 &#123;print $1&#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line">bin</span><br><span class="line">sync</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出第三行以后的行</span></span><br><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'NR &gt;3 &#123;print $1&#125;'</span> demo.txt</span><br><span class="line">sys</span><br><span class="line">sync</span><br></pre></td></tr></table></figure><p>下面的例子输出第一个字段等于指定值的行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'$1 == "root" &#123;print $1&#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line"></span><br><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'$1 == "root" || $1 == "bin" &#123;print $1&#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line">bin</span><br></pre></td></tr></table></figure><h2 id="五、if-语句"><a href="#五、if-语句" class="headerlink" title="五、if 语句"></a>五、if 语句</h2><p><code>awk</code>提供了<code>if</code>结构，用于编写复杂的条件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'&#123;if ($1 &gt; "m") print $1&#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line">sys</span><br><span class="line">sync</span><br></pre></td></tr></table></figure><p>上面代码输出第一个字段的第一个字符大于<code>m</code>的行。</p><p><code>if</code>结构还可以指定<code>else</code>部分。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ awk -F <span class="string">':'</span> <span class="string">'&#123;if ($1 &gt; "m") print $1; else print "---"&#125;'</span> demo.txt</span><br><span class="line">root</span><br><span class="line">---</span><br><span class="line">---</span><br><span class="line">sys</span><br><span class="line">sync</span><br></pre></td></tr></table></figure><h2 id="六、相关连接"><a href="#六、相关连接" class="headerlink" title="六、相关连接"></a>六、相关连接</h2><ul><li><a href="https://gregable.com/2010/09/why-you-should-know-just-little-awk.html" target="_blank" rel="noopener">An Awk tutorial by Example</a>, Greg Grothaus</li><li><a href="https://likegeeks.com/awk-command/" target="_blank" rel="noopener">30 Examples for Awk Command in Text Processing</a>, Mokhtar Ebrahim</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux输出转换命令 xargs</title>
      <link href="/2020/11/27/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E8%BE%93%E5%87%BA%E8%BD%AC%E6%8D%A2%E5%91%BD%E4%BB%A4%20xargs/"/>
      <url>/2020/11/27/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E8%BE%93%E5%87%BA%E8%BD%AC%E6%8D%A2%E5%91%BD%E4%BB%A4%20xargs/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"hello world"</span> | xargs <span class="built_in">echo</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、基本用法"><a href="#一、基本用法" class="headerlink" title="一、基本用法"></a>一、基本用法</h2><p><code>xargs</code>命令的作用，是将标准输入转为命令行参数。</p><p>原因：大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数</p><p>如下面 echo 不接受标准输出做参数，可用 xargs 做转换：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"hello world"</span> | xargs <span class="built_in">echo</span></span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h2 id="二、参数"><a href="#二、参数" class="headerlink" title="二、参数"></a>二、参数</h2><h3 id="d-指定分隔符"><a href="#d-指定分隔符" class="headerlink" title="-d 指定分隔符"></a><code>-d</code> 指定分隔符</h3><p>默认情况下，<code>xargs</code>将换行符和空格作为分隔符，把标准输入分解成一个个命令行参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"one two three"</span> | xargs mkdir</span><br></pre></td></tr></table></figure><p>上面代码中，<code>mkdir</code>会新建三个子目录，执行<code>mkdir one two three</code>。</p><p><code>-d</code>参数可以更改分隔符</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -e <span class="string">"a\tb\tc"</span> | xargs -d <span class="string">"\t"</span> <span class="built_in">echo</span></span><br><span class="line">a b c</span><br></pre></td></tr></table></figure><p>上面的命令指定制表符<code>\t</code>作为分隔符，所以<code>a\tb\tc</code>就转换成了三个命令行参数。<code>echo</code>命令的<code>-e</code>参数表示解释转义字符。</p><h3 id="p-t打印将要执行的命令"><a href="#p-t打印将要执行的命令" class="headerlink" title="-p -t打印将要执行的命令"></a><code>-p -t</code>打印将要执行的命令</h3><p><code>-p</code>参数打印出要执行的命令，询问用户是否要执行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'one two three'</span> | xargs -p touch</span><br><span class="line">touch one two three ?...</span><br></pre></td></tr></table></figure><p><code>-t</code>参数则是打印出最终要执行的命令，然后直接执行，不需要用户确认。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'one two three'</span> | xargs -t rm</span><br><span class="line">rm one two three</span><br></pre></td></tr></table></figure><h3 id="I-传递参数起别名"><a href="#I-传递参数起别名" class="headerlink" title="-I 传递参数起别名"></a><code>-I</code> 传递参数起别名</h3><p>如果<code>xargs</code>要将命令行参数传给多个命令，可以使用<code>-I</code>参数。【貌似，会按空格或回车对参数进行分割，然后重复执行命令，而不是当成命令的多个参数】</p><p><code>-I</code>指定每一项命令行参数的替代字符串。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat foo.txt</span><br><span class="line">one</span><br><span class="line">two</span><br><span class="line">three</span><br><span class="line"></span><br><span class="line">$ cat foo.txt | xargs -I file sh -c <span class="string">'echo file; mkdir file'</span></span><br><span class="line">one </span><br><span class="line">two</span><br><span class="line">three</span><br><span class="line"></span><br><span class="line">$ ls </span><br><span class="line">one two three</span><br></pre></td></tr></table></figure><p>上面代码中，<code>foo.txt</code>是一个三行的文本文件。我们希望对每一项命令行参数，执行两个命令（<code>echo</code>和<code>mkdir</code>），使用<code>-I file</code>表示<code>file</code>是命令行参数的替代字符串。执行命令时，具体的参数会替代掉<code>echo file; mkdir file</code>里面的两个<code>file</code>。</p><h3 id="l-L-指定多少行作为一个命令行参数"><a href="#l-L-指定多少行作为一个命令行参数" class="headerlink" title="-l -L 指定多少行作为一个命令行参数"></a><code>-l -L</code> 指定多少行作为一个命令行参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -e <span class="string">"a\nb\nc"</span> | xargs -L 1 <span class="built_in">echo</span></span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">c</span><br></pre></td></tr></table></figure><h3 id="n-指定一行内多项作为一个命令行参数"><a href="#n-指定一行内多项作为一个命令行参数" class="headerlink" title="-n 指定一行内多项作为一个命令行参数"></a><code>-n</code> 指定一行内多项作为一个命令行参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> &#123;0..9&#125; | xargs -n 2 <span class="built_in">echo</span></span><br><span class="line">0 1</span><br><span class="line">2 3</span><br><span class="line">4 5</span><br><span class="line">6 7</span><br><span class="line">8 9</span><br></pre></td></tr></table></figure><h3 id="max-procs-多线程执行"><a href="#max-procs-多线程执行" class="headerlink" title="--max-procs 多线程执行"></a><code>--max-procs</code> 多线程执行</h3><p><code>xargs</code>默认只用一个进程执行命令。如果命令要执行多次，必须等上一次执行完，才能执行下一次。</p><p><code>--max-procs</code>参数指定同时用多少个进程并行执行命令。<code>--max-procs 2</code>表示同时最多使用两个进程，<code>--max-procs 0</code>表示不限制进程数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -q | xargs -n 1 --max-procs 0 docker <span class="built_in">kill</span></span><br></pre></td></tr></table></figure><p>上面命令表示，同时关闭尽可能多的 Docker 容器，这样运行速度会快很多</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell 编程</title>
      <link href="/2020/11/19/Linux/Shell%20%E7%BC%96%E7%A8%8B/"/>
      <url>/2020/11/19/Linux/Shell%20%E7%BC%96%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="0-基础"><a href="#0-基础" class="headerlink" title="0 基础"></a>0 基础</h2><p>第一行</p><p>指明脚本应使用的解释器的名字</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br></pre></td></tr></table></figure><p>编程规范：</p><ul><li>大写字母表示常量，小写字母表示变量</li></ul><h2 id="1-变量"><a href="#1-变量" class="headerlink" title="1 变量"></a>1 变量</h2><p><strong>变量</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意等号两边不能有空格</span></span><br><span class="line">foo=<span class="string">"yes"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$foo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有空格的字符串需要用引号包围</span></span><br><span class="line">b=<span class="string">"abc efg"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用其他变量的值</span></span><br><span class="line">c=<span class="string">"hhh <span class="variable">$b</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将执行命令的结果赋值</span></span><br><span class="line">d=$(ls -la)</span><br><span class="line">d1=`ls -la` <span class="comment"># ``等价 $()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 算数扩展，注意是两个括号</span></span><br><span class="line">e=$((5*7))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用&#123;&#125;限定变量名的范围</span></span><br><span class="line">f=aa.txt</span><br><span class="line">g=<span class="variable">$&#123;f&#125;</span>1</span><br></pre></td></tr></table></figure><p><strong>环境变量</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> xx=xxx<span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="built_in">source</span> xxx_file<span class="comment"># 让文件中的环境变量立即生效</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$xx</span><span class="comment"># 输出变量的值</span></span><br></pre></td></tr></table></figure><p><strong>位置参数变量</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$n</span><span class="comment"># $0为命令本身；$1-$9代表第一到第九个参数；$&#123;10&#125;十以上的用大括号括起来</span></span><br><span class="line">$*<span class="comment"># 代表命令行中所有参数，并把所有参数看成一个整体</span></span><br><span class="line"><span class="variable">$@</span><span class="comment"># 代表命令行中所有参数，但把每个参数区分对待？</span></span><br><span class="line"><span class="variable">$#</span><span class="comment"># 参数个数</span></span><br></pre></td></tr></table></figure><p><strong>预定义变量</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$<span class="comment"># 当前进程号</span></span><br><span class="line">$!<span class="comment"># 后台运行的最后一个进程的进程号</span></span><br><span class="line">$?<span class="comment"># 最后一次执行命令的返回状态，0代表成功，其他都是失败 可自定义</span></span><br></pre></td></tr></table></figure><h2 id="2-条件判断"><a href="#2-条件判断" class="headerlink" title="2 条件判断"></a>2 条件判断</h2><p><strong>运算符</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$((m+n))<span class="comment"># $(()) 中间写运算式</span></span><br><span class="line">$[m+n]<span class="comment"># 推荐这种方式</span></span><br><span class="line">expr m + n<span class="comment"># 不推荐</span></span><br></pre></td></tr></table></figure><p><strong>条件判断</strong></p><p>test</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写法一</span></span><br><span class="line"><span class="built_in">test</span> expression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">[ expression ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line">[[ expression ]]<span class="comment"># 推荐使用这种写法，包含前两种的用法，且还支持模式匹配</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数字判断</span></span><br><span class="line">(( expression ))</span><br></pre></td></tr></table></figure><p>字符串判断</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[ -n string ]<span class="comment"># 如果字符串string的长度大于零，则为真</span></span><br><span class="line">[ -z string ]<span class="comment"># 如果字符串string的长度为零，则为真</span></span><br><span class="line">[ string1 = string2 ]<span class="comment"># 如果string1和string2相同，则为真</span></span><br><span class="line">[ string1 == string2 ] <span class="comment"># 等同于[ string1 = string2 ]</span></span><br><span class="line">[ string1 != string2 ]<span class="comment"># 如果string1和string2不相同，则为真</span></span><br><span class="line">[ string1 <span class="string">'&gt;'</span> string2 ]<span class="comment"># 如果按照字典顺序string1排列在string2之后，则为真</span></span><br><span class="line">[ string1 <span class="string">'&lt;'</span> string2 ]<span class="comment"># 如果按照字典顺序string1排列在string2之前，则为真</span></span><br></pre></td></tr></table></figure><p>整数判断</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(( m &gt; n ))<span class="comment"># 可以直接用 &gt; &lt; == &gt;= &lt;= !=，空格都要有！</span></span><br></pre></td></tr></table></figure><p>逻辑判断</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[ expr1 &amp;&amp; expr2 ]]<span class="comment"># &amp;&amp; || !</span></span><br></pre></td></tr></table></figure><h2 id="3-流控制"><a href="#3-流控制" class="headerlink" title="3 流控制"></a>3 流控制</h2><p>if</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [[ <span class="variable">$x</span> = 5 ]]; <span class="keyword">then</span><span class="comment"># 等号两边有空格，也可以用 == 代替</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"x=5"</span></span><br><span class="line"><span class="keyword">elif</span> [[ <span class="variable">$x</span> = 10 ]]; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"x=10"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"no"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>case</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> $变量名 <span class="keyword">in</span></span><br><span class="line"><span class="string">"值 1"</span>）</span><br><span class="line">如果变量的值等于值 1，则执行程序 1</span><br><span class="line">;;</span><br><span class="line"></span><br><span class="line"><span class="string">"值 2"</span>）</span><br><span class="line">如果变量的值等于值 2，则执行程序 2</span><br><span class="line">;;</span><br><span class="line"></span><br><span class="line">*）</span><br><span class="line">如果变量的值都不是以上的值，则执行此程序</span><br><span class="line">;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>while</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">a=5</span><br><span class="line"><span class="keyword">while</span> [[ <span class="variable">$a</span>&gt;0 ]]; <span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$a</span></span><br><span class="line">a=$(( a-1 ))</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> finish</span><br></pre></td></tr></table></figure><p>for</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (( i=0; i&lt;5; i++ )); <span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># =========================</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> A B C D; <span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell, linux, 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark</title>
      <link href="/2020/11/05/Hadoop/Spark/"/>
      <url>/2020/11/05/Hadoop/Spark/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="一、SparkCore"><a href="#一、SparkCore" class="headerlink" title="一、SparkCore"></a>一、SparkCore</h2><h3 id="RDD-创建"><a href="#RDD-创建" class="headerlink" title="RDD 创建"></a>RDD 创建</h3><ul><li>从集合中创建</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">listRdd.foreach(println)</span><br><span class="line"></span><br><span class="line">val arrayRDD: RDD[Int] = sc.parallelize(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">arrayRDD.foreach(println)</span><br></pre></td></tr></table></figure><ul><li>由外部存储系统的数据集创建</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val lines: RDD[String] = sc.textFile(<span class="string">"in"</span>)</span><br></pre></td></tr></table></figure><h3 id="RDD-转换算子"><a href="#RDD-转换算子" class="headerlink" title="RDD 转换算子"></a>RDD 转换算子</h3><h4 id="Value-类型"><a href="#Value-类型" class="headerlink" title="Value 类型"></a>Value 类型</h4><h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">val mulRdd: RDD[Int] = listRdd.map(_ * <span class="number">2</span>)</span><br><span class="line">mulRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h5><p>对每一个分区中的数据批处理。相当于只给每个分区的数据，只发送一次计算；而 map 的实现会给每个数据发送一次计算，增加了网络传输消耗；但是 mapPartitions 由于以整个分区为单位，可能会造成 OOM</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">val mapParRdd: RDD[Int] = listRdd.mapPartitions(datas =&gt; &#123;</span><br><span class="line">  datas.map(_ * <span class="number">2</span>)</span><br><span class="line">&#125;)</span><br><span class="line">mapParRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">val tupleRdd: RDD[(Int, String)] = listRdd.mapPartitionsWithIndex &#123;</span><br><span class="line">  <span class="keyword">case</span> (num, datas) =&gt; &#123;</span><br><span class="line">    datas.map((_, <span class="string">"partition_num: "</span> + num))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">tupleRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h5><p>扁平化，变成一个一个单独的元素</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[List[Int]] = sc.makeRDD(Array(List(<span class="number">1</span>, <span class="number">2</span>), List(<span class="number">3</span>, <span class="number">4</span>)))</span><br><span class="line">val flatRdd: RDD[Int] = listRdd.flatMap(datas =&gt; datas)</span><br><span class="line">flatRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h5><p>将同一个分区的元素，放到一个数组里</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">16</span>, <span class="number">4</span>)</span><br><span class="line">val glomRdd: RDD[Array[Int]] = listRdd.glom()</span><br><span class="line">glomRdd.collect().foreach(array =&gt; &#123;</span><br><span class="line">  println(array.mkString(<span class="string">","</span>))</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h5 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h5><p>同一个分区的放到一个迭代对象中。结果 tuple 中，第一个元素是 key，后面是 iterator</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">9</span>)</span><br><span class="line">val groupRdd: RDD[(Int, Iterable[Int])] = listRdd.groupBy(i =&gt; i % <span class="number">2</span>)</span><br><span class="line">groupRdd.collect().foreach(println)</span><br><span class="line">--------------------</span><br><span class="line">(<span class="number">0</span>,CompactBuffer(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">(<span class="number">1</span>,CompactBuffer(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>))</span><br></pre></td></tr></table></figure><h5 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h5><p>按条件筛选</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">9</span>)</span><br><span class="line">val filterRdd: RDD[Int] = listRdd.filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">filterRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h5><p>抽样。</p><p>参数介绍：withReplacement，是否重复抽样（可重复，泊松抽样；不可重复，伯努利抽样）</p><p>fraction，打分？（可重复下，需≥0，代表大概可重复的次数；不可重复下，需[0,1]，代表大概抽取比例）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="comment">// val sampleRdd: RDD[Int] = listRdd.sample(false, 0.7, 333)</span></span><br><span class="line">val sampleRdd: RDD[Int] = listRdd.sample(<span class="keyword">true</span>, <span class="number">4</span>, <span class="number">333</span>)</span><br><span class="line">sampleRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h5><p>去重</p><p>注意：distinct 计算后，原数据分区会被打乱，是因为中间进行了 shuffle 操作。同时也因为 shuffle 导致必须等待所有分区都计算完成后才能进行下一个操作；而没有 shuffle 操作的算子，执行完一个分区的操作后就可以继续进行下一个操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">val disRdd: RDD[Int] = listRdd.distinct()</span><br><span class="line">val disRdd: RDD[Int] = listRdd.distinct(<span class="number">2</span>)  <span class="comment">// 设置去重后的分区数</span></span><br><span class="line">disRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h5 id="coalease"><a href="#coalease" class="headerlink" title="coalease"></a>coalease</h5><p>缩减分区。实际为合并分区，即将其中某几个分区合并；若要扩大分区，需要添加 shuffle 参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(<span class="number">1</span> to <span class="number">16</span>, <span class="number">4</span>)</span><br><span class="line">println(<span class="string">"before: "</span>, listRdd.partitions.size)</span><br><span class="line">val coalRdd: RDD[Int] = listRdd.coalesce(<span class="number">3</span>)</span><br><span class="line">println(<span class="string">"after: "</span>, coalRdd.partitions.size)</span><br></pre></td></tr></table></figure><h5 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h5><p>对 coalease 的封装，<code>shuffle = true</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.repartition(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h5><p>排序，可自己设置排序规则</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val listRdd: RDD[Int] = sc.makeRDD(List(<span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">2</span>))</span><br><span class="line">val sortRdd: RDD[Int] = listRdd.sortBy(x =&gt; x)</span><br><span class="line"><span class="comment">// val sortRdd: RDD[Int] = listRdd.sortBy(x =&gt; x%3)</span></span><br><span class="line">sortRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure><h4 id="双-Value-类型"><a href="#双-Value-类型" class="headerlink" title="双 Value 类型"></a>双 Value 类型</h4><h5 id="union"><a href="#union" class="headerlink" title="union"></a>union</h5><p>合并两个 Rdd</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rdd3 = rdd1.union(rdd2)</span><br></pre></td></tr></table></figure><h5 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h5><p>去除相同元素，不同的会保留</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rdd3 = rdd1.subtract(rdd2)</span><br></pre></td></tr></table></figure><h5 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h5><p>求交集后返回</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rdd3 = rdd1.intersection(rdd2)</span><br></pre></td></tr></table></figure><h5 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h5><p>笛卡尔积</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rdd3 = rdd1.cartesian(rdd2)</span><br></pre></td></tr></table></figure><h5 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h5><p>将两个 rdd 对应元素组合在一起（tuple？key-value？）。两个 rdd 分区数量和元素数量必须都相同；会把分区中的拆成一个一个的元素，组合的元素还在原来的分区里。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1: RDD[Int] = sc.makeRDD(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line">val rdd2: RDD[String] = sc.makeRDD(Array(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>), <span class="number">2</span>)</span><br><span class="line">val zipRdd: RDD[(Int, String)] = rdd1.zip(rdd2)</span><br><span class="line">zipRdd.collect().foreach(println)</span><br><span class="line">zipRdd.saveAsTextFile(<span class="string">"output"</span>)</span><br></pre></td></tr></table></figure><h4 id="Key-Value-类型"><a href="#Key-Value-类型" class="headerlink" title="Key-Value 类型"></a>Key-Value 类型</h4><h5 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h5><p>根据 key 进行重新分区（因此 rdd 需要是 kv 的形式），也可自定义分区类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val arrayRdd: RDD[(Int, String)] = sc.makeRDD(Array((<span class="number">1</span>, <span class="string">"aaa"</span>), (<span class="number">2</span>, <span class="string">"bbb"</span>), (<span class="number">3</span>, <span class="string">"ccc"</span>), (<span class="number">4</span>, <span class="string">"ddd"</span>)), <span class="number">2</span>)</span><br><span class="line">val parRdd: RDD[(Int, String)] = arrayRdd.partitionBy(<span class="keyword">new</span> org.apache.spark.HashPartitioner(<span class="number">3</span>))</span><br><span class="line">parRdd.saveAsTextFile(<span class="string">"output"</span>)</span><br></pre></td></tr></table></figure><h3 id="Rdd-Action-行动算子"><a href="#Rdd-Action-行动算子" class="headerlink" title="Rdd Action 行动算子"></a>Rdd Action 行动算子</h3><h3 id="综合练习"><a href="#综合练习" class="headerlink" title="综合练习"></a>综合练习</h3><h2 id="二、SparkSQL"><a href="#二、SparkSQL" class="headerlink" title="二、SparkSQL"></a>二、SparkSQL</h2><p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。</p><p>Rdd → DataFrame → DataSet</p><ul><li>DataFrame：在 Rdd 的基础上，装饰了表结构，让每一个字段包含意义</li><li>DataSet：在 DataFrame 基础上，装饰了读取操作，让数据的读取像操作对象一样简单</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell</span><br></pre></td></tr></table></figure><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><h4 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; val df = spark.read.json(<span class="string">"/opt/module/spark-2.3.2-local/mydata/user.json"</span>)</span><br><span class="line">&gt; df.show</span><br><span class="line">==========</span><br><span class="line"><span class="comment"># user.json</span></span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"123"</span>, <span class="string">"age"</span>:20&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"456"</span>, <span class="string">"age"</span>:20&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"789"</span>, <span class="string">"age"</span>:20&#125;</span><br></pre></td></tr></table></figure><h4 id="SQL-风格语法"><a href="#SQL-风格语法" class="headerlink" title="SQL 风格语法"></a>SQL 风格语法</h4><ul><li>单个 Session 内 View 可见</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; df.createTempView(<span class="string">"user"</span>)</span><br><span class="line">&gt; spark.sql(<span class="string">"select * from user"</span>).show()</span><br></pre></td></tr></table></figure><ul><li>创建全局表</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; df.createGlobalTempView(<span class="string">"user_g"</span>)</span><br><span class="line">&gt; spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.user_g"</span>).show()</span><br></pre></td></tr></table></figure><h4 id="DSL-风格语法"><a href="#DSL-风格语法" class="headerlink" title="DSL 风格语法"></a>DSL 风格语法</h4><p>以对象的方式来操作数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; df.select(<span class="string">"name"</span>).show()</span><br><span class="line">&gt; df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line">&gt; df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line">&gt; df.groupBy(<span class="string">"age"</span>).count().show()</span><br></pre></td></tr></table></figure><h4 id="Rdd-转为-DataFrame"><a href="#Rdd-转为-DataFrame" class="headerlink" title="Rdd 转为 DataFrame"></a>Rdd 转为 DataFrame</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">val</span> <span class="title">rdd1</span> </span>= sc.makeRDD(<span class="type">List</span>((<span class="string">"zhangsan"</span>, <span class="number">20</span>), (<span class="string">"lisi"</span>, <span class="number">14</span>)))</span><br><span class="line">&gt; <span class="keyword">val</span> peopleRdd = rdd1.map(t=&gt;&#123;<span class="type">People</span>(t._1, t._2)&#125;)</span><br><span class="line">&gt; <span class="keyword">val</span> df = peopleRdd.toDF</span><br><span class="line">&gt; df.show</span><br></pre></td></tr></table></figure><h4 id="DataFrame-转为-Rdd"><a href="#DataFrame-转为-Rdd" class="headerlink" title="DataFrame 转为 Rdd"></a>DataFrame 转为 Rdd</h4><p>注意这里面转换之后，并不会还原成 People 结构，而只是一个 Row 对象。这是因为 DataFrame 本身不存数据的类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; df.rdd</span><br></pre></td></tr></table></figure><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>Dataset是具有强类型的数据集合，需要提供对应的类型信息。</p><p>解决 DataFrame 中取数只能通过下标来取的问题（啥意思？？）</p><h4 id="创建-1"><a href="#创建-1" class="headerlink" title="创建"></a>创建</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">People</span>(<span class="string">"Andy"</span>, <span class="number">21</span>)).toDS()</span><br></pre></td></tr></table></figure><h4 id="Rdd-转换为-DataSet"><a href="#Rdd-转换为-DataSet" class="headerlink" title="Rdd 转换为 DataSet"></a>Rdd 转换为 DataSet</h4><p>Rdd + 结构 → DataFrame；DataFram + 类型 → DataSet</p><p>Rdd + 结构 + 类型 → DataSet</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">val</span> <span class="title">mapRdd</span> </span>= rdd.map(t=&gt;&#123;<span class="type">Person</span>(t._1, t._2)&#125;)</span><br><span class="line">&gt; <span class="keyword">val</span> ds = mapRdd.toDS</span><br><span class="line">&gt; ds.show</span><br></pre></td></tr></table></figure><h4 id="DataSet-转换为-Rdd"><a href="#DataSet-转换为-Rdd" class="headerlink" title="DataSet 转换为 Rdd"></a>DataSet 转换为 Rdd</h4><p>转换回来仍保留着类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ds.rdd</span><br></pre></td></tr></table></figure><h2 id="三、SparkStreaming"><a href="#三、SparkStreaming" class="headerlink" title="三、SparkStreaming"></a>三、SparkStreaming</h2>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark, 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive 常用操作 &amp; 练习</title>
      <link href="/2020/10/21/Hadoop/Hive%20%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%20&amp;%20%E7%BB%83%E4%B9%A0/"/>
      <url>/2020/10/21/Hadoop/Hive%20%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%20&amp;%20%E7%BB%83%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="一、常用操作"><a href="#一、常用操作" class="headerlink" title="一、常用操作"></a>一、常用操作</h2><h2 id="二、练习"><a href="#二、练习" class="headerlink" title="二、练习"></a>二、练习</h2>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> 练习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>console 日志输出到文件</title>
      <link href="/2020/10/09/Linux/console%20%E6%97%A5%E5%BF%97%E8%BE%93%E5%87%BA%E5%88%B0%E6%96%87%E4%BB%B6/"/>
      <url>/2020/10/09/Linux/console%20%E6%97%A5%E5%BF%97%E8%BE%93%E5%87%BA%E5%88%B0%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">some_command 2&gt;&amp;1 | tee output.txt</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>在Linux中，如果想将一个程序在控制台中的输出字符输出到文件中，不保留控制台内的文字，可以用下面命令：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">some_command &gt; output.txt</span><br></pre></td></tr></table></figure><ul><li><p>命令结果会输出到<code>output.txt</code>中，换成<code>&gt;&gt;</code>可以追加到文件末尾</p></li><li><p>但如果想输出到文件同时，保留控制台的内容，需要使用tee命令，示例如下：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">some_command | tee output.txt</span><br></pre></td></tr></table></figure><ul><li>有时会发现上述命令后屏幕有输出，但文件内容为空，此时可能是由于some_command输出的字符从std error文件描述符输出，需要先将std error的输出导向到std output：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">some_command 2&gt;&amp;1 | tee output.txt</span><br></pre></td></tr></table></figure><p>其中，2代表std error，1代表std output，&gt;&amp;是linux中fd到fd的重定向操作符。</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot 集成 Prometheus</title>
      <link href="/2020/09/23/Spring/SpringBoot%E9%9B%86%E6%88%90Prometheus/"/>
      <url>/2020/09/23/Spring/SpringBoot%E9%9B%86%E6%88%90Prometheus/</url>
      
        <content type="html"><![CDATA[<h2 id="一、添加依赖"><a href="#一、添加依赖" class="headerlink" title="一、添加依赖"></a>一、添加依赖</h2><ul><li>Maven <code>pom.xml</code></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--  第一条必须加，否则会导致 Could not autowire. No beans of 'xxxx' type found 的错误  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-actuator<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.micrometer<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>micrometer-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.micrometer<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>micrometer-registry-prometheus<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>Gradle <code>build.gradle</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">implementation &apos;org.springframework.boot:spring-boot-starter-actuator&apos;</span><br><span class="line">compile &apos;io.micrometer:micrometer-registry-prometheus&apos;</span><br><span class="line">compile &apos;io.micrometer:micrometer-core&apos;</span><br></pre></td></tr></table></figure><ul><li>打开 Prometheus 监控接口 <code>application.properties</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server.port=8088</span><br><span class="line">spring.application.name=springboot2-prometheus</span><br><span class="line">management.endpoints.web.exposure.include=*</span><br><span class="line">management.metrics.tags.application=$&#123;spring.application.name&#125;</span><br></pre></td></tr></table></figure><p>可以直接运行程序，访问<code>http://localhost:8088/actuator/prometheus</code>可以看到下面的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># HELP jvm_buffer_total_capacity_bytes An estimate of the total capacity of the buffers in this pool</span><br><span class="line"># TYPE jvm_buffer_total_capacity_bytes gauge</span><br><span class="line">jvm_buffer_total_capacity_bytes&#123;id=&quot;direct&quot;,&#125; 90112.0</span><br><span class="line">jvm_buffer_total_capacity_bytes&#123;id=&quot;mapped&quot;,&#125; 0.0</span><br><span class="line"># HELP tomcat_sessions_expired_sessions_total  </span><br><span class="line"># TYPE tomcat_sessions_expired_sessions_total counter</span><br><span class="line">tomcat_sessions_expired_sessions_total 0.0</span><br><span class="line"># HELP jvm_classes_unloaded_classes_total The total number of classes unloaded since the Java virtual machine has started execution</span><br><span class="line"># TYPE jvm_classes_unloaded_classes_total counter</span><br><span class="line">jvm_classes_unloaded_classes_total 1.0</span><br><span class="line"># HELP jvm_buffer_count_buffers An estimate of the number of buffers in the pool</span><br><span class="line"># TYPE jvm_buffer_count_buffers gauge</span><br><span class="line">jvm_buffer_count_buffers&#123;id=&quot;direct&quot;,&#125; 11.0</span><br><span class="line">jvm_buffer_count_buffers&#123;id=&quot;mapped&quot;,&#125; 0.0</span><br><span class="line"># HELP system_cpu_usage The &quot;recent cpu usage&quot; for the whole system</span><br><span class="line"># TYPE system_cpu_usage gauge</span><br><span class="line">system_cpu_usage 0.0939447637893599</span><br><span class="line"># HELP jvm_gc_max_data_size_bytes Max size of old generation memory pool</span><br><span class="line"># TYPE jvm_gc_max_data_size_bytes gauge</span><br><span class="line">jvm_gc_max_data_size_bytes 2.841116672E9</span><br><span class="line"></span><br><span class="line"># 此处省略超多字...</span><br></pre></td></tr></table></figure><h2 id="二、Prometheus-安装与配置"><a href="#二、Prometheus-安装与配置" class="headerlink" title="二、Prometheus 安装与配置"></a>二、Prometheus 安装与配置</h2><p>使用 docker 运行 Prometheus（仅初始测试）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name prometheus -d -p 9090:9090 prom/prometheus:latest</span><br></pre></td></tr></table></figure><p>写配置文件<code>prometheus.yml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my global config</span></span><br><span class="line">global:</span><br><span class="line">  scrape_interval:     15s <span class="comment"># Set the scrape interval to every 15 seconds. Default is every 1 minute.</span></span><br><span class="line">  evaluation_interval: 15s <span class="comment"># Evaluate rules every 15 seconds. The default is every 1 minute.</span></span><br><span class="line">  <span class="comment"># scrape_timeout is set to the global default (10s).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load rules once and periodically evaluate them according to the global 'evaluation_interval'.</span></span><br><span class="line">rule_files:</span><br><span class="line">  <span class="comment"># - "first_rules.yml"</span></span><br><span class="line">  <span class="comment"># - "second_rules.yml"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A scrape configuration containing exactly one endpoint to scrape:</span></span><br><span class="line"><span class="comment"># Here it's Prometheus itself.</span></span><br><span class="line">scrape_configs:</span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  - job_name: <span class="string">'prometheus'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># metrics_path defaults to '/metrics'</span></span><br><span class="line">    <span class="comment"># scheme defaults to 'http'.</span></span><br><span class="line"></span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [<span class="string">'localhost:9090'</span>]</span><br><span class="line">  <span class="comment"># demo job</span></span><br><span class="line">  -  job_name: <span class="string">'springboot-actuator-prometheus-test'</span> <span class="comment"># job name</span></span><br><span class="line">     metrics_path: <span class="string">'/actuator/prometheus'</span> <span class="comment"># 指标获取路径</span></span><br><span class="line">     scrape_interval: 5s <span class="comment"># 间隔</span></span><br><span class="line">     basic_auth: <span class="comment"># Spring Security basic auth </span></span><br><span class="line">       username: <span class="string">'actuator'</span></span><br><span class="line">       password: <span class="string">'actuator'</span></span><br><span class="line">     static_configs:</span><br><span class="line">     - targets: [<span class="string">'docker.for.mac.localhost:18080'</span>] <span class="comment"># 实例的地址，默认的协议是http （这里开始有问题，直接写 localhost 是访问容器内的地址，而不是宿主机的。可通过在网页上方 status -&gt; targets 查看对应的服务情况</span></span><br></pre></td></tr></table></figure><p>运行 docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 9090:9090 -v $(<span class="built_in">pwd</span>)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml</span><br></pre></td></tr></table></figure><p>访问 <code>http://localhost:9090</code>，可看到如下界面</p><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/16fcafb94a6bf392.jpeg" alt="img"></p><ul><li>点击 <code>Insert metric at cursor</code> ，即可选择监控指标；点击 <code>Graph</code> ，即可让指标以图表方式展示；点击<code>Execute</code> 按钮，即可看到指标图</li></ul><h2 id="三、Grafana-安装和配置"><a href="#三、Grafana-安装和配置" class="headerlink" title="三、Grafana 安装和配置"></a>三、Grafana 安装和配置</h2><p>1、启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name=grafana -p 3000:3000 grafana/grafana</span><br></pre></td></tr></table></figure><p>2、登录</p><p>访问 <code>http://localhost:3000/login</code> ，初始账号/密码为：<code>admin/admin</code> </p><p>3、配置数据源</p><ul><li>点击左侧齿轮<code>Configuration</code>中<code>Add Data Source</code>，会看到如下界面：</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20200924110615218.png" alt="image-20200924110615218" style="zoom: 25%;"><ul><li>这里我们选择Prometheus 当做数据源，这里我们就配置一下Prometheus 的访问地址，点击 <code>Save &amp; Test</code></li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20200924110807215.png" alt="image-20200924110807215" style="zoom:25%;"><p>4、创建监控 Dashboard</p><ul><li>点击导航栏上的 <code>+</code> 按钮，并点击Dashboard，将会看到类似如下的界面</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20200924110949259.png" alt="image-20200924110949259" style="zoom:25%;"><ul><li>点击<code>+ Add new panel</code></li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20200924111151204.png" alt="image-20200924111151204" style="zoom:25%;"><h2 id="四、自定义监控指标"><a href="#四、自定义监控指标" class="headerlink" title="四、自定义监控指标"></a>四、自定义监控指标</h2><p>1、创建 Prometheus 监控管理类<code>PrometheusCustomMonitor</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io.micrometer.core.instrument.Counter;</span><br><span class="line"><span class="keyword">import</span> io.micrometer.core.instrument.DistributionSummary;</span><br><span class="line"><span class="keyword">import</span> io.micrometer.core.instrument.MeterRegistry;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.annotation.PostConstruct;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PrometheusCustomMonitor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Counter requestErrorCount;</span><br><span class="line">    <span class="keyword">private</span> Counter orderCount;</span><br><span class="line">    <span class="keyword">private</span> DistributionSummary amountSum;</span><br><span class="line">    <span class="keyword">private</span> AtomicInteger failCaseNum;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> MeterRegistry registry;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PrometheusCustomMonitor</span><span class="params">(MeterRegistry registry)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.registry = registry;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostConstruct</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        requestErrorCount = registry.counter(<span class="string">"requests_error_total"</span>, <span class="string">"status"</span>, <span class="string">"error"</span>);</span><br><span class="line">        orderCount = registry.counter(<span class="string">"order_request_count"</span>, <span class="string">"order"</span>, <span class="string">"test-svc"</span>);</span><br><span class="line">        amountSum = registry.summary(<span class="string">"order_amount_sum"</span>, <span class="string">"orderAmount"</span>, <span class="string">"test-svc"</span>);</span><br><span class="line">        failCaseNum = registry.gauge(<span class="string">"fail_case_num"</span>, <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Counter <span class="title">getRequestErrorCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> requestErrorCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Counter <span class="title">getOrderCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> orderCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DistributionSummary <span class="title">getAmountSum</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> amountSum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AtomicInteger <span class="title">getFailCaseNum</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> failCaseNum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2、新增<code>/order</code>接口</p><p>当 <code>flag=&quot;1&quot;</code>时，抛异常，模拟下单失败情况。在接口中统计<code>order_request_count</code>和<code>order_amount_sum</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.annotation.Resource;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> PrometheusCustomMonitor monitor;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/order"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">order</span><span class="params">(@RequestParam(defaultValue = <span class="string">"0"</span>)</span> String flag) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 统计下单次数</span></span><br><span class="line">        monitor.getOrderCount().increment();</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"1"</span>.equals(flag)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"出错啦"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();</span><br><span class="line">        <span class="keyword">int</span> amount = random.nextInt(<span class="number">100</span>);</span><br><span class="line">        <span class="comment">// 统计金额</span></span><br><span class="line">        monitor.getAmountSum().record(amount);</span><br><span class="line"></span><br><span class="line">        monitor.getFailCaseNum().set(amount);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"下单成功, 金额: "</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3、新增全局异常处理器<code>GlobalExceptionHandler</code></p><p>统计下单失败次数<code>requests_error_total</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.ControllerAdvice;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.ExceptionHandler;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.ResponseBody;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.annotation.Resource;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ControllerAdvice</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalExceptionHandler</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> PrometheusCustomMonitor monitor;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@ResponseBody</span></span><br><span class="line">    <span class="meta">@ExceptionHandler</span>(value = Exception.class)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">handle</span><span class="params">(Exception e)</span> </span>&#123;</span><br><span class="line">        monitor.getRequestErrorCount().increment();</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"error, message: "</span> + e.getMessage();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>4、测试</p><p>启动项目，访问<code>http://localhost:8080/order</code>和<code>http://localhost:8080/order?flag=1</code>模拟下单成功和失败的情况，然后我们访问<code>http://localhost:8080/actuator/prometheus</code>，可以看到我们自定义指标已经被 <code>/prometheus</code> 端点暴露出来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># HELP requests_error_total  </span><br><span class="line"># TYPE requests_error_total counter</span><br><span class="line">requests_error_total&#123;application=&quot;springboot-actuator-prometheus-test&quot;,status=&quot;error&quot;,&#125; 41.0</span><br><span class="line"># HELP order_request_count_total  </span><br><span class="line"># TYPE order_request_count_total counter</span><br><span class="line">order_request_count_total&#123;application=&quot;springboot-actuator-prometheus-test&quot;,order=&quot;test-svc&quot;,&#125; 94.0</span><br><span class="line"># HELP order_amount_sum  </span><br><span class="line"># TYPE order_amount_sum summary</span><br><span class="line">order_amount_sum_count&#123;application=&quot;springboot-actuator-prometheus-test&quot;,orderAmount=&quot;test-svc&quot;,&#125; 53.0</span><br><span class="line">order_amount_sum_sum&#123;application=&quot;springboot-actuator-prometheus-test&quot;,orderAmount=&quot;test-svc&quot;,&#125; 2701.0</span><br></pre></td></tr></table></figure><p>5、使用 Prometheus 监控</p><p>重新运行 docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 9090:9090 -v $(<span class="built_in">pwd</span>)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml</span><br></pre></td></tr></table></figure><p>选择对应指标后可以看到数据变化</p><p>6、使用 Grafana 展示</p><p>在 Dashboard 界面选择对应的监控指标即可</p><hr><p>参考资料：</p><p><a href="https://prometheus.io/docs/concepts/metric_types/" target="_blank" rel="noopener">Metric types | Prometheus</a></p><p><a href="https://blog.csdn.net/typa01_kk/article/details/76696618" target="_blank" rel="noopener">IntelliJ IDEA创建第一个Spring Boot项目_Study Notes-CSDN博客</a></p><p><a href="https://cloud.tencent.com/developer/article/1508319" target="_blank" rel="noopener">Spring Boot 使用 Micrometer 集成 Prometheus 监控 Java 应用性能 【springboot 2.0】</a></p><p><a href="https://micrometer.io/docs/concepts" target="_blank" rel="noopener">Micrometer Application Monitoring【官方文档】</a></p><p><a href="https://juejin.im/post/6844904052417904653" target="_blank" rel="noopener">Spring Boot 微服务应用集成Prometheus + Grafana 实现监控告警 ★</a></p><p><a href="https://blog.kubernauts.io/https-blog-kubernauts-io-monitoring-java-spring-boot-applications-with-prometheus-part-1-c0512f2acd7b" target="_blank" rel="noopener">Monitoring Java Spring Boot applications with Prometheus: Part 1 | by Arush Salil | Kubernauts 【放弃这个教程？】client java 不支持 springboot 2.x，最高支持 1.5</a></p><p><a href="https://segmentfault.com/a/1190000015309478" target="_blank" rel="noopener">Spring Boot 参考指南（端点）_风继续吹 - SegmentFault 思否</a></p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot, Prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>虚拟机 Hadoop 环境配置</title>
      <link href="/2020/09/17/Hadoop/%E8%99%9A%E6%8B%9F%E6%9C%BAHadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
      <url>/2020/09/17/Hadoop/%E8%99%9A%E6%8B%9F%E6%9C%BAHadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="一、安装-CentOS-6-amp-基本配置"><a href="#一、安装-CentOS-6-amp-基本配置" class="headerlink" title="一、安装 CentOS 6 &amp; 基本配置"></a>一、安装 CentOS 6 &amp; 基本配置</h2><blockquote><p>win10通过VMware安装CentOS6.5 - 简书<br><a href="https://www.jianshu.com/p/9d5b9757a1ef" target="_blank" rel="noopener">https://www.jianshu.com/p/9d5b9757a1ef</a></p></blockquote><p>1、关闭防火墙</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p>2、创建普通用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd Ace</span><br><span class="line">passwd 123</span><br></pre></td></tr></table></figure><p>3、创建软件存储文件夹，并更改所有权</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /opt/software /opt/module</span><br><span class="line">chown Ace:Ace /opt/software /opt/module</span><br></pre></td></tr></table></figure><p>4、用户添加到 sudoers</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span><br><span class="line">Ace ALL=(ALL) NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>5、改 Hosts</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">for</span> ((i=101;i&lt;105;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"192.168.87.<span class="variable">$i</span> hadoop<span class="variable">$i</span>"</span> &gt;&gt; /etc/hosts</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>6、改静态 ip</p><p><code>vim /etc/sysconfig/network-scripts/ifcfg-eth0</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=eth0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=192.168.87.100</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.87.2</span><br><span class="line">DNS1=192.168.87.2</span><br><span class="line">NAME=<span class="string">"System eth0"</span></span><br></pre></td></tr></table></figure><p>【创建新虚拟机，下面的都要做一遍，可以写脚本解决（看下面，推荐）】</p><p>6、改 ip 地址（同上6）</p><p><code>vim /etc/sysconfig/network-scripts/ifcfg-eth0</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IPADDR=192.168.87.100   <span class="comment"># 改成对应的</span></span><br></pre></td></tr></table></figure><p>7、改主机名</p><p><code>vim /etc/sysconfig/network</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HOSTNAME=hadoopxxx</span><br></pre></td></tr></table></figure><p>8、删除多余网卡</p><p><code>vim /etc/udev/rules.d/70-persistent-net.rules</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一行删掉（只保留一个网卡就行，注释也删掉，手动删的话记得和后面脚本的行数要对应上）</span></span><br><span class="line"><span class="comment"># 第二行最后 NAME="eth1" 改为 NAME="eth0"</span></span><br></pre></td></tr></table></figure><p>9、拍快照，克隆</p><p>【脚本】</p><ul><li>分发脚本 xsync</li></ul><p><code>vim xsync</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span> ((pcount==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`basename <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line"></span><br><span class="line">dirname=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> dirname=<span class="variable">$dirname</span></span><br><span class="line"></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>((i=102;i&lt;105;i++)); <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"------------- hadoop<span class="variable">$i</span> ------------"</span></span><br><span class="line">    rsync -avlP <span class="variable">$dirname</span>/<span class="variable">$fname</span> hadoop<span class="variable">$i</span>:<span class="variable">$dirname</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>移动到<code>bin</code>目录下，<code>sudo mv xsync /bin</code></p><p>安装<code>rsync</code>，<code>sudo yum install -y rsync</code></p><p>改权限，<code>chmod +x xsync</code></p><ul><li>执行相同命令脚本</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span> ((pcount==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"==== <span class="variable">$i</span> <span class="variable">$1</span> ===="</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>移动到<code>/bin</code>，改权限</p><ul><li>自动配置网络脚本</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">id=<span class="variable">$1</span></span><br><span class="line"><span class="comment"># sed -i "s/before replace/after replace/"</span></span><br><span class="line">sudo sed -i <span class="string">"s/192.168.87.101/192.168.87.<span class="variable">$id</span>/"</span> /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">sudo sed -i <span class="string">"s/hadoop101/hadoop<span class="variable">$id</span>/"</span> /etc/sysconfig/network</span><br><span class="line"></span><br><span class="line">file=/etc/udev/rules.d/70-persistent-net.rules</span><br><span class="line"></span><br><span class="line"><span class="comment"># count "SUBSYSTEM" word number</span></span><br><span class="line">nu=$(grep -c SUBSYSTEM <span class="variable">$file</span>)</span><br><span class="line"><span class="keyword">if</span>((<span class="variable">$nu</span> &gt; 1));</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">        <span class="comment"># delete 8th line</span></span><br><span class="line">        sed -i <span class="string">'8d'</span> <span class="variable">$file</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/eth1/eth0/'</span> <span class="variable">$file</span></span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><p>改权限，<code>chmod +x change_network</code></p><h2 id="二、安装-JAVA-和-Hadoop"><a href="#二、安装-JAVA-和-Hadoop" class="headerlink" title="二、安装 JAVA 和 Hadoop"></a>二、安装 JAVA 和 Hadoop</h2><p>1、下载/上传 java 和 hadoop 的包到 <code>/opt/software</code></p><p>2、解压 java 和 hadoop 到 <code>/opt/module</code></p><p>3、安装（配置环境变量）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"><span class="comment"># 在末尾添加</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>4、测试安装情况</p><p>执行下面命令后，能出现版本号即为成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">$ hadoop version</span><br></pre></td></tr></table></figure><p>5、使用 xsync 同步到多个机器上</p><h2 id="三、Hadoop-配置"><a href="#三、Hadoop-配置" class="headerlink" title="三、Hadoop 配置"></a>三、Hadoop 配置</h2><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Apache Hadoop 3.2.1 – Hadoop: Setting up a Single Node Cluster.</a></p><ul><li>安装插件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ssh</span><br><span class="line">$ sudo yum install pdsh</span><br><span class="line">  </span><br><span class="line"><span class="comment"># pdsh 可能默认找不到</span></span><br><span class="line">$ wget http://mirrors.mit.edu/epel/6/i386/epel-release-6-8.noarch.rpm</span><br><span class="line">$ rpm -Uvh epel-release-6-8.noarch.rpm</span><br><span class="line">$ yum install pdsh</span><br></pre></td></tr></table></figure><ul><li>环境配置</li></ul><p><code>vim etc/hadoop/hadoop-env.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/your-java-home-path</span><br></pre></td></tr></table></figure><h3 id="3-1-本地运行模式"><a href="#3-1-本地运行模式" class="headerlink" title="3.1 本地运行模式"></a>3.1 本地运行模式</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir input</span><br><span class="line">$ cp etc/hadoop/*.xml input</span><br><span class="line">$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output <span class="string">'dfs[a-z.]+'</span></span><br><span class="line">$ cat output/*</span><br></pre></td></tr></table></figure><h3 id="3-2-伪分布式"><a href="#3-2-伪分布式" class="headerlink" title="3.2 伪分布式"></a>3.2 伪分布式</h3><p>1、配置</p><p><code>vim etc/hadoop/core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>etc/hadoop/hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>暂时不配置 yarn 了</strong></p><p><code>etc/hadoop/yarn-env.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>2、设置免密码登录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span><br><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>3、执行</p><ol><li><p>Format the filesystem:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs namenode -format</span><br></pre></td></tr></table></figure></li><li><p>Start NameNode daemon and DataNode daemon:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>The hadoop daemon log output is written to the <code>$HADOOP_LOG_DIR</code> directory (defaults to <code>$HADOOP_HOME/logs</code>).</p></li><li><p>Browse the web interface for the NameNode; by default it is available at:</p><ul><li>NameNode - <code>http://localhost:9870/</code></li></ul></li><li><p>Make the HDFS directories required to execute MapReduce jobs:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -mkdir /user</span><br><span class="line">$ bin/hdfs dfs -mkdir /user/&lt;username&gt;</span><br></pre></td></tr></table></figure></li><li><p>Copy the input files into the distributed filesystem:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -mkdir input</span><br><span class="line">$ bin/hdfs dfs -put etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure></li><li><p>Run some of the examples provided:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos;</span><br></pre></td></tr></table></figure></li><li><p>Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -get output output</span><br><span class="line">$ cat output/*</span><br></pre></td></tr></table></figure><p>or View the output files on the distributed filesystem:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -cat output/*</span><br></pre></td></tr></table></figure></li><li><p>When you’re done, stop the daemons with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-3-完全分布式"><a href="#3-3-完全分布式" class="headerlink" title="3.3 完全分布式"></a>3.3 完全分布式</h3><p>同步两个软件，/etc/profile</p><h4 id="3-3-1-集群配置"><a href="#3-3-1-集群配置" class="headerlink" title="3.3.1 集群配置"></a>3.3.1 集群配置</h4><ul><li>集群部署规划</li></ul><p>NN 1个； 2NN 1个；RM 1个；DN 3个、NM 3个 —— 最少共需六台机器，但是开不起那么多个虚拟机，因此按下表进行合并配置</p><table><thead><tr><th></th><th>hadoop102</th><th>hadoop103</th><th>hadoop104</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode, DataNode</td><td>DataNode</td><td>SecondaryNameNode, DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager, NodeManager</td><td>NodeManager</td></tr></tbody></table><ul><li>配置集群</li></ul><p><strong>1）核心配置文件</strong></p><p>配置<code>etc/hadoop/core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2）HDFS配置文件</strong></p><p>配置``etc/hadoop/hadoop-env.sh`</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>配置<code>etc/hadoop/hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>3）YARN配置文件</strong></p><p>配置<code>etc/hadoop/yarn-env.sh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>配置<code>etc/hadoop/yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>4）MapReduce配置文件</strong></p><p>配置<code>etc/hadoop/mapred-env.sh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>配置<code>etc/hadoop/mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">~</span><br><span class="line">~</span><br><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">~</span><br><span class="line">~</span><br></pre></td></tr></table></figure><ul><li>在集群上分发配置好的Hadoop配置文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ xsync /opt/module/hadoop-2.7.2/</span><br></pre></td></tr></table></figure><h4 id="3-3-2-集群单点启动"><a href="#3-3-2-集群单点启动" class="headerlink" title="3.3.2 集群单点启动"></a>3.3.2 集群单点启动</h4><p>1）如果集群是第一次启动，需要格式化NameNode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p>2）在 hadoop102 上启动 NameNode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>3）在 hadoop102、hadoop103、hadoop104 上<strong>分别</strong>启动DataNode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure><p>4）在 hadoop104 上启动 secondarynamenode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop-daemon.sh start secondarynamenode</span><br></pre></td></tr></table></figure><p>5）查看服务启动情况 jps</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop102]$ jps</span><br><span class="line">4182 Jps</span><br><span class="line">2842 DataNode</span><br><span class="line">2747 NameNode</span><br><span class="line"></span><br><span class="line">[hadoop103]$ jps</span><br><span class="line">1712 DataNode</span><br><span class="line">2215 Jps</span><br><span class="line"></span><br><span class="line">[hadoop104]$ jps</span><br><span class="line">1680 DataNode</span><br><span class="line">2266 Jps</span><br><span class="line">2171 SecondaryNameNode</span><br></pre></td></tr></table></figure><h4 id="3-3-3-集群一键启动"><a href="#3-3-3-集群一键启动" class="headerlink" title="3.3.3 集群一键启动"></a>3.3.3 集群一键启动</h4><ul><li>配置机器间 ssh 无密登录</li></ul><p>「方法1：共用一个秘钥」</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成秘钥</span></span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line"><span class="comment"># 将秘钥添加到本机的 authorized_keys 中，实现本机无密登录</span></span><br><span class="line">$ ssh-copy-id hadoop102</span><br><span class="line"><span class="comment"># 共享同一个秘钥，实现集群机器间无密登录</span></span><br><span class="line">$ xsync ~/.ssh</span><br></pre></td></tr></table></figure><p>「方法2：每个机器单独生成秘钥」</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在每个机器上生成秘钥（每个机器执行一遍）</span></span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line"><span class="comment"># 把每个机器的秘钥分发到别的机器上（每个机器执行一遍）</span></span><br><span class="line">$ ssh-copy-id hadoop102</span><br><span class="line">$ ssh-copy-id hadoop103</span><br><span class="line">$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure><ul><li>添加机器名，配置<code>etc/hadoop/slaves</code>，并同步到其他机器（xsync）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><ul><li>启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop102]$ start-dfs.sh</span><br><span class="line">[hadoop103]$ start-yarn.sh  <span class="comment"># 应该在ResouceManager所在的机器上启动YARN</span></span><br></pre></td></tr></table></figure><ul><li>测试（单词计数）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个文件夹 wcinput，里面放一个文件，添加计数文件中的内容，如：</span></span><br><span class="line">qwe</span><br><span class="line">ddd</span><br><span class="line">smo</span><br><span class="line">123</span><br><span class="line">qwe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这个文件夹上传到 hdfs</span></span><br><span class="line">$ hadoop fs -put wcinput/ /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 MapReduce 程序</span></span><br><span class="line">$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看执行结果</span></span><br><span class="line">[Ace@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 Ace supergroup          0 2020-09-27 10:15 /output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 Ace supergroup         24 2020-09-27 10:15 /output/part-r-00000</span><br><span class="line">[Ace@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /output/part-r-00000</span><br><span class="line">1231</span><br><span class="line">ddd1</span><br><span class="line">qwe2</span><br><span class="line">smo1</span><br></pre></td></tr></table></figure><ul><li>停止</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop102]$ stop-dfs.sh</span><br><span class="line">[hadoop103]$ stop-yarn.sh</span><br></pre></td></tr></table></figure><h4 id="3-3-4-配置历史服务器-amp-日志聚集"><a href="#3-3-4-配置历史服务器-amp-日志聚集" class="headerlink" title="3.3.4 配置历史服务器 &amp; 日志聚集"></a>3.3.4 配置历史服务器 &amp; 日志聚集</h4><ul><li>配置<code>etc/hadoop/mapred-site.xml</code>，添加：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>配置<code>etc/hadoop/yarn-site.xml</code></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>[分发配置]</p></li><li><p>启动</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop102]$ start-dfs.sh</span><br><span class="line">[hadoop103]$ start-yarn.sh</span><br><span class="line">[hadoop104]$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /output/xxx</span><br></pre></td></tr></table></figure><ul><li>查看<ul><li>打开 yarn web <a href="http://hadoop103:8088/" target="_blank" rel="noopener">http://hadoop103:8088/</a></li><li>打开 history，再点 log 就能看到任务具体的日志信息了</li></ul></li></ul><h4 id="3-3-5-集群时间同步"><a href="#3-3-5-集群时间同步" class="headerlink" title="3.3.5 集群时间同步"></a>3.3.5 集群时间同步</h4><p>1）检查 ntp 是否安装</p><p>查看 ntp 包（切换到 root 用户）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]<span class="comment"># rpm -qa | grep ntp</span></span><br><span class="line">ntp-4.2.6p5-15.el6.centos.x86_64</span><br><span class="line">ntpdate-4.2.6p5-15.el6.centos.x86_64</span><br></pre></td></tr></table></figure><p>如果没有这两个服务要安装一下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ntp</span><br></pre></td></tr></table></figure><p>先停止 ntp 服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 ntpd 服务是否在运行</span></span><br><span class="line">$ service ntpd status</span><br><span class="line"><span class="comment"># 如果在运行先关闭</span></span><br><span class="line">$ service ntpd stop</span><br><span class="line">$ chkconfig ntpd off</span><br><span class="line"><span class="comment"># 再查看一下 ntpd 运行情况</span></span><br><span class="line">$ chkconfig --list ntpd</span><br></pre></td></tr></table></figure><p>2）修改ntp配置文件<code>/etc/ntp.conf</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）</span></span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改2（集群在局域网中，不使用其他互联网上的时间），将下面四行注释掉</span></span><br><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><p>3）修改<code>/etc/sysconfig/ntpd</code>文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">增加内容如下（让硬件时间与系统时间一起同步）</span><br><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure><p>4）重新启动ntpd服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ service ntpd status</span><br><span class="line">$ service ntpd start</span><br></pre></td></tr></table></figure><p>5）设置ntpd服务开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop102]$ chkconfig ntpd on</span><br></pre></td></tr></table></figure><p>6）其他机器配置（必须root用户）</p><p>在其他机器配置10分钟与时间服务器同步一次</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ crontab -e</span><br><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop102</span><br></pre></td></tr></table></figure><p>测试（修改任意机器时间），十分钟后查看机器是否与时间服务器同步</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ date -s <span class="string">"2017-9-11 11:11:11"</span></span><br></pre></td></tr></table></figure><p>7）若主机时间不对</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ service ntpd stop</span><br><span class="line">$ ntpdate us.pool.ntp.org</span><br><span class="line">$ service ntpd start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接</span></span><br><span class="line">$ sudo ntpdate -u pool.ntp.org</span><br></pre></td></tr></table></figure><h3 id="3-4-常用端口记录"><a href="#3-4-常用端口记录" class="headerlink" title="3.4 常用端口记录"></a>3.4 常用端口记录</h3><blockquote><p><a href="https://blog.csdn.net/yeruby/article/details/49406073" target="_blank" rel="noopener">Hadoop默认端口应用一览_在路上的学习者-CSDN博客</a></p><p><a href="https://blog.csdn.net/baiBenny/article/details/53887328" target="_blank" rel="noopener">Hadoop常用端口号_baiBenny的博客-CSDN博客</a></p></blockquote><table><thead><tr><th>组件</th><th>节点</th><th>默认端口</th><th>配置</th><th>用途说明</th></tr></thead><tbody><tr><td>HDFS</td><td>DataNode</td><td>50020</td><td>dfs.datanode.ipc.address</td><td>ipc服务的端口</td></tr><tr><td>HDFS</td><td>NameNode</td><td>50070</td><td>dfs.namenode.http-address</td><td>http服务的端口，可查看 HDFS 存储内容</td></tr><tr><td>HDFS</td><td>NameNode</td><td>8020</td><td>fs.defaultFS</td><td>接收Client连接的RPC端口，用于获取文件系统metadata信息</td></tr><tr><td>YARN</td><td>Resource Manager</td><td>8088</td><td>yarn.resourcemanager.webapp.address</td><td>Yarn http服务的端口</td></tr><tr><td>HBase</td><td>Master</td><td>16000</td><td></td><td>Master RPC Port（远程通信调用）</td></tr><tr><td></td><td>Master</td><td>16010</td><td></td><td>Master Web Port</td></tr><tr><td></td><td>Regionserver</td><td>16020</td><td></td><td>Regionserver RPC Port</td></tr><tr><td></td><td>Regionserver</td><td>16030</td><td></td><td>Regionserver Web Port</td></tr><tr><td>Spark</td><td></td><td>4040</td><td></td><td>查看 Spark Job</td></tr></tbody></table><h3 id="3-5-HA"><a href="#3-5-HA" class="headerlink" title="3.5 HA"></a>3.5 HA</h3><ul><li>配置两个 NameNode</li><li>配置 JournalNode 用于将 Active NN 的数据 同步到 Standby NN 上「解决元数据同步的问题」</li><li>配置 Zookeeper，解决主备 NN 切换的问题，防止脑裂</li><li>在 NN 上启动 failoverController（zkfc），作为 Zookeeper 的客户端，实现与 zk 集群的交互和监测</li></ul><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20201110104755190.png" alt="image-20201110104755190" style="zoom: 33%;"><h3 id="3-6-其他"><a href="#3-6-其他" class="headerlink" title="3.6 其他"></a>3.6 其他</h3><h4 id="3-6-1-增加-Yarn-队列"><a href="#3-6-1-增加-Yarn-队列" class="headerlink" title="3.6.1 增加 Yarn 队列"></a>3.6.1 增加 Yarn 队列</h4><p><a href="https://blog.csdn.net/lijingjingchn/article/details/84876193" target="_blank" rel="noopener">https://blog.csdn.net/lijingjingchn/article/details/84876193</a></p><p>修改 <code>etc/hadoop/capacity-scheduler.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>default, myqueue<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The queues at the this level (root is the root queue).</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40.9<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Default queue target capacity.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.myqueue.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>59.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Default queue target capacity.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>刷新配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -refreshQueues</span><br></pre></td></tr></table></figure><p>注意：</p><p>热更新只能增加队列，要删除队列只能重启 RM。</p><p><a href="https://stackoverflow.com/questions/42589764/how-to-delete-a-queue-in-yarn" target="_blank" rel="noopener">https://stackoverflow.com/questions/42589764/how-to-delete-a-queue-in-yarn</a></p><h4 id="3-6-2-配置-node-label"><a href="#3-6-2-配置-node-label" class="headerlink" title="3.6.2 配置 node label"></a>3.6.2 配置 node label</h4><blockquote><p>官方文档参考：<br><a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/NodeLabel.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/NodeLabel.html</a></p><p>cloudera 文档参考（推荐）<br><a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_yarn-resource-management/content/configuring_node_labels.html" target="_blank" rel="noopener">https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_yarn-resource-management/content/configuring_node_labels.html</a></p></blockquote><p>1）先创建 node-label-conf 存放的路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /node-labels-conf</span><br></pre></td></tr></table></figure><p>2）配置yarn-site.xml ，增加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">&lt;!--开启node label --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.node-labels.fs-store.root-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/node-labels-conf/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--9000端口号从core-site.xml中找--&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.node-labels.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--????????  在 hadoop2.8.2 版本之前需要配置yarn.node-labels.configuration-type配置项。--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.node-labels.configuration-type<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>centralized or delegated-centralized or distributed<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><del>刷新</del></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn rmadmin -refreshQueues</span><br></pre></td></tr></table></figure><p>3）重启 RM</p><p>4）添加 label</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加</span></span><br><span class="line">yarn rmadmin -addToClusterNodeLabels <span class="string">"my_label_test"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">[Ace@hadoop102 hadoop]$ yarn cluster --list-node-labels</span><br><span class="line">21/03/24 18:40:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.87.103:8032</span><br><span class="line">Node Labels: my_label_test</span><br></pre></td></tr></table></figure><p>5）给 node 打 label</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yarn rmadmin -replaceLabelsOnNode <span class="string">"hadoop102=my_label_test"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想删除node伤的标签</span></span><br><span class="line">yarn rmadmin -replaceLabelsOnNode <span class="string">"hadoop102"</span></span><br></pre></td></tr></table></figure><p>6）将队列与 label 关联</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 遵循层级配置</span><br><span class="line"># 1）root配置</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.accessible-node-labels.my_label_test.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"># 2) 子队列配置（给myqueue队列分配my_label_test的全部资源）</span><br><span class="line"># 如果不指定此字段，则将从其父字段继承。 ?????????</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.myqueue.accessible-node-labels<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>my_label_test<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.myqueue.accessible-node-labels.my_label_test.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"># 当资源请求未指定节点标签时，应用将被提交到该值对应的分区。默认情况下，该值为空，即应用程序将被分配没有标签的节点上的容器。</span><br><span class="line">yarn.scheduler.capacity.<span class="tag">&lt;<span class="name">queue-path</span>&gt;</span>.default-node-label-expression</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 刷新队列</span></span><br><span class="line">yarn rmadmin -refreshQueues</span><br></pre></td></tr></table></figure><p>7）查看效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[Ace@hadoop102 hadoop]$ yarn node -list</span><br><span class="line">21/03/24 18:50:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.87.103:8032</span><br><span class="line">Total Nodes:3</span><br><span class="line">         Node-Id     Node-StateNode-Http-AddressNumber-of-Running-Containers</span><br><span class="line"> hadoop104:44284        RUNNING   hadoop104:8042                           0</span><br><span class="line"> hadoop103:41733        RUNNING   hadoop103:8042                           0</span><br><span class="line"> hadoop102:43484        RUNNING   hadoop102:8042                           0</span><br><span class="line"> </span><br><span class="line"> [Ace@hadoop102 hadoop]$ yarn node -status hadoop102:43484</span><br><span class="line">21/03/24 19:01:17 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.87.103:8032</span><br><span class="line">Node Report :</span><br><span class="line">Node-Id : hadoop102:43484</span><br><span class="line">Rack : /default-rack</span><br><span class="line">Node-State : RUNNING</span><br><span class="line">Node-Http-Address : hadoop102:8042</span><br><span class="line">Last-Health-Update : 星期三 24/三月/21 06:59:23:634CST</span><br><span class="line">Health-Report :</span><br><span class="line">Containers : 0</span><br><span class="line">Memory-Used : 0MB</span><br><span class="line">Memory-Capacity : 8192MB</span><br><span class="line">CPU-Used : 0 vcores</span><br><span class="line">CPU-Capacity : 8 vcores</span><br><span class="line">Node-Labels : my_label_test</span><br></pre></td></tr></table></figure><p>8）测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount -D mapreduce.job.queuename=myqueue -D node_label_expression=my_label_test /wcinput /output/xxx</span><br></pre></td></tr></table></figure><p><strong>2.7.2 版本有超多bug &amp; 不完善</strong></p><p>a) 设置队列后 web ui 没有分组显示</p><p>b) 为了能在指定 label node 上运行，需要把默认队列的 capacity 和 max-capacity 都设为0。但是这样会导致只能起一个 AM（一个 job）</p><blockquote><p><a href="https://dbaplus.cn/news-73-1900-1.html" target="_blank" rel="noopener">介绍label稳定版和非稳定版区别，以及自己开发的Yarn资源监控</a></p></blockquote><h4 id="3-6-3-添加-Prometheus-监控"><a href="#3-6-3-添加-Prometheus-监控" class="headerlink" title="3.6.3 添加 Prometheus 监控"></a>3.6.3 添加 Prometheus 监控</h4><h5 id="3-6-3-1-HDFS-监控"><a href="#3-6-3-1-HDFS-监控" class="headerlink" title="3.6.3.1 HDFS 监控"></a>3.6.3.1 HDFS 监控</h5><blockquote><p><a href="https://www.jesseyates.com/2019/03/03/hdfs-blocks-missing-vs-corrupt.html" target="_blank" rel="noopener">https://www.jesseyates.com/2019/03/03/hdfs-blocks-missing-vs-corrupt.html</a></p></blockquote><ul><li>配置 <code>etc/hadoop/hadoop-env.sh</code>，添加如下内容</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add prom</span></span><br><span class="line"><span class="comment"># if 是必要的防止多次source 这个文件，导致端口被重复注册的问题（会无法启动namenode）</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$HADOOP_NAMENODE_OPTS</span> != *<span class="string">"javaagent"</span>* ]]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">export</span> HADOOP_NAMENODE_OPTS=<span class="string">"<span class="variable">$HADOOP_NAMENODE_OPTS</span> -javaagent:/opt/module/hadoop-2.7.2/jmx_prometheus_javaagent-0.13.0.jar=9300:/opt/module/hadoop-2.7.2/namenode.yml"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><ul><li>创建 <code>${HADOOP_HOME}/namenode.yml</code></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lowercaseOutputName:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">lowercaseOutputLabelNames:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">whitelistObjectNames:</span> <span class="string">["Hadoop:*",</span> <span class="string">"java.lang:*"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><ul><li><p>重启 HDFS（Namenode）</p></li><li><p>查看效果：<a href="http://hadoop102:9300" target="_blank" rel="noopener">http://hadoop102:9300</a></p></li></ul><h5 id="3-6-3-2-Yarn-监控"><a href="#3-6-3-2-Yarn-监控" class="headerlink" title="3.6.3.2 Yarn 监控"></a>3.6.3.2 Yarn 监控</h5><ul><li>配置 <code>etc/hadoop/hadoop-env.sh</code>，添加如下内容</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add prom</span></span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_OPTS=<span class="string">"<span class="variable">$YARN_RESOURCEMANAGER_OPTS</span> -javaagent:/opt/module/hadoop-2.7.2/jmx_prometheus_javaagent-0.13.0.jar=3333:/opt/module/hadoop-2.7.2/resourcemanager.yml"</span></span><br></pre></td></tr></table></figure><ul><li>创建 <code>${HADOOP_HOME}/capacity-scheduler.xml</code>（这里面有啥区别？？）</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lowercaseOutputName:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">lowercaseOutputLabelNames:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">whitelistObjectNames:</span> <span class="string">["Hadoop:*",</span> <span class="string">"java.lang:*"</span><span class="string">]</span></span><br></pre></td></tr></table></figure><ul><li>重启 Resource Manager</li><li>查看效果：<a href="http://hadoop104:8042/jmx" target="_blank" rel="noopener">http://hadoop104:8042/jmx</a></li></ul><p>可能有用的参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#125;, &#123;</span><br><span class="line">  &quot;name&quot; : &quot;Hadoop:service=NodeManager,name=NodeManagerMetrics&quot;,</span><br><span class="line">  &quot;modelerType&quot; : &quot;NodeManagerMetrics&quot;,</span><br><span class="line">  &quot;tag.Context&quot; : &quot;yarn&quot;,</span><br><span class="line">  &quot;tag.Hostname&quot; : &quot;hadoop103&quot;,</span><br><span class="line">  &quot;ContainersLaunched&quot; : 15,</span><br><span class="line">  &quot;ContainersCompleted&quot; : 1,</span><br><span class="line">  &quot;ContainersFailed&quot; : 0,</span><br><span class="line">  &quot;ContainersKilled&quot; : 11,</span><br><span class="line">  &quot;ContainersIniting&quot; : 0,</span><br><span class="line">  &quot;ContainersRunning&quot; : 3,</span><br><span class="line">  &quot;AllocatedGB&quot; : 4,</span><br><span class="line">  &quot;AllocatedContainers&quot; : 3,</span><br><span class="line">  &quot;AvailableGB&quot; : 4,</span><br><span class="line">  &quot;AllocatedVCores&quot; : 3,</span><br><span class="line">  &quot;AvailableVCores&quot; : 5,</span><br><span class="line">  &quot;ContainerLaunchDurationNumOps&quot; : 15,</span><br><span class="line">  &quot;ContainerLaunchDurationAvgTime&quot; : 408.33333333333337</span><br><span class="line">&#125;, &#123;</span><br></pre></td></tr></table></figure><p>==看到的内存比实际的多？==</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># HELP hadoop_nodemanager_availablegb AvailableGB (Hadoop&lt;service=NodeManager, name=NodeManagerMetrics&gt;&lt;&gt;AvailableGB)</span><br><span class="line"># TYPE hadoop_nodemanager_availablegb untyped</span><br><span class="line">hadoop_nodemanager_availablegb&#123;name=&quot;NodeManagerMetrics&quot;,&#125; 8.0</span><br><span class="line"></span><br><span class="line"># HELP hadoop_nodemanager_allocatedgb Current allocated memory in GB (Hadoop&lt;service=NodeManager, name=NodeManagerMetrics&gt;&lt;&gt;AllocatedGB)</span><br><span class="line"># TYPE hadoop_nodemanager_allocatedgb untyped</span><br><span class="line">hadoop_nodemanager_allocatedgb&#123;name=&quot;NodeManagerMetrics&quot;,&#125; 0.0</span><br></pre></td></tr></table></figure><p><strong>更细粒度监控</strong></p><p>可以通过这个地址看到 json / xml 格式的监控信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;rm http address:port&gt;/ws/v1/cluster/scheduler</span><br></pre></td></tr></table></figure><h4 id="3-6-4-NM-内存容量动态更新"><a href="#3-6-4-NM-内存容量动态更新" class="headerlink" title="3.6.4 NM 内存容量动态更新"></a>3.6.4 NM 内存容量动态更新</h4><blockquote><p><a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_command-line-installation/content/determine-hdp-memory-config.html" target="_blank" rel="noopener">NM内存配置参数推荐</a></p><p><a href="https://www.huaweicloud.com/articles/4c9feebe11ae7f2d2d448a53292db16d.html" target="_blank" rel="noopener">YARN的Memory和CPU调优配置详解</a></p><p><a href="https://my.oschina.net/cjun/blog/688827" target="_blank" rel="noopener">yarn集群上内存和cpu调优和设置</a></p></blockquote><h4 id="3-6-5-hdfs-balancer"><a href="#3-6-5-hdfs-balancer" class="headerlink" title="3.6.5 hdfs balancer"></a>3.6.5 hdfs balancer</h4><blockquote><p><a href="https://www.expecc.com/post/increasing-hdfs-balancer-performance" target="_blank" rel="noopener">Increasing HDFS Balancer Performance</a></p><p><a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.0/bk_hdfs-administration/content/configuring_balancer.html" target="_blank" rel="noopener">HDFS Administration - cloudera</a></p><p><a href="https://docs.cloudera.com/documentation/enterprise/5-7-x/topics/admin_hdfs_balancer.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--780a" target="_blank" rel="noopener">HDFS Balancers - cloudera</a></p><p><a href="https://www.jianshu.com/p/f7c1cd476601" target="_blank" rel="noopener">HDFS balance策略详解</a></p></blockquote><h3 id="3-7-升级-Yarn"><a href="#3-7-升级-Yarn" class="headerlink" title="3.7 升级 Yarn"></a>3.7 升级 Yarn</h3><h4 id="3-7-1-升级版本到-2-8-5"><a href="#3-7-1-升级版本到-2-8-5" class="headerlink" title="3.7.1 升级版本到 2.8.5"></a>3.7.1 升级版本到 2.8.5</h4><p>需要修改的文件</p><ul><li><code>yarn-env.sh</code></li><li><code>yarn-site.xml</code></li><li><code>slaves</code></li><li><code>core-site.xml</code></li><li><code>hadoop-env.sh</code></li><li><code>hdfs-site.xml</code></li></ul><p>测试能否只升级 RM，不升级 NM（可以）</p><blockquote><p><a href="https://blog.csdn.net/qq_16038125/article/details/103177181" target="_blank" rel="noopener">hadoop各版本特性</a></p><p><a href="http://www.bianqi.info/2018/10/hadoop-rollupdate/" target="_blank" rel="noopener">Hadoop2.7.6 滚动升级Hadoop2.8.5</a></p><p><a href="https://www.huaweicloud.com/articles/88a17b2e42fafc98bf9a50b7d1175d3a.html" target="_blank" rel="noopener">大数据的yarn类型系统分析与云储存</a></p><p><a href="https://mp.weixin.qq.com/s/3i0swPCcFplqJW12CNV57g" target="_blank" rel="noopener">小米Hadoop YARN平滑升级3.1实践</a></p><p><a href="https://blog.csdn.net/wypblog/article/details/103849721" target="_blank" rel="noopener">Hadoop 2.7 不停服升级到 3.2 在滴滴的实践</a></p></blockquote><h2 id="四、Zookeeper-配置"><a href="#四、Zookeeper-配置" class="headerlink" title="四、Zookeeper 配置"></a>四、Zookeeper 配置</h2><h3 id="4-1-本地模式"><a href="#4-1-本地模式" class="headerlink" title="4.1 本地模式"></a>4.1 本地模式</h3><p><strong>1、安装前准备</strong></p><ul><li>安装Jdk</li><li>拷贝Zookeeper安装包到Linux系统下</li><li>解压到指定目录</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p><strong>2、配置修改</strong></p><ul><li>修改<code>conf/zoo_sample.cfg</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure><ul><li>修改 zoo.cfg 文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br></pre></td></tr></table></figure><p>并创建 zkData 文件夹</p><p><strong>3、操作Zookeeper</strong></p><ul><li>启动Zookeeper</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure><ul><li>查看进程是否启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">4020 Jps</span><br><span class="line">4001 QuorumPeerMain</span><br></pre></td></tr></table></figure><ul><li>查看状态：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure><ul><li>停止Zookeeper</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h3 id="4-2-集群模式"><a href="#4-2-集群模式" class="headerlink" title="4.2 集群模式"></a>4.2 集群模式</h3><p><strong>1、配置</strong></p><ul><li>修改 zoo.cfg 文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br><span class="line"># 2888 为集群间通信端口号</span><br><span class="line"># 3888 为选举端口号</span><br><span class="line"># server.2/3/4 为id，不相同即可</span><br></pre></td></tr></table></figure><ul><li>创建 <code>zkData/myid</code>，每个机器写不同的，要和前面的对应上</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim zkData/myid</span><br><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line">2</span><br><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line">3</span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line">4</span><br></pre></td></tr></table></figure><ul><li>配置 <code>bin/zkEnv.sh</code></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 替换前</span></span><br><span class="line">ZOO_LOG_DIR=<span class="string">"."</span></span><br><span class="line"><span class="comment"># 替换后</span></span><br><span class="line">ZOO_LOG_DIR=<span class="string">"/opt/module/zookeeper-3.4.10/logs"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 JAVA_HOME（远程启动的时候才是必要的，因为会丢失环境变量）</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><ul><li>同步 xsync</li></ul><p><strong>2、启动</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个机器都要单独启动</span></span><br><span class="line">./bin/zkServer.sh start</span><br></pre></td></tr></table></figure><p>仅启动一台机器时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br></pre></td></tr></table></figure><p>启动两台机器（超过半数）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader / follower</span><br></pre></td></tr></table></figure><p><strong>3、集群脚本</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"==== start <span class="variable">$i</span> zookeeper ===="</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.4.10/bin/zkServer.sh start"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"==== stop <span class="variable">$i</span> zookeeper ===="</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="string">"status"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"==== status <span class="variable">$i</span> zookeeper ===="</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.4.10/bin/zkServer.sh status"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h3 id="4-3-客户端操作"><a href="#4-3-客户端操作" class="headerlink" title="4.3 客户端操作"></a>4.3 客户端操作</h3><ul><li>启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/zkCli.sh</span><br></pre></td></tr></table></figure><ul><li>执行</li></ul><table><thead><tr><th>命令基本语法</th><th>功能描述</th></tr></thead><tbody><tr><td>help</td><td>显示所有操作命令</td></tr><tr><td>ls path [watch]</td><td>使用 ls 命令来查看当前znode中所包含的内容</td></tr><tr><td>ls2 path [watch]</td><td>查看当前节点数据并能看到更新次数等数据</td></tr><tr><td>create</td><td>普通创建<br>-s  含有序列<br>-e  临时（重启或者超时消失）</td></tr><tr><td>get path [watch]</td><td>获得节点的值</td></tr><tr><td>set</td><td>设置节点的具体值</td></tr><tr><td>stat</td><td>查看节点状态</td></tr><tr><td>delete</td><td>删除节点</td></tr><tr><td>rmr</td><td>递归删除节点</td></tr></tbody></table><h2 id="五、Kakfa-配置"><a href="#五、Kakfa-配置" class="headerlink" title="五、Kakfa 配置"></a>五、Kakfa 配置</h2><h3 id="5-1-环境准备"><a href="#5-1-环境准备" class="headerlink" title="5.1 环境准备"></a>5.1 环境准备</h3><ul><li>在 hadoop102 103 104 上均安装 Kafka</li><li>jar 包下载 <a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">https://kafka.apache.org/downloads</a><ul><li>命名中有两个版本号，第一个为 scala 版本，第二个是 kafka 版本</li></ul></li></ul><h3 id="5-2-集群配置"><a href="#5-2-集群配置" class="headerlink" title="5.2 集群配置"></a>5.2 集群配置</h3><ul><li>修改 <code>config/server.properties</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># broker的全局唯一编号，不能重复</span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line"># 删除topic功能使能（kafka 2.x 版本 不需要配置）</span><br><span class="line">delete.topic.enable=true</span><br><span class="line"></span><br><span class="line"># kafka运行时数据存放的路径</span><br><span class="line">log.dirs=/opt/module/kafka_2.12-2.3.1/data</span><br><span class="line"></span><br><span class="line"># 配置连接Zookeeper集群地址</span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181</span><br></pre></td></tr></table></figure><ul><li><p>分发安装包到所有物理机上 xsync</p></li><li><p>将 hadoop103 104 上<code>config/server.properties</code> 中的 <code>broker.id=x</code>进行修改</p></li></ul><p><strong>单点启动 / 停止</strong> </p><p>依次在 hadoop102、hadoop103、hadoop104 节点上启动/停止 kafka，执行下面的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line"><span class="comment"># 前台运行</span></span><br><span class="line">$ bin/kafka-server-start.sh config/server.properties</span><br><span class="line"><span class="comment"># 后台运行</span></span><br><span class="line">$ bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">$ bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><p><strong>群起 / 群停</strong></p><p>由于 Kafka 中没有给集群启动停止的脚本，需要自己写<code>kk-all.sh</code></p><p>需要注意：要先在 <code>~/.bashrc</code> 中配置 java 环境变量（xsync）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"==== start <span class="variable">$i</span> kafka ===="</span></span><br><span class="line">                <span class="comment"># ssh $i "source /etc/profile"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"/opt/module/kafka_2.12-2.3.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.12-2.3.1/config/server.properties"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"==== stop <span class="variable">$i</span> kafka ===="</span></span><br><span class="line">                <span class="comment"># ssh $i "source /etc/profile"</span></span><br><span class="line">                ssh <span class="variable">$i</span> <span class="string">"/opt/module/kafka_2.12-2.3.1/bin/kafka-server-stop.sh"</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>启动/停止 Kafka：（记得先启动 Zookeeper）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ./kk-all.sh start</span><br><span class="line">$ jps</span><br><span class="line">1637 QuorumPeerMain</span><br><span class="line">6071 Kafka</span><br><span class="line">6538 Jps</span><br><span class="line"></span><br><span class="line">$ ./kk-all.sh stop</span><br></pre></td></tr></table></figure><h3 id="5-3-命令行操作"><a href="#5-3-命令行操作" class="headerlink" title="5.3 命令行操作"></a>5.3 命令行操作</h3><ul><li>查看当前服务器中的所有 topic</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list</span><br></pre></td></tr></table></figure><ul><li>创建 topic</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure><p><code>--replication-factor</code> 定义副本数；<code>--partitions</code> 定义分区数</p><ul><li>删除 topic</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first</span><br></pre></td></tr></table></figure><ul><li>查看某个 Topic 的详情</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first</span><br></pre></td></tr></table></figure><ul><li>发送消息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first</span><br><span class="line">&gt;hello world</span><br><span class="line">&gt;atguigu atguigu</span><br></pre></td></tr></table></figure><ul><li>消费消息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure><p><code>--from-beginning</code> 会把 first 主题中以往所有的数据都读取出来</p><h2 id="六、Spark-配置"><a href="#六、Spark-配置" class="headerlink" title="六、Spark 配置"></a>六、Spark 配置</h2><h3 id="6-1-环境准备"><a href="#6-1-环境准备" class="headerlink" title="6.1 环境准备"></a>6.1 环境准备</h3><p>下载地址：<a href="https://archive.apache.org/dist/spark/" target="_blank" rel="noopener">https://archive.apache.org/dist/spark/</a></p><p>有两种版本，hadoop 版下载就能用，不依赖其他组价；without-hadoop 需要依赖已有的 hadoop 组件</p><blockquote><p>spark-2.3.2-bin-hadoop2.7.tgz<br>spark-2.3.2-bin-without-hadoop.tgz </p></blockquote><h3 id="6-2-本地模式-Local"><a href="#6-2-本地模式-Local" class="headerlink" title="6.2 本地模式 Local"></a>6.2 本地模式 Local</h3><ul><li>Jar</li></ul><p>解压后进入 Spark 根目录执行（一个算 PI 的程序）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.3.2.jar 100</span><br><span class="line"></span><br><span class="line">bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.11-2.3.2.jar 100</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一大堆迭代过程 </span></span><br><span class="line">Pi is roughly 3.1424043142404314</span><br></pre></td></tr></table></figure><p>可以通过访问 <a href="http://hadoop102:4040" target="_blank" rel="noopener">http://hadoop102:4040</a> 查看任务运行情况</p><p><strong>「问题」</strong>：运行结束这个页面就关闭了，不能查历史任务执行情况</p><p><strong>「解决」</strong>：添加 Spark History Server</p><ul><li>Spark-shell</li></ul><p>进入 Spark-shell，<code>bin/spark-shell</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建两个文件，里面输入几行单词</span></span><br><span class="line">$ vim 1.txt  <span class="comment"># xxxxx</span></span><br><span class="line">$ vim 2.txt  <span class="comment"># xxxxx</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入 spark-shell 执行</span></span><br><span class="line">$ bin/spark-shell</span><br><span class="line">&gt; sc.textFile(<span class="string">"input"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,1)).reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure><h3 id="6-3-Standalone-模式"><a href="#6-3-Standalone-模式" class="headerlink" title="6.3 Standalone 模式"></a>6.3 Standalone 模式</h3><p>构建一个由 Master + Slave 构成的 Spark 集群，Spark 运行在集群中。</p><p>这个要和 Hadoop 中的 Standalone 区别开来.这里的 Standalone 是指只用 Spark 来搭建一个集群, 不需要借助其他的框架.是相对于 Yarn 和 Mesos 来说的.</p><h4 id="6-3-1-Spark-server配置"><a href="#6-3-1-Spark-server配置" class="headerlink" title="6.3.1 Spark server配置"></a>6.3.1 Spark server配置</h4><p>1、进入配置文件目录conf，配置spark-evn.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> conf/</span><br><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>在 <code>spark-env.sh</code> 文件中配置如下内容:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=hadoop102</span><br><span class="line">SPARK_MASTER_PORT=7077 <span class="comment"># 默认端口就是7077, 可以省略不配</span></span><br></pre></td></tr></table></figure><p>2、修改 slaves 文件，添加 worker 节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp slaves.template slaves</span><br><span class="line"><span class="comment"># 在slaves文件中配置如下内容:</span></span><br><span class="line">hadoop201</span><br><span class="line">hadoop202</span><br><span class="line">hadoop203</span><br></pre></td></tr></table></figure><p>3、修改 sbin/spark-config.sh，添加 JAVA_HOME （防止 JAVA_HOME is not set 报错）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure><p>4、分发spark-standalone</p><p>5、启动 Spark 集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>6、网页查看信息：<a href="http://hadoop102:8080/" target="_blank" rel="noopener">http://hadoop102:8080/</a></p><p>7、测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop102:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 6 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.3.2.jar 100</span><br></pre></td></tr></table></figure><h4 id="6-3-2-spark-history-server"><a href="#6-3-2-spark-history-server" class="headerlink" title="6.3.2 spark-history-server"></a>6.3.2 spark-history-server</h4><p>在 Spark-shell 没有退出之前，看到正在执行的任务的日志情况:<a href="http://hadoop102:4040" target="_blank" rel="noopener">http://hadoop102:4040</a>. 但是退出之后，执行的所有任务记录全部丢失</p><p>所以需要配置任务的历史服务器, 方便在任何需要的时候去查看日志。</p><ul><li>配置spark-default.conf文件，开启 Log</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><p>在 <code>spark-defaults.conf</code> 文件中, 添加如下内容:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir               hdfs://hadoop102:9000/spark-job-log</span><br></pre></td></tr></table></figure><p>注意:</p><p><code>hdfs://hadoop201:9000/spark-job-log</code> 目录必须提前存在, 名字随意</p><ul><li>修改spark-env.sh文件，添加如下配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log"</span></span><br></pre></td></tr></table></figure><ul><li>分发配置文件</li><li>启动历史服务<ul><li>需要先启动 HDFS <code>$HADOOP_HOME/sbin/start-dfs.sh</code></li><li>然后再启动: <code>sbin/start-history-server.sh</code></li></ul></li></ul><p>ui 地址: <a href="http://hadoop102:18080" target="_blank" rel="noopener">http://hadoop102:18080</a></p><h3 id="6-4-Yarn-模式"><a href="#6-4-Yarn-模式" class="headerlink" title="6.4 Yarn 模式"></a>6.4 Yarn 模式</h3><h4 id="6-4-1-spark-server-配置"><a href="#6-4-1-spark-server-配置" class="headerlink" title="6.4.1 spark server 配置"></a>6.4.1 spark server 配置</h4><ul><li>修改 <code>${HADOOP_HOME}/etc/hadoop/yarn-site.xml</code>（仅虚拟机中配置，防止内存不够）</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默&gt;认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默&gt;认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改 <code>conf/spark-env.sh</code>，分发</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><ul><li>测试（注意 master、deploy-mode 参数的变化）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br><span class="line">$ start-yarn.sh</span><br><span class="line"></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.3.2.jar 100</span><br></pre></td></tr></table></figure><ul><li>spark-shell</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master yarn</span><br></pre></td></tr></table></figure><p>Yarn：<a href="http://hadoop103:8088" target="_blank" rel="noopener">http://hadoop103:8088</a></p><h4 id="6-4-2-spark-history-server"><a href="#6-4-2-spark-history-server" class="headerlink" title="6.4.2 spark-history-server"></a>6.4.2 spark-history-server</h4><p><code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code> 中添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop104:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 填 yarn historyserver 的物理机  --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>$SPARK_HOME/conf/spark-defaults.conf</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.addresshadoop102:18080    # spark history Server 物理机</span><br><span class="line">spark.history.ui.port 18080</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line">spark.eventLog.dir               hdfs://hadoop102:9000/spark-job-log</span><br><span class="line">spark.history.fs.logDirectory hdfs://hadoop102:9000/spark-job-log</span><br></pre></td></tr></table></figure><ul><li>相关服务<ul><li>spark: <code>sbin/start-all.sh</code></li><li>HDFS: <code>[hadoop102]$ start-dfs.sh</code></li><li>Yarn: <code>[hadoop103]$ start-yarn.sh</code></li><li>Yarn-history: <code>[hadoop104]$ mr-jobhistory-daemon.sh start historyserver</code></li><li>Spark-history: <code>[hadoop102] $ sbin/start-history-server.sh</code></li></ul></li></ul><p>【可以正常展示了】</p><p>相关文档解释：<a href="https://www.cnblogs.com/sorco/p/7070922.html" target="_blank" rel="noopener">spark深入：配置文件与日志 - Super_Orco - 博客园</a></p><ul><li>查看方式<ul><li>通过 YARN 查询<ul><li><a href="http://hadoop103:8088/" target="_blank" rel="noopener">http://hadoop103:8088/</a></li></ul></li><li>直接在 spark history server 中查询<ul><li><a href="http://hadoop102:18080/" target="_blank" rel="noopener">http://hadoop102:18080/</a></li></ul></li></ul></li></ul><h3 id="6-5-WordCount-程序"><a href="#6-5-WordCount-程序" class="headerlink" title="6.5 WordCount 程序"></a>6.5 WordCount 程序</h3><p>略</p><h2 id="七、Hive-配置"><a href="#七、Hive-配置" class="headerlink" title="七、Hive 配置"></a>七、Hive 配置</h2><h3 id="7-1-单机默认配置"><a href="#7-1-单机默认配置" class="headerlink" title="7.1 单机默认配置"></a>7.1 单机默认配置</h3><p>下载地址：<a href="http://archive.apache.org/dist/hive/" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/</a></p><p><strong>安装部署：</strong></p><ul><li>修改 <code>conf/hive-env.sh.template</code> </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ mv hive-env.sh.template hive-env.sh</span><br><span class="line">$ vim hive-env.sh</span><br><span class="line">~</span><br><span class="line">~</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/opt/module/hive-2.3.0-bin/conf</span><br><span class="line">~</span><br><span class="line">~</span><br></pre></td></tr></table></figure><ul><li>hadoop 相关配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 hdfs yarn</span></span><br><span class="line">$ start-dfs.sh</span><br><span class="line">$ start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 hive warehouse（存数据的地方）</span></span><br><span class="line">$ hadoop fs -mkdir /tmp</span><br><span class="line">$ hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">$ hadoop fs -chmod g+w /tmp</span><br><span class="line">$ hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><ul><li>Hive 基本操作 </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hive</span><br><span class="line"><span class="comment"># 查看数据库</span></span><br><span class="line">hive&gt; show databases;</span><br><span class="line"><span class="comment"># 打开默认数据库 </span></span><br><span class="line">hive&gt; use default;</span><br><span class="line"><span class="comment"># 显示 default 数据库中的表 </span></span><br><span class="line">hive&gt; show tables;</span><br><span class="line"><span class="comment"># 创建一张表</span></span><br><span class="line">hive&gt; create table student(id int, name string);</span><br><span class="line"><span class="comment"># 查看表的结构 </span></span><br><span class="line">hive&gt; desc student;</span><br><span class="line"><span class="comment"># 向表中插入数据</span></span><br><span class="line">hive&gt; insert into student values(1000,<span class="string">"ss"</span>);</span><br><span class="line"><span class="comment"># 查询表中数据</span></span><br><span class="line">hive&gt; select * from student;</span><br><span class="line"><span class="comment"># 退出 </span></span><br><span class="line">hive hive&gt; quit;</span><br></pre></td></tr></table></figure><h3 id="7-2-修改默认数据库（derby-gt-MySQL）"><a href="#7-2-修改默认数据库（derby-gt-MySQL）" class="headerlink" title="7.2 修改默认数据库（derby -&gt; MySQL）"></a>7.2 修改默认数据库（derby -&gt; MySQL）</h3><p>derby 只支持单个客户端连接，仅适用于简单测试。更换成关系型数据库（如MySQL），可支持多客户端连接。</p><h4 id="7-2-1-安装-MySQL"><a href="#7-2-1-安装-MySQL" class="headerlink" title="7.2.1 安装 MySQL"></a>7.2.1 安装 MySQL</h4><ul><li>卸载原有的，安装新的 MySQL </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卸载原有的</span></span><br><span class="line">$ su -  <span class="comment"># 切换到 root 用户</span></span><br><span class="line">$ rpm -qa | grep mysql </span><br><span class="line">mysql-libs-5.1.73-7.el6.x86_64</span><br><span class="line">$ rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装新的 MySQL-server</span></span><br><span class="line">$ rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">cat /root/.mysql_secret <span class="comment"># 记住默认的root登录密码</span></span><br><span class="line">OEXaQuS8IWkG19Xs</span><br><span class="line">$ service mysql status</span><br><span class="line">$ service mysql start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 MySQL-client</span></span><br><span class="line">$ rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line"><span class="comment"># 修改 root 密码</span></span><br><span class="line">$ mysql -uroot -pOEXaQuS8IWkG19Xs</span><br><span class="line">mysql&gt;SET PASSWORD=PASSWORD(<span class="string">'123456'</span>);</span><br><span class="line">mysql&gt;<span class="built_in">exit</span></span><br></pre></td></tr></table></figure><ul><li>配置 MySQL 远程登录</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -uroot -p123456</span><br><span class="line">mysql&gt; use mysql;</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">mysql&gt; select User, Host, Password from user;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 user 表，把 Host 表内容修改为%</span></span><br><span class="line">mysql&gt; update user <span class="built_in">set</span> host=<span class="string">'%'</span> <span class="built_in">where</span> host=<span class="string">'localhost'</span>;</span><br><span class="line"><span class="comment"># 其他的都删掉</span></span><br><span class="line">mysql&gt; delete from user <span class="built_in">where</span> host=<span class="string">'hadoop102'</span>;</span><br><span class="line">mysql&gt; delete from user <span class="built_in">where</span> host=<span class="string">'127.0.0.1'</span>;</span><br><span class="line">mysql&gt; delete from user <span class="built_in">where</span> host=<span class="string">'::1'</span>;</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">mysql&gt; quit;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 都做完切换回原来的用户</span></span><br></pre></td></tr></table></figure><h4 id="7-2-2-修改-hive-元数据库"><a href="#7-2-2-修改-hive-元数据库" class="headerlink" title="7.2.2 修改 hive 元数据库"></a>7.2.2 修改 hive 元数据库</h4><ul><li>拷贝驱动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive-2.3.0-bin/lib/</span><br></pre></td></tr></table></figure><ul><li>配置 Metastore 到 MySQL</li></ul><p>创建 <code>conf/hive-site.xml</code> </p><blockquote><p>官方配置文档<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration" target="_blank" rel="noopener">AdminManual Metastore Administration - Apache Hive - Apache Software Foundation</a></p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>用于启动 hive-metastore 的配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 可以显示表头列名 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 显示当前数据库名 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop102:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初试化hive库</span></span><br><span class="line">$ bin/schematool -initSchema -dbType mysql</span><br><span class="line"><span class="comment"># 启动metastore节点</span></span><br><span class="line">$ nohup bin/hive --service metastore &amp;</span><br><span class="line"><span class="comment"># 启动hiveserver2（可选？） </span></span><br><span class="line">$ nohup bin/hive --service hiveserver2 &amp;</span><br><span class="line"><span class="comment"># 命令行启动</span></span><br><span class="line">$ bin/hive</span><br></pre></td></tr></table></figure><h3 id="7-3-Beeline-连接"><a href="#7-3-Beeline-连接" class="headerlink" title="7.3 Beeline 连接"></a>7.3 Beeline 连接</h3><blockquote><p>Hive学习之路 （四）Hive的连接3种连接方式 - 扎心了，老铁 - 博客园<br><a href="https://www.cnblogs.com/qingyunzong/p/8715925.html" target="_blank" rel="noopener">https://www.cnblogs.com/qingyunzong/p/8715925.html</a></p></blockquote><h3 id="7-4-常用交互命令"><a href="#7-4-常用交互命令" class="headerlink" title="7.4 常用交互命令"></a>7.4 常用交互命令</h3><p>1、<code>-e</code>不进入 hive 的交互窗口执行 sql 语句 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hive -e <span class="string">"select id from student;"</span></span><br></pre></td></tr></table></figure><p>2、<code>-f</code>执行脚本中 sql 语句 </p><ul><li>创建 hivef.sql 文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ touch hivef.sql</span><br><span class="line">select *from student;</span><br><span class="line">$ bin/hive -f xxx/hivef.sql &gt; xxx/result.txt</span><br></pre></td></tr></table></figure><h3 id="7-5-集成-Tez-引擎"><a href="#7-5-集成-Tez-引擎" class="headerlink" title="7.5 集成 Tez 引擎"></a>7.5 集成 Tez 引擎</h3><ul><li>解压 tez：<code>/opt/module/tez-0.9.1-bin</code></li><li>同时上传一个 tez 包到 hdfs，用于给集群中其他节点用</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put /opt/software/apache-tez-0.9.1-bin.tar.gz/ /tez</span><br></pre></td></tr></table></figure><ul><li>在 HIVE_HOME/conf 下创建 <code>tez-site.xml</code></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/tez/apache-tez-0.9.1-bin.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.use.cluster.hadoop-libs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.history.logging.service.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span>        </span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改 <code>hive-env.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Folder containing extra libraries required for hive compilation/execution can be controlled by:</span></span><br><span class="line"><span class="built_in">export</span> TEZ_HOME=/opt/module/tez-0.9.1-bin    <span class="comment">#是你的tez的解压目录</span></span><br><span class="line"><span class="built_in">export</span> TEZ_JARS=<span class="string">""</span></span><br><span class="line"><span class="keyword">for</span> jar <span class="keyword">in</span> `ls <span class="variable">$TEZ_HOME</span> |grep jar`; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">export</span> TEZ_JARS=<span class="variable">$TEZ_JARS</span>:<span class="variable">$TEZ_HOME</span>/<span class="variable">$jar</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> jar <span class="keyword">in</span> `ls <span class="variable">$TEZ_HOME</span>/lib`; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">export</span> TEZ_JARS=<span class="variable">$TEZ_JARS</span>:<span class="variable">$TEZ_HOME</span>/lib/<span class="variable">$jar</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar<span class="variable">$TEZ_JARS</span></span><br></pre></td></tr></table></figure><ul><li>修改 <code>hive-site.xml</code></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Hive</span></span><br><span class="line">$ bin/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">hive (default)&gt; create table student(</span><br><span class="line">id int,</span><br><span class="line">name string);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向表中插入数据</span></span><br><span class="line">hive (default)&gt; insert into student values(1,<span class="string">"zhangsan"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有报错就表示成功了</span></span><br><span class="line">hive (default)&gt; select * from student;</span><br><span class="line">1       zhangsan</span><br></pre></td></tr></table></figure><h2 id="八、HBase-配置"><a href="#八、HBase-配置" class="headerlink" title="八、HBase 配置"></a>八、HBase 配置</h2><h3 id="8-1-集群配置"><a href="#8-1-集群配置" class="headerlink" title="8.1 集群配置"></a>8.1 集群配置</h3><ul><li><code>conf/hbase-env.sh</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、注释掉下面两行</span></span><br><span class="line"><span class="comment"># Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MASTER_OPTS=<span class="string">"<span class="variable">$HBASE_MASTER_OPTS</span> -XX:PermSize=128m -XX:MaxPermSize=128m"</span></span><br><span class="line"><span class="built_in">export</span> HBASE_REGIONSERVER_OPTS=<span class="string">"<span class="variable">$HBASE_REGIONSERVER_OPTS</span> -XX:PermSize=128m -XX:MaxPermSiz    e=128m"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用单独的 Zookeeper</span></span><br><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li><code>conf/hbase-site.sh</code></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 都要根据自己的机器进行配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/HBase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:2181,hadoop103:2181,hadoop104:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.4.10/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>单独启动</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 HDFS、Zookeeper</span></span><br><span class="line">$ start-dfs.sh</span><br><span class="line">$ <span class="variable">$&#123;Zookeeper_BASE&#125;</span>/bin/zkServer.sh start  <span class="comment"># 每个机器都要单独启动</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># HBase</span></span><br><span class="line">$ bin/hbase-daemon.sh start master   <span class="comment"># 在其中一台启动</span></span><br><span class="line">$ bin/hbase-daemon.sh start regionserver <span class="comment"># 都要启动</span></span><br><span class="line"></span><br><span class="line">$ bin/hbase-daemon.sh stop master   <span class="comment"># 在其中一台启动</span></span><br><span class="line">$ bin/hbase-daemon.sh stop regionserver <span class="comment"># 都要启动</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以在 hadoop102:16010 查看ui界面</span></span><br></pre></td></tr></table></figure><ul><li>群起 / 群停</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 HDFS、Zookeeper</span></span><br><span class="line">$ start-dfs.sh</span><br><span class="line">$ <span class="variable">$&#123;Zookeeper_BASE&#125;</span>/bin/zkServer.sh start  <span class="comment"># 每个机器都要单独启动</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 conf/regionservers，添加所有 regionserver 的 host</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br><span class="line"></span><br><span class="line"><span class="comment"># 群起 / 群停</span></span><br><span class="line">$ bin/start-hbase.sh</span><br><span class="line">$ bin/stop-hbase.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># regionservers</span></span><br><span class="line">$ bin/hbase-daemons.sh start regionserver</span><br><span class="line">$ bin/hbase-daemons.sh stop regionserver</span><br></pre></td></tr></table></figure><ul><li>进入 shell</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有 kerberos 认证，需要先 kinit 一下</span></span><br><span class="line">$ bin/hbase shell</span><br></pre></td></tr></table></figure><h3 id="8-2-常用操作"><a href="#8-2-常用操作" class="headerlink" title="8.2 常用操作"></a>8.2 常用操作</h3><p>使用 <code>help</code> 查看各种命令使用方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有指令</span></span><br><span class="line">&gt; <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某一组命令的帮助</span></span><br><span class="line">&gt; <span class="built_in">help</span> <span class="string">'COMMAND_GROUP'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看单个命令帮助</span></span><br><span class="line">&gt; <span class="built_in">help</span> <span class="string">'COMMAND'</span></span><br></pre></td></tr></table></figure><h4 id="8-2-1-namespace"><a href="#8-2-1-namespace" class="headerlink" title="8.2.1 namespace"></a>8.2.1 namespace</h4><p><code>Commands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables</code></p><ul><li>list_namespace</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有库名</span></span><br><span class="line">&gt; list_namespace</span><br></pre></td></tr></table></figure><ul><li>create_namespace</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; create_namespace <span class="string">'school'</span></span><br></pre></td></tr></table></figure><ul><li>delete_namespace</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：只能删除空库</span></span><br><span class="line">&gt; delete_namespace <span class="string">'school'</span></span><br></pre></td></tr></table></figure><ul><li>list_namespace_tables</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出库中所有表</span></span><br><span class="line">&gt; list_namespace_tables <span class="string">'school'</span></span><br></pre></td></tr></table></figure><h4 id="8-2-2-DDL"><a href="#8-2-2-DDL" class="headerlink" title="8.2.2 DDL"></a>8.2.2 DDL</h4><p><code>Commands: alter, alter_async, alter_status, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, locate_region, show_filters</code></p><ul><li>list</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list 列出所有表</span></span><br><span class="line">&gt; list</span><br></pre></td></tr></table></figure><ul><li>create</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create '库名:表名', &#123; NAME =&gt; '列族名1', 属性名 =&gt; 属性值&#125;, &#123;NAME =&gt; '列族名2', 属性名 =&gt; 属性值&#125;, …</span></span><br><span class="line">&gt; create <span class="string">'school:student'</span>, &#123;NAME=&gt;<span class="string">'info'</span>&#125;</span><br><span class="line">&gt; create <span class="string">'school:student'</span>, &#123;NAME=&gt;<span class="string">'info'</span>, VERSIONS=&gt;5&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你只需要创建列族，而不需要定义列族属性，那么可以采用以下快捷写法：</span></span><br><span class="line"><span class="comment"># create'表名','列族名1' ,'列族名2', …</span></span><br><span class="line"><span class="comment"># 不写库名，默认 namespace 为 default</span></span><br><span class="line">&gt; create <span class="string">'student'</span>,<span class="string">'info'</span></span><br></pre></td></tr></table></figure><ul><li>desc</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; desc <span class="string">'student'</span></span><br></pre></td></tr></table></figure><ul><li>disable</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 停用表，防止对表进行写数据；在修改或删除表之前要 disable</span></span><br><span class="line">&gt; <span class="built_in">disable</span> <span class="string">'student'</span></span><br><span class="line">&gt; is_disable <span class="string">'student'</span></span><br></pre></td></tr></table></figure><ul><li>enable</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用表</span></span><br><span class="line">&gt; <span class="built_in">enable</span> <span class="string">'student'</span></span><br><span class="line">&gt; is_enable <span class="string">'student'</span></span><br></pre></td></tr></table></figure><ul><li>alter</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要先 disable</span></span><br><span class="line">&gt; alter <span class="string">'student'</span>, &#123;NAME =&gt; <span class="string">'info'</span>, VERSIONS =&gt; <span class="string">'5'</span>&#125;</span><br></pre></td></tr></table></figure><ul><li>drop</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要先 disable</span></span><br><span class="line">&gt; drop <span class="string">'student'</span></span><br></pre></td></tr></table></figure><ul><li>count</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看行数</span></span><br><span class="line">&gt; count <span class="string">'student'</span></span><br></pre></td></tr></table></figure><ul><li>truncate</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除表数据</span></span><br><span class="line">&gt; truncate <span class="string">'student'</span></span><br></pre></td></tr></table></figure><h4 id="8-2-3-DML"><a href="#8-2-3-DML" class="headerlink" title="8.2.3 DML"></a>8.2.3 DML</h4><p><code>Commands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve</code></p><ul><li>scan</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">&gt; scan <span class="string">'student'</span>, &#123;<span class="built_in">limit</span> =&gt; 5&#125;</span><br><span class="line"><span class="comment"># 查看每行最近十次修改的数据</span></span><br><span class="line">&gt; scan <span class="string">'student'</span>, &#123;RAW =&gt; <span class="literal">true</span>, VERSIONS =&gt; 10&#125;</span><br></pre></td></tr></table></figure><ul><li>put</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put '表名', '行键', '列族:列名', '值'</span></span><br><span class="line">&gt; put <span class="string">'student'</span>, <span class="string">'1001'</span>, <span class="string">'info:name'</span>, <span class="string">'Nick'</span></span><br></pre></td></tr></table></figure><ul><li>get</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; get <span class="string">'student'</span>,<span class="string">'1001'</span></span><br></pre></td></tr></table></figure><ul><li>delete</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除某rowkey的全部数据：</span></span><br><span class="line">&gt; deleteall <span class="string">'student'</span>, <span class="string">'1001'</span></span><br><span class="line"><span class="comment"># 删除某rowkey的某一列数据：</span></span><br><span class="line">&gt; delete <span class="string">'student'</span>, <span class="string">'1002'</span>, <span class="string">'info:sex'</span></span><br></pre></td></tr></table></figure><h4 id="8-2-4-其他操作"><a href="#8-2-4-其他操作" class="headerlink" title="8.2.4 其他操作"></a>8.2.4 其他操作</h4><ul><li>flush</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将内存数据落盘</span></span><br><span class="line">&gt; flush <span class="string">'student'</span></span><br></pre></td></tr></table></figure><h2 id="九、Flink-配置"><a href="#九、Flink-配置" class="headerlink" title="九、Flink 配置"></a>九、Flink 配置</h2><h3 id="9-1-Standalone-模式"><a href="#9-1-Standalone-模式" class="headerlink" title="9.1 Standalone 模式"></a>9.1 Standalone 模式</h3><p>1、准备安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink-1.10.1-bin-scala_2.12.tgz</span><br></pre></td></tr></table></figure><p>2、修改 <code>conf/flink-conf.yaml</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: hadoop102</span><br></pre></td></tr></table></figure><p>3、修改 <code>conf/slaves</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>4、分发</p><p>5、启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-cluster.sh</span><br></pre></td></tr></table></figure><p>6、访问 <a href="http://localhost:8081" target="_blank" rel="noopener">http://localhost:8081</a> 可以对 flink 集群和任务进行监控管理</p><p>7、提交任务</p><ul><li>web 模式</li></ul><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20210316201034608.png" alt="image-20210316201034608"></p><ul><li>命令行</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flink run -c com.atguigu.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop, 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vim 常用操作</title>
      <link href="/2020/09/17/Linux/vim%20%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
      <url>/2020/09/17/Linux/vim%20%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取消高亮</span></span><br><span class="line">:noh</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="取消搜索后高亮"><a href="#取消搜索后高亮" class="headerlink" title="取消搜索后高亮"></a>取消搜索后高亮</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># no high light search</span></span><br><span class="line">:nohlsearch</span><br><span class="line"><span class="comment"># 简写</span></span><br><span class="line">:noh</span><br></pre></td></tr></table></figure><h4 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w <span class="comment"># 移至下一单词</span></span><br><span class="line">b <span class="comment"># 移至上一单词</span></span><br></pre></td></tr></table></figure><h4 id="剪切、复制、粘贴、删除"><a href="#剪切、复制、粘贴、删除" class="headerlink" title="剪切、复制、粘贴、删除"></a>剪切、复制、粘贴、删除</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dd <span class="comment"># 删除当前行</span></span><br><span class="line">5dd <span class="comment"># 删除5行</span></span><br><span class="line"></span><br><span class="line">yy <span class="comment"># 复制当前行</span></span><br><span class="line">5yy <span class="comment"># 复制5行</span></span><br><span class="line"></span><br><span class="line">p <span class="comment"># 粘贴</span></span><br></pre></td></tr></table></figure><h4 id="设置-tab-键长度"><a href="#设置-tab-键长度" class="headerlink" title="设置 tab 键长度"></a>设置 tab 键长度</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:<span class="built_in">set</span> tabstop=4</span><br></pre></td></tr></table></figure><h4 id="开启自动缩进"><a href="#开启自动缩进" class="headerlink" title="开启自动缩进"></a>开启自动缩进</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">:<span class="built_in">set</span> autoindent</span><br><span class="line">ctrl+d <span class="comment"># 停止自动缩进</span></span><br></pre></td></tr></table></figure><h4 id="字符串替换"><a href="#字符串替换" class="headerlink" title="字符串替换"></a>字符串替换</h4><p><code>:s</code>（substitute）命令用来查找和替换字符串。语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:&#123;作用范围&#125;s/&#123;目标&#125;/&#123;替换&#125;/&#123;替换标志&#125;</span><br></pre></td></tr></table></figure><p>例如<code>:%s/foo/bar/g</code>会在全局范围(<code>%</code>)查找<code>foo</code>并替换为<code>bar</code>，所有出现都会被替换（<code>g</code>）。</p><p><strong>作用范围</strong></p><p>当前行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:s/foo/bar/g</span><br></pre></td></tr></table></figure><p>全文：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s/foo/bar/g</span><br></pre></td></tr></table></figure><p>2-11行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:5,12s/foo/bar/g</span><br></pre></td></tr></table></figure><p>当前行<code>.</code>与接下来两行<code>+2</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:.,+2s/foo/bar/g</span><br></pre></td></tr></table></figure><p><strong>替换标志</strong></p><p>上文中命令结尾的<code>g</code>即是替换标志之一，表示全局<code>global</code>替换（即替换目标的所有出现）。 还有很多其他有用的替换标志：</p><p>空替换标志表示只替换从光标位置开始，目标的第一次出现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s/foo/bar</span><br></pre></td></tr></table></figure><p><code>i</code>表示大小写不敏感查找，<code>I</code>表示大小写敏感：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">:%s/foo/bar/i</span><br><span class="line"># 等效于模式中的\c（不敏感）或\C（敏感）</span><br><span class="line">:%s/foo\c/bar</span><br></pre></td></tr></table></figure><p><code>c</code>表示需要确认，例如全局查找<code>&quot;foo&quot;</code>替换为<code>&quot;bar&quot;</code>并且需要确认：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:%s/foo/bar/gc</span><br></pre></td></tr></table></figure><p>回车后Vim会将光标移动到每一次<code>&quot;foo&quot;</code>出现的位置，并提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">replace with bar (y/n/a/q/l/^E/^Y)?</span><br></pre></td></tr></table></figure><p>按下<code>y</code>表示替换，<code>n</code>表示不替换，<code>a</code>表示替换所有，<code>q</code>表示退出查找模式， <code>l</code>表示替换当前位置并退出。<code>^E</code>与<code>^Y</code>是光标移动快捷键</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vim, 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux压缩命令 tar</title>
      <link href="/2020/09/01/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar%202/"/>
      <url>/2020/09/01/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar%202/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压（1.15版本后 tar 自动识别压缩方式）</span></span><br><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.gz -C /home/xxx</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、常用压缩参数"><a href="#一、常用压缩参数" class="headerlink" title="一、常用压缩参数"></a>一、常用压缩参数</h2><p><strong>必选参数，压缩解压都要用到其中一个：</strong></p><p>-c:  建立压缩档案</p><p>-x：解压</p><p>-t：查看内容</p><p>-r：向压缩归档文件末尾追加文件</p><p>-u：更新原压缩包中的文件</p><p><strong>可选参数：</strong></p><p>-z：有gzip属性的</p><p>-j： 有bz2属性的</p><p>-Z：有compress属性的</p><p>-v：显示所有过程</p><p>-O：将文件解开到标准输出</p><p>-C：解压时指定文件夹</p><p><strong>下面的参数-f是必须的</strong></p><p>-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。</p><h2 id="二、举个栗子"><a href="#二、举个栗子" class="headerlink" title="二、举个栗子"></a>二、举个栗子</h2><p><strong>压缩</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将目录里所有jpg文件打包成tar.jpg</span></span><br><span class="line">tar -cvf jpg.tar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</span></span><br><span class="line">tar -czvf jpg.tar.gz *.jpg</span><br><span class="line"><span class="comment"># 打包文件夹</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># rar格式的压缩，需要先下载 rar for linux</span></span><br><span class="line">rar a jpg.rar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># zip格式的压缩，需要先下载 zip for linux</span></span><br><span class="line">zip jpg.zip *.jpg</span><br><span class="line"><span class="comment"># zip 文件夹</span></span><br><span class="line">zip -r folder.zip folder</span><br></pre></td></tr></table></figure><p><strong>解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压 tar包</span></span><br><span class="line">tar -xvf file.tar </span><br><span class="line"><span class="comment"># 指定文件夹</span></span><br><span class="line">tar -xvf file.tar -C /home/xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压tar.gz</span></span><br><span class="line">tar -xzvf file.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 tar.bz2</span></span><br><span class="line">tar -xjvf file.tar.bz2  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压rar</span></span><br><span class="line">unrar e file.rar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压zip</span></span><br><span class="line">unzip file.zip</span><br></pre></td></tr></table></figure><p>从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.bz2</span><br><span class="line">tar -xvf filename.tar.xz</span><br><span class="line">tar -xvf filename.tar.Z</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux压缩命令 tar</title>
      <link href="/2020/09/01/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar/"/>
      <url>/2020/09/01/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压（1.15版本后 tar 自动识别压缩方式）</span></span><br><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.gz -C /home/xxx</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、常用压缩参数"><a href="#一、常用压缩参数" class="headerlink" title="一、常用压缩参数"></a>一、常用压缩参数</h2><p><strong>必选参数，压缩解压都要用到其中一个：</strong></p><p>-c:  建立压缩档案</p><p>-x：解压</p><p>-t：查看内容</p><p>-r：向压缩归档文件末尾追加文件</p><p>-u：更新原压缩包中的文件</p><p><strong>可选参数：</strong></p><p>-z：有gzip属性的</p><p>-j： 有bz2属性的</p><p>-Z：有compress属性的</p><p>-v：显示所有过程</p><p>-O：将文件解开到标准输出</p><p>-C：解压时指定文件夹</p><p><strong>下面的参数-f是必须的</strong></p><p>-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。</p><h2 id="二、举个栗子"><a href="#二、举个栗子" class="headerlink" title="二、举个栗子"></a>二、举个栗子</h2><p><strong>压缩</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将目录里所有jpg文件打包成tar.jpg</span></span><br><span class="line">tar -cvf jpg.tar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</span></span><br><span class="line">tar -czvf jpg.tar.gz *.jpg</span><br><span class="line"><span class="comment"># 打包文件夹</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># rar格式的压缩，需要先下载 rar for linux</span></span><br><span class="line">rar a jpg.rar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># zip格式的压缩，需要先下载 zip for linux</span></span><br><span class="line">zip jpg.zip *.jpg</span><br><span class="line"><span class="comment"># zip 文件夹</span></span><br><span class="line">zip -r folder.zip folder</span><br></pre></td></tr></table></figure><p><strong>解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压 tar包</span></span><br><span class="line">tar -xvf file.tar </span><br><span class="line"><span class="comment"># 指定文件夹</span></span><br><span class="line">tar -xvf file.tar -C /home/xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压tar.gz</span></span><br><span class="line">tar -xzvf file.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 tar.bz2</span></span><br><span class="line">tar -xjvf file.tar.bz2  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压rar</span></span><br><span class="line">unrar e file.rar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压zip</span></span><br><span class="line">unzip file.zip</span><br></pre></td></tr></table></figure><p>从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.bz2</span><br><span class="line">tar -xvf filename.tar.xz</span><br><span class="line">tar -xvf filename.tar.Z</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux压缩命令 tar</title>
      <link href="/2020/09/01/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar%203/"/>
      <url>/2020/09/01/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar%203/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压（1.15版本后 tar 自动识别压缩方式）</span></span><br><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.gz -C /home/xxx</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、常用压缩参数"><a href="#一、常用压缩参数" class="headerlink" title="一、常用压缩参数"></a>一、常用压缩参数</h2><p><strong>必选参数，压缩解压都要用到其中一个：</strong></p><p>-c:  建立压缩档案</p><p>-x：解压</p><p>-t：查看内容</p><p>-r：向压缩归档文件末尾追加文件</p><p>-u：更新原压缩包中的文件</p><p><strong>可选参数：</strong></p><p>-z：有gzip属性的</p><p>-j： 有bz2属性的</p><p>-Z：有compress属性的</p><p>-v：显示所有过程</p><p>-O：将文件解开到标准输出</p><p>-C：解压时指定文件夹</p><p><strong>下面的参数-f是必须的</strong></p><p>-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。</p><h2 id="二、举个栗子"><a href="#二、举个栗子" class="headerlink" title="二、举个栗子"></a>二、举个栗子</h2><p><strong>压缩</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将目录里所有jpg文件打包成tar.jpg</span></span><br><span class="line">tar -cvf jpg.tar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</span></span><br><span class="line">tar -czvf jpg.tar.gz *.jpg</span><br><span class="line"><span class="comment"># 打包文件夹</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># rar格式的压缩，需要先下载 rar for linux</span></span><br><span class="line">rar a jpg.rar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># zip格式的压缩，需要先下载 zip for linux</span></span><br><span class="line">zip jpg.zip *.jpg</span><br><span class="line"><span class="comment"># zip 文件夹</span></span><br><span class="line">zip -r folder.zip folder</span><br></pre></td></tr></table></figure><p><strong>解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压 tar包</span></span><br><span class="line">tar -xvf file.tar </span><br><span class="line"><span class="comment"># 指定文件夹</span></span><br><span class="line">tar -xvf file.tar -C /home/xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压tar.gz</span></span><br><span class="line">tar -xzvf file.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 tar.bz2</span></span><br><span class="line">tar -xjvf file.tar.bz2  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压rar</span></span><br><span class="line">unrar e file.rar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压zip</span></span><br><span class="line">unzip file.zip</span><br></pre></td></tr></table></figure><p>从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.bz2</span><br><span class="line">tar -xvf filename.tar.xz</span><br><span class="line">tar -xvf filename.tar.Z</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>构建Hadoop的Docker编译环境</title>
      <link href="/2020/09/01/Hadoop/%E6%9E%84%E5%BB%BA%20Hadoop%20%E7%9A%84%20Docker%20%E7%BC%96%E8%AF%91%E7%8E%AF%E5%A2%83/"/>
      <url>/2020/09/01/Hadoop/%E6%9E%84%E5%BB%BA%20Hadoop%20%E7%9A%84%20Docker%20%E7%BC%96%E8%AF%91%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h2 id="一、配置-docker-环境"><a href="#一、配置-docker-环境" class="headerlink" title="一、配置 docker 环境"></a>一、配置 docker 环境</h2><blockquote><p>参考链接：<br><a href="https://www.jianshu.com/p/0d3c17b4dddb" target="_blank" rel="noopener">Hadoop安装之一：使用Docker编译64位的Hadoop - 简书</a></p></blockquote><h3 id="1-制作-CentOS-7-基础镜像（可选）"><a href="#1-制作-CentOS-7-基础镜像（可选）" class="headerlink" title="1. 制作 CentOS 7 基础镜像（可选）"></a>1. 制作 CentOS 7 基础镜像（可选）</h3><p>Docker Hub上已经提供了<a href="https://link.jianshu.com?t=https://hub.docker.com/_/centos/" target="_blank" rel="noopener">CentOS7的官方镜像</a>，但并未激活 Systemd（用来启动守护进程），制作一个启动 Systemd 的镜像。（这里编译Hadoop其实用不到systemd）</p><ul><li>Dockerfile</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 镜像来源</span></span><br><span class="line">FROM centos:7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像创建者</span></span><br><span class="line">MAINTAINER <span class="string">"you"</span> &lt;your@email.here&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置一个环境变量</span></span><br><span class="line">ENV container docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行命令</span></span><br><span class="line"><span class="comment"># 设置systemd</span></span><br><span class="line">RUN (<span class="built_in">cd</span> /lib/systemd/system/sysinit.target.wants/; <span class="keyword">for</span> i <span class="keyword">in</span> *; <span class="keyword">do</span> [ <span class="variable">$i</span> == \</span><br><span class="line">systemd-tmpfiles-setup.service ] || rm -f <span class="variable">$i</span>; <span class="keyword">done</span>); \</span><br><span class="line">rm -f /lib/systemd/system/multi-user.target.wants/*;\</span><br><span class="line">rm -f /etc/systemd/system/*.wants/*;\</span><br><span class="line">rm -f /lib/systemd/system/<span class="built_in">local</span>-fs.target.wants/*; \</span><br><span class="line">rm -f /lib/systemd/system/sockets.target.wants/*udev*; \</span><br><span class="line">rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \</span><br><span class="line">rm -f /lib/systemd/system/basic.target.wants/*;\</span><br><span class="line">rm -f /lib/systemd/system/anaconda.target.wants/*;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂载一个本地文件夹</span></span><br><span class="line">VOLUME [ <span class="string">"/sys/fs/cgroup"</span> ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置容器启动时的执行命令</span></span><br><span class="line">CMD [<span class="string">"/usr/sbin/init"</span>]</span><br></pre></td></tr></table></figure><ul><li>生成镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t centos7-systemd .</span><br></pre></td></tr></table></figure><h3 id="2-安装-Oracle-Java"><a href="#2-安装-Oracle-Java" class="headerlink" title="2. 安装 Oracle Java"></a>2. 安装 Oracle Java</h3><blockquote><p>参考链接<br><a href="https://blog.csdn.net/weixin_40651304/article/details/78833642" target="_blank" rel="noopener">使用yum卸载、安装jdk_不做小白的博客-CSDN博客</a></p></blockquote><p>注意不要使用 openjdk，会导致编译 hive 时出现问题</p><ul><li>启动刚刚生成的镜像</li><li>从官网下载 oracle java <code>jdk-8u202-linux-x64.tar.gz</code></li><li>安装 Java，配置环境变量</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/<span class="built_in">local</span>/java</span><br><span class="line">cp jdk-8u202-linux-x64.tar.gz /usr/<span class="built_in">local</span>/java</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/java</span><br><span class="line">tar -xzvf jdk-8u202-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">vim /etc/profile</span><br><span class="line">~</span><br><span class="line">~</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/java/jdk1.8.0_202</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=/usr/<span class="built_in">local</span>/java/jdk1.8.0_202/jre  </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/<span class="built_in">local</span>/java/jdk1.8.0_202/bin  </span><br><span class="line"><span class="built_in">export</span> CLASSPATH=./:/usr/<span class="built_in">local</span>/java/jdk1.8.0_202/lib:/usr/<span class="built_in">local</span>/java/jdk1.8.0_202/jre/lib</span><br><span class="line">~</span><br><span class="line">~</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 JAVA 是否安装成功</span></span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><ul><li>保存镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit 容器id 镜像名</span><br></pre></td></tr></table></figure><h3 id="3-制作编译镜像"><a href="#3-制作编译镜像" class="headerlink" title="3. 制作编译镜像"></a>3. 制作编译镜像</h3><ul><li>编译脚本</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ vi compile.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认编译版本(支持传参)</span></span><br><span class="line">version=<span class="variable">$&#123;1:-2.7.3&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入源代码目录</span></span><br><span class="line"><span class="built_in">cd</span> /hadoop-<span class="variable">$version</span>-src</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始编译</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">"\n\ncompile hadoop <span class="variable">$version</span>..."</span></span><br><span class="line">mvn clean package -Pdist,native -DskipTests -Dtar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="keyword">if</span> [[ $? -eq 0]]; <span class="keyword">then</span></span><br><span class="line"> <span class="built_in">echo</span> -e <span class="string">"\n\ncompile hadoop <span class="variable">$version</span> success!\n\n"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"> <span class="built_in">echo</span> -e <span class="string">"\n\ncompile hadoop <span class="variable">$version</span> fail!\n\n"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><ul><li>Dockerfile（其中有不少安装包不是必要的）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 镜像来源(第二步生成的本地镜像)</span></span><br><span class="line">FROM centos7-systemd-java</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像创建者</span></span><br><span class="line">MAINTAINER <span class="string">"you"</span> &lt;your@email.here&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行命令安装环境依赖</span></span><br><span class="line"><span class="comment"># 使用 -y 同意全部询问</span></span><br><span class="line">RUN yum update -y &amp;&amp; \</span><br><span class="line">    yum groupinstall -y <span class="string">"Development Tools"</span> &amp;&amp; \</span><br><span class="line">    yum install -y wget \</span><br><span class="line">               protobuf-devel \</span><br><span class="line">               protobuf-compiler \</span><br><span class="line">               maven \</span><br><span class="line">               cmake \</span><br><span class="line">               pkgconfig \</span><br><span class="line">               openssl-devel \</span><br><span class="line">               zlib-devel \</span><br><span class="line">               gcc \</span><br><span class="line">               automake \</span><br><span class="line">               autoconf \</span><br><span class="line">               make</span><br><span class="line">               </span><br><span class="line"><span class="comment"># 复制编辑脚本文件到镜像中</span></span><br><span class="line">COPY compile.sh /root/compile.sh</span><br><span class="line"><span class="comment"># 设置脚本文件的可运行权限</span></span><br><span class="line">RUN chmod +x /root/compile.sh</span><br></pre></td></tr></table></figure><ul><li>生成镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker build -t centos7-hadoop-compiler .</span><br></pre></td></tr></table></figure><h2 id="二、编译源码"><a href="#二、编译源码" class="headerlink" title="二、编译源码"></a>二、编译源码</h2><ul><li>hive（大概10分钟）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -Pdist -DskipTests</span><br></pre></td></tr></table></figure><ul><li>hadoop（大概15分钟）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -Pdist,native -DskipTests -Dtar</span><br></pre></td></tr></table></figure><p>也可以使用 docker image 中的脚本编译</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> VERSION=2.7.3</span><br><span class="line">$ sudo docker run -v $(<span class="built_in">pwd</span>)/hadoop-<span class="variable">$VERSION</span>-src:/hadoop-<span class="variable">$VERSION</span>-src --privileged=<span class="literal">true</span>  centos7-hadoop-complier /root/compile.sh <span class="variable">$VERSION</span></span><br></pre></td></tr></table></figure><p><strong>要添加 privileged 参数！</strong></p><blockquote><p><a href="https://blog.csdn.net/halcyonbaby/article/details/43499409" target="_blank" rel="noopener">[docker]privileged参数_追寻神迹-CSDN博客</a></p></blockquote><p>使用该参数，container内的root拥有真正的root权限。<br>否则，container内的root只是外部的一个普通用户权限。</p><ul><li>总结</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pull docker image</span></span><br><span class="line">docker pull shuofxz/hadoop-compiler:1.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># === HADOOP ===</span></span><br><span class="line"><span class="comment"># hadoop download link</span></span><br><span class="line"><span class="comment"># new version</span></span><br><span class="line">https://hadoop.apache.org/releases.html</span><br><span class="line"><span class="comment"># old version</span></span><br><span class="line">https://archive.apache.org/dist/hadoop/common/</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile command (about 15 minutes to complete)</span></span><br><span class="line">mvn package -Pdist,native -DskipTests -Dtar</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile with script file</span></span><br><span class="line">$ <span class="built_in">export</span> VERSION=2.7.3</span><br><span class="line">$ sudo docker run -v $(<span class="built_in">pwd</span>)/hadoop-<span class="variable">$VERSION</span>-src:/hadoop-<span class="variable">$VERSION</span>-src --privileged=<span class="literal">true</span> shuofxz/hadoop-compiler:1.0 /root/hadoop-compile.sh <span class="variable">$VERSION</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># === HIVE ===</span></span><br><span class="line"><span class="comment"># hive download link</span></span><br><span class="line"><span class="comment"># select corresponding branch src file to download</span></span><br><span class="line">https://github.com/apache/hive</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile command (about 10 minutes to complete)</span></span><br><span class="line">mvn clean package -Pdist -DskipTests</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile with script file</span></span><br><span class="line">$ <span class="built_in">export</span> VERSION=2.3.0</span><br><span class="line">$ sudo docker run -v $(<span class="built_in">pwd</span>)/hive-rel-release-<span class="variable">$VERSION</span>:/hive-rel-release-<span class="variable">$VERSION</span> --privileged=<span class="literal">true</span> shuofxz/hadoop-compiler:1.0 /root/hive-compile.sh <span class="variable">$VERSION</span></span><br></pre></td></tr></table></figure><p>hadoop-2.7.0-src.tar.gz  release-2.3.0.tar.gz</p><hr><p>已包括各种库的 image，可以直接编译 hadoop（不好用）</p><p>GitHub - kiwenlau/compile-hadoop: Compile Hadoop in Docker container<br><a href="https://github.com/kiwenlau/compile-hadoop" target="_blank" rel="noopener">https://github.com/kiwenlau/compile-hadoop</a></p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker,教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux压缩命令 tar</title>
      <link href="/2020/09/01/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar/"/>
      <url>/2020/09/01/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%20tar/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压（1.15版本后 tar 自动识别压缩方式）</span></span><br><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.gz -C /home/xxx</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、常用压缩参数"><a href="#一、常用压缩参数" class="headerlink" title="一、常用压缩参数"></a>一、常用压缩参数</h2><p><strong>必选参数，压缩解压都要用到其中一个：</strong></p><p>-c:  建立压缩档案</p><p>-x：解压</p><p>-t：查看内容</p><p>-r：向压缩归档文件末尾追加文件</p><p>-u：更新原压缩包中的文件</p><p><strong>可选参数：</strong></p><p>-z：有gzip属性的</p><p>-j： 有bz2属性的</p><p>-Z：有compress属性的</p><p>-v：显示所有过程</p><p>-O：将文件解开到标准输出</p><p>-C：解压时指定文件夹</p><p><strong>下面的参数-f是必须的</strong></p><p>-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。</p><h2 id="二、举个栗子"><a href="#二、举个栗子" class="headerlink" title="二、举个栗子"></a>二、举个栗子</h2><p><strong>压缩</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将目录里所有jpg文件打包成tar.jpg</span></span><br><span class="line">tar -cvf jpg.tar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</span></span><br><span class="line">tar -czvf jpg.tar.gz *.jpg</span><br><span class="line"><span class="comment"># 打包文件夹</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># rar格式的压缩，需要先下载 rar for linux</span></span><br><span class="line">rar a jpg.rar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># zip格式的压缩，需要先下载 zip for linux</span></span><br><span class="line">zip jpg.zip *.jpg</span><br><span class="line"><span class="comment"># zip 文件夹</span></span><br><span class="line">zip -r folder.zip folder</span><br></pre></td></tr></table></figure><p><strong>解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压 tar包</span></span><br><span class="line">tar -xvf file.tar </span><br><span class="line"><span class="comment"># 指定文件夹</span></span><br><span class="line">tar -xvf file.tar -C /home/xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压tar.gz</span></span><br><span class="line">tar -xzvf file.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 tar.bz2</span></span><br><span class="line">tar -xjvf file.tar.bz2  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压rar</span></span><br><span class="line">unrar e file.rar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压zip</span></span><br><span class="line">unzip file.zip</span><br></pre></td></tr></table></figure><p>从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.bz2</span><br><span class="line">tar -xvf filename.tar.xz</span><br><span class="line">tar -xvf filename.tar.Z</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux压缩命令 tar</title>
      <link href="/2020/09/01/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux/"/>
      <url>/2020/09/01/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压（1.15版本后 tar 自动识别压缩方式）</span></span><br><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.gz -C /home/xxx</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="一、常用压缩参数"><a href="#一、常用压缩参数" class="headerlink" title="一、常用压缩参数"></a>一、常用压缩参数</h2><p><strong>必选参数，压缩解压都要用到其中一个：</strong></p><p>-c:  建立压缩档案</p><p>-x：解压</p><p>-t：查看内容</p><p>-r：向压缩归档文件末尾追加文件</p><p>-u：更新原压缩包中的文件</p><p><strong>可选参数：</strong></p><p>-z：有gzip属性的</p><p>-j： 有bz2属性的</p><p>-Z：有compress属性的</p><p>-v：显示所有过程</p><p>-O：将文件解开到标准输出</p><p>-C：解压时指定文件夹</p><p><strong>下面的参数-f是必须的</strong></p><p>-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。</p><h2 id="二、举个栗子"><a href="#二、举个栗子" class="headerlink" title="二、举个栗子"></a>二、举个栗子</h2><p><strong>压缩</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将目录里所有jpg文件打包成tar.jpg</span></span><br><span class="line">tar -cvf jpg.tar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz</span></span><br><span class="line">tar -czvf jpg.tar.gz *.jpg</span><br><span class="line"><span class="comment"># 打包文件夹</span></span><br><span class="line">tar -czvf xxx.tar.gz /etc/folder</span><br><span class="line"></span><br><span class="line"><span class="comment"># rar格式的压缩，需要先下载 rar for linux</span></span><br><span class="line">rar a jpg.rar *.jpg </span><br><span class="line"></span><br><span class="line"><span class="comment"># zip格式的压缩，需要先下载 zip for linux</span></span><br><span class="line">zip jpg.zip *.jpg</span><br><span class="line"><span class="comment"># zip 文件夹</span></span><br><span class="line">zip -r folder.zip folder</span><br></pre></td></tr></table></figure><p><strong>解压</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压 tar包</span></span><br><span class="line">tar -xvf file.tar </span><br><span class="line"><span class="comment"># 指定文件夹</span></span><br><span class="line">tar -xvf file.tar -C /home/xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压tar.gz</span></span><br><span class="line">tar -xzvf file.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压 tar.bz2</span></span><br><span class="line">tar -xjvf file.tar.bz2  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压rar</span></span><br><span class="line">unrar e file.rar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压zip</span></span><br><span class="line">unzip file.zip</span><br></pre></td></tr></table></figure><p>从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf filename.tar.gz</span><br><span class="line">tar -xvf filename.tar.bz2</span><br><span class="line">tar -xvf filename.tar.xz</span><br><span class="line">tar -xvf filename.tar.Z</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux, 压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 批量操作</title>
      <link href="/2020/08/28/Docker/Docker%20%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/"/>
      <url>/2020/08/28/Docker/Docker%20%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi $(docker images | grep <span class="string">"none"</span> | awk <span class="string">'&#123;print $3&#125;'</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>列出所有的容器 ID</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -aq</span><br></pre></td></tr></table></figure><ul><li>停止所有的容器</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop $(docker ps -aq)</span><br></pre></td></tr></table></figure><ul><li>删除所有的容器</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rm $(docker ps -aq)</span><br></pre></td></tr></table></figure><ul><li>删除所有的镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi $(docker images -q)</span><br></pre></td></tr></table></figure><ul><li>删除指定名称镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi $(docker images | grep <span class="string">"none"</span> | awk <span class="string">'&#123;print $3&#125;'</span>)</span><br></pre></td></tr></table></figure><ul><li>复制文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker cp mycontainer:/opt/file.txt /opt/local/</span><br><span class="line">docker cp /opt/local/file.txt mycontainer:/opt/</span><br></pre></td></tr></table></figure><p>现在的docker有了专门清理资源(container、image、网络)的命令。 </p><p>docker 1.13 中增加了 <code>docker system prune</code>的命令，针对container、image可以使用<code>docker container prune</code>、<code>docker image prune</code>命令。</p><ul><li>删除所有不使用的镜像</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker image prune --force --all</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">docker image prune -f -a</span><br></pre></td></tr></table></figure><ul><li>删除所有停止的容器</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container prune -f</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cron语法</title>
      <link href="/2020/08/17/Linux/cron%E8%AF%AD%E6%B3%95/"/>
      <url>/2020/08/17/Linux/cron%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*   *    *   *   *   *  *</span><br><span class="line">秒 分钟  小时 天  月  星期 年</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------------</span></span><br><span class="line"><span class="comment"># linux crontab 只有5个</span></span><br><span class="line"> *    *   *   *   * </span><br><span class="line">分钟  小时 天  月  星期</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="1-表达式详解"><a href="#1-表达式详解" class="headerlink" title="1 表达式详解"></a>1 表达式详解</h2><p>一个cron表达式有至少6个（也可能7个）有空格分隔的时间元素。</p><p>按顺序依次为</p><ul><li>1 秒（0~59）</li><li>2 分钟（0~59）</li><li>3 小时（0~23）</li><li>4 天（0~31）</li><li>5 月（0~11）</li><li>6 星期（1~7 1=SUN 或 SUN，MON，TUE，WED，THU，FRI，SAT）</li><li>7 年份（1970－2099）</li></ul><p>每个元素格式：</p><ul><li>一个具体值（如6）</li><li>一个连续区间（9-12）</li><li>一个列表(1,3,5)</li></ul><p>特殊字符</p><ul><li>通配符（*），所有可能的值</li><li>空符号（？），表示不指定值<ul><li>由于”月份中的日期”和”星期中的日期”这两个元素互斥的,必须要对其中一个设置?</li></ul></li><li>增量符（/）<ul><li>如第二位12/10 表示从第12分钟开始，每10分钟（它和“12，22，32…”）</li></ul></li><li>最后（L）<ul><li>仅被用于天（月）和天（星期）两个子表达式，它是单词“last”的缩写</li><li>“6L”表示这个月的倒数第６天</li></ul></li><li>平日（W）<ul><li>仅能用于日域中，它用来指定离指定日的最近的一个工作日（1-5）</li><li>日域中的 15W 意味着 “离该月15号的最近一个平日</li></ul></li></ul><h2 id="2-例子"><a href="#2-例子" class="headerlink" title="2 例子"></a>2 例子</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0 0 10,14,16 * * ?  每天上午10点，下午2点，4点</span><br><span class="line">0 0/30 9-17 * * ?    朝九晚五工作时间内每半小时</span><br><span class="line">0 0 12 ? * WED  表示每个星期三中午12点</span><br><span class="line">0 0 12 * * ?  每天中午12点触发</span><br><span class="line">0 15 10 ? * *  每天上午10:15触发</span><br><span class="line">0 15 10 * * ? 2005 2005年的每天上午10:15触发</span><br><span class="line">0 * 14 * * ?   在每天下午2点到下午2:59期间的每1分钟触发</span><br><span class="line">0 0/5 14,18 * * ?  在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发</span><br><span class="line">0 0-5 14 * * ? 在每天下午2点到下午2:05期间的每1分钟触发</span><br><span class="line">0 10,44 14 ? 3 WED 每年三月的星期三的下午2:10和2:44触发</span><br><span class="line">0 15 10 ? * MON-FRI 周一至周五的上午10:15触发</span><br><span class="line">0 15 10 L * ? 每月最后一日的上午10:15触发</span><br><span class="line">0 15 10 ? * 6L 每月的最后一个星期五上午10:15触发</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cron </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo常用命令</title>
      <link href="/2020/08/13/%E9%85%8D%E7%BD%AE/Hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/08/13/%E9%85%8D%E7%BD%AE/Hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 生成静态文件</span></span><br><span class="line">hexo g</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动服务</span></span><br><span class="line">hexo s</span><br><span class="line"><span class="meta">#</span><span class="bash"> 部署</span></span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><ul><li><strong>生成静态文件</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br><span class="line"><span class="meta">#</span><span class="bash"> 简写</span></span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure><ul><li><strong>启动服务预览文章</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br><span class="line"><span class="meta">#</span><span class="bash"> 简写</span></span><br><span class="line">hexo s</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定端口</span></span><br><span class="line">hexo server -p 5000</span><br></pre></td></tr></table></figure><ul><li><strong>一键部署</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy</span><br><span class="line"><span class="meta">#</span><span class="bash"> 简写</span></span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2020/08/12/Docker/Docker%E9%83%A8%E7%BD%B2Vue%E9%A1%B9%E7%9B%AE/"/>
      <url>/2020/08/12/Docker/Docker%E9%83%A8%E7%BD%B2Vue%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>[手把手系列之]Docker 部署 vue 项目 - 掘金<br><a href="https://juejin.im/post/6844903837774397447" target="_blank" rel="noopener">https://juejin.im/post/6844903837774397447</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/07/10/hello-world/"/>
      <url>/2020/07/10/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mac使用代理ssh远程连接服务器 &amp; keep alive</title>
      <link href="/2020/07/10/%E9%85%8D%E7%BD%AE/Mac%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86ssh%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%20&amp;%20keep%20alive/"/>
      <url>/2020/07/10/%E9%85%8D%E7%BD%AE/Mac%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86ssh%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%20&amp;%20keep%20alive/</url>
      
        <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接连接</span></span><br><span class="line">ssh -p 端口号 服务器用户名@ip地址</span><br><span class="line"><span class="comment"># eg: ssh -p 22 userkunyu@119.29.37.63</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过代理连接</span></span><br><span class="line">ssh -o ProxyCommand=<span class="string">"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p"</span> 需要访问的服务器的用户名@需要访问的服务器ip</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="1-直接连接"><a href="#1-直接连接" class="headerlink" title="1 直接连接"></a>1 直接连接</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ssh -p 端口号 服务器用户名@ip地址</span></span><br><span class="line">ssh -p 22 userkunyu@119.29.37.63</span><br></pre></td></tr></table></figure><h2 id="2-通过代理连接"><a href="#2-通过代理连接" class="headerlink" title="2 通过代理连接"></a>2 通过代理连接</h2><ol><li>直接连接</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ssh -o ProxyCommand="nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p" 需要访问的服务器的用户名@需要访问的服务器ip</span></span><br><span class="line">ssh -o ProxyCommand=<span class="string">"nc -X 5 -x 192.168.0.255:9999 %h %p"</span> user_name@192.168.77.200</span><br></pre></td></tr></table></figure><ol start="2"><li>使用SSH配置文件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi ~/.ssh/config</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Host *</span><br><span class="line">    ProxyCommand nc -X 5 -x 192.168.0.255:9999 %h %p</span><br></pre></td></tr></table></figure><p>配置好了之后就可以和直接连接一样使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh uesr@ip</span><br></pre></td></tr></table></figure><blockquote><p>Mac下SSH跳点连接及代理连接_Dawnworld-CSDN博客_mac ssh 代理<br><a href="https://blog.csdn.net/thundon/article/details/46858957" target="_blank" rel="noopener">https://blog.csdn.net/thundon/article/details/46858957</a></p></blockquote><h2 id="3-Keep-alive"><a href="#3-Keep-alive" class="headerlink" title="3 Keep alive"></a>3 Keep alive</h2><blockquote><p><a href="http://bluebiu.com/blog/iterm2-ssh-session-idle.html" target="_blank" rel="noopener">http://bluebiu.com/blog/iterm2-ssh-session-idle.html</a></p></blockquote><p><strong>方案一：</strong></p><p>在本机 <code>vim ~/.ssh/config</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 在开头添加</span><br><span class="line">Host *</span><br><span class="line">    ServerAliveInterval 60</span><br></pre></td></tr></table></figure><p>我觉得60秒就好了，而且基本去连的机器都保持，所以配置了<code>*</code>，如果有需要针对某个机器，可以自行配置为需要的<code>serverHostName</code>。</p><p><strong>方案二：</strong></p><p>单次连接</p><p>添加下面的参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -o ServerAliveInterval=30 user@host</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac安装node</title>
      <link href="/2020/07/10/%E9%85%8D%E7%BD%AE/Mac%E5%AE%89%E8%A3%85node/"/>
      <url>/2020/07/10/%E9%85%8D%E7%BD%AE/Mac%E5%AE%89%E8%A3%85node/</url>
      
        <content type="html"><![CDATA[<p>Mac安装及降级node版本</p><a id="more"></a><h2 id="1-安装最新版Node"><a href="#1-安装最新版Node" class="headerlink" title="1 安装最新版Node"></a>1 安装最新版Node</h2><ol><li>安装HomeBrew</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/ruby -e <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)</span>"</span></span><br></pre></td></tr></table></figure><ol start="2"><li>安装Node</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></figure><ol start="3"><li>验证Node是否安装成功</li></ol><p>输入下面两条指令看是否可以都输出版本号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure><h2 id="2-降级Node"><a href="#2-降级Node" class="headerlink" title="2 降级Node"></a>2 降级Node</h2><p>由于开发需要或版本兼容性，需要安装低版本的Node，按下面的方式操作</p><ol><li><p>卸载Node</p><p>如果你是按前面的方法安装的Node，则用下面的命令卸载</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew uninstall node</span><br></pre></td></tr></table></figure><ol start="2"><li>查看可用的Node版本</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew search node</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Formulae</span><br><span class="line">libbitcoin-node      node                 node-sass            node@12            nodebrew             nodenvllnode               node-build           node@10              node_exporter        nodeenv</span><br></pre></td></tr></table></figure><ol start="3"><li>安装你需要的版本</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里安装v12版本</span></span><br><span class="line">brew install node@12</span><br></pre></td></tr></table></figure><ol start="4"><li>连接Node</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew link node@12</span><br><span class="line"><span class="comment"># 这一步可能会报错, 按照提示执行命令就ok了, 比如我最后执行的是brew link --overwrite --force node@12</span></span><br></pre></td></tr></table></figure><ol start="5"><li>检查Node是否安装成功</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iTerm2配置</title>
      <link href="/2020/07/10/%E9%85%8D%E7%BD%AE/iTerm2%E9%85%8D%E7%BD%AE/"/>
      <url>/2020/07/10/%E9%85%8D%E7%BD%AE/iTerm2%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>iTerm2配置</p><p>Oh-my-zsh安装，主题配置</p><a id="more"></a><h2 id="1-安装iTerm2"><a href="#1-安装iTerm2" class="headerlink" title="1 安装iTerm2"></a>1 安装iTerm2</h2><p>iTerm2 是一款完全免费的，专为 Mac OS 用户打造的命令行应用。直接在官网上 <a href="http://iterm2.com/" target="_blank" rel="noopener">http://iterm2.com/</a> 下载并安装即可。</p><p>设置为默认终端</p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/Snipaste_2020-07-10_14-26-44.jpg" alt="Snipaste_2020-07-10_14-26-44" style="zoom: 33%;"><h2 id="2-安装-oh-my-zsh"><a href="#2-安装-oh-my-zsh" class="headerlink" title="2 安装 oh-my-zsh"></a>2 安装 oh-my-zsh</h2><p>bash是mac中terminal自带的shell，把它换成oh-my-zsh，这个的功能要多得多。拥有语法高亮，命令行tab补全，自动提示符，显示Git仓库状态等功能。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c <span class="string">"<span class="variable">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>"</span></span><br></pre></td></tr></table></figure><p><strong>解决权限问题</strong></p><p>如果安装完重启iterm之后，出现下面的提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[oh-my-zsh] Insecure completion-dependent directories detected:</span><br><span class="line">drwxrwxrwx 7 hans admin 238 2 9 10:13 /usr/local/share/zsh</span><br><span class="line">drwxrwxrwx 6 hans admin 204 10 1 2017 /usr/local/share/zsh/site-functions</span><br><span class="line"></span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>解决方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 /usr/<span class="built_in">local</span>/share/zsh</span><br><span class="line">chmod 755 /usr/<span class="built_in">local</span>/share/zsh/site-functions</span><br></pre></td></tr></table></figure><h2 id="3-配置主题"><a href="#3-配置主题" class="headerlink" title="3 配置主题"></a>3 配置主题</h2><h2 id="4-Vim配置"><a href="#4-Vim配置" class="headerlink" title="4 Vim配置"></a>4 Vim配置</h2><p>设置鼠标滚动</p><p><img src="https://raw.githubusercontent.com/shuopic/ImgBed/master/NoteImgs/image-20200710182315449.png" alt="image-20200710182315449"></p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux后台执行命令 nohup</title>
      <link href="/2019/10/22/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%20nohup/"/>
      <url>/2019/10/22/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%20nohup/</url>
      
        <content type="html"><![CDATA[<p>当在终端工作时，可能一个持续运行的作业占住屏幕输出，或终端退出时导致命令结束。为了避免这些问题，可以将这些进程放到后台运行，且不受终端关闭的影响，可使用下面的方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup command &gt; myout.file 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="1-后台执行命令"><a href="#1-后台执行命令" class="headerlink" title="1 后台执行命令"></a>1 后台执行命令</h2><h3 id="1-1-命令-amp"><a href="#1-1-命令-amp" class="headerlink" title="1.1 命令&amp;"></a>1.1 命令<code>&amp;</code></h3><p>在命令后面加上<code>&amp;</code>实现后台运行（控制台关掉(退出帐户时)，作业就会<strong>停止</strong>运行）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command &amp;</span><br></pre></td></tr></table></figure><p>例：<code>python run.py &amp;</code></p><h3 id="1-2-命令nohup"><a href="#1-2-命令nohup" class="headerlink" title="1.2 命令nohup"></a>1.2 命令<code>nohup</code></h3><p><code>nohup</code>命令可以在你退出帐户之后继续运行相应的进程。nohup就是不挂起的意思( no hang up)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup command &amp;</span><br></pre></td></tr></table></figure><p>例：<code>nohup run.py &amp;</code></p><h2 id="2-kill进程"><a href="#2-kill进程" class="headerlink" title="2 kill进程"></a>2 kill进程</h2><p>执行后台任务命令后，会返回一个进程号，可通过这个进程号kill掉进程。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -9 进程号</span><br></pre></td></tr></table></figure><h2 id="3-输出重定向"><a href="#3-输出重定向" class="headerlink" title="3 输出重定向"></a>3 输出重定向</h2><p>由于使用前面的命令将任务放到后台运行，因此任务的输出也不打印到屏幕上了，所以需要将输出重定向到文件中，以方便查看输出内容。</p><ul><li>将输出重定向到 file（覆盖）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command1 &gt; file1</span><br></pre></td></tr></table></figure><ul><li>将输出重定向到 file（追加）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command1 &gt;&gt; file1</span><br></pre></td></tr></table></figure><ul><li>将 stdout 和 stderr 合并后重定向到 file<ul><li>2&gt;1代表什么，2与&gt;结合代表错误重定向，而1则代表错误重定向到一个文件1，而不代表标准输出；换成2&gt;&amp;1，&amp;与1结合就代表标准输出了，就变成错误重定向到标准输出.</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">command1 &gt; file1 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p><strong>完整写法：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup command &gt;out.file 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h2 id="4-其他"><a href="#4-其他" class="headerlink" title="4 其他"></a>4 其他</h2><ul><li>nohup执行python程序时，print无法输出<ul><li>这是因为python的输出有缓冲，导致nohup.out并不能够马上看到输出</li><li>python 有个-u参数，使得python不启用缓冲</li><li><code>nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;</code></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux统计文件数目 wc</title>
      <link href="/2019/10/22/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E7%BB%9F%E8%AE%A1%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E6%95%B0%E7%9B%AE%20wc/"/>
      <url>/2019/10/22/Linux/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/Linux%E7%BB%9F%E8%AE%A1%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E6%95%B0%E7%9B%AE%20wc/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l | grep &quot;^-&quot; | wc -l</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="1-统计文件夹下的文件数目"><a href="#1-统计文件夹下的文件数目" class="headerlink" title="1 统计文件夹下的文件数目"></a>1 统计文件夹下的文件数目</h2><ul><li>统计当前目录下文件的个数（不包括目录）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l | grep &quot;^-&quot; | wc -l</span><br></pre></td></tr></table></figure><ul><li>统计当前目录下文件的个数（包括子目录）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls -lR| grep &quot;^-&quot; | wc -l</span><br></pre></td></tr></table></figure><ul><li>查看某目录下文件夹(目录)的个数（包括子目录）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls -lR | grep &quot;^d&quot; | wc -l</span><br></pre></td></tr></table></figure><p><strong>命令原理：</strong></p><ul><li><code>ls -l</code><ul><li>详细输出该文件夹下文件信息</li><li><code>ls -lR</code>是列出所有文件，包括子目录</li></ul></li><li><code>grep &quot;^-&quot;</code><ul><li>过滤<code>ls</code>的输出信息，只保留一般文件；只保留目录是<code>grep &quot;^d&quot;</code></li></ul></li><li><code>wc -l</code><ul><li>统计输出信息的行数</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python打印更详细的异常信息</title>
      <link href="/2019/10/21/Python/Python%E6%89%93%E5%8D%B0%E6%9B%B4%E8%AF%A6%E7%BB%86%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF/"/>
      <url>/2019/10/21/Python/Python%E6%89%93%E5%8D%B0%E6%9B%B4%E8%AF%A6%E7%BB%86%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<p>打印Python异常信息的几种方式</p><a id="more"></a><h2 id="1-简单的异常信息"><a href="#1-简单的异常信息" class="headerlink" title="1 简单的异常信息"></a>1 简单的异常信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    a = <span class="number">1</span>/<span class="number">0</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><p>打印最简单的message信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">division by zero</span><br></pre></td></tr></table></figure><h2 id="2-更完整的信息"><a href="#2-更完整的信息" class="headerlink" title="2 更完整的信息"></a>2 更完整的信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    a = <span class="number">1</span>/<span class="number">0</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">'str(e):\t'</span>, e)</span><br><span class="line">    print(<span class="string">'repr(e):\t'</span>, repr(e))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'traceback.format_exc():\n%s'</span> % traceback.format_exc()) <span class="comment">#字符串</span></span><br><span class="line">    traceback.print_exc() <span class="comment">#执行函数</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">str(e): division by zero</span><br><span class="line">repr(e): ZeroDivisionError(&apos;division by zero&apos;)</span><br><span class="line">traceback.format_exc():</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/Users/ace/Play/test/异常信息.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    a = 1/0</span><br><span class="line">ZeroDivisionError: division by zero</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/Users/ace/Play/test/异常信息.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    a = 1/0</span><br><span class="line">ZeroDivisionError: division by zero</span><br></pre></td></tr></table></figure><p><code>traceback.format_exc()</code>和<code>traceback.print_exc()</code>都可以打印完整的错误信息</p><p><code>traceback.format_exc()</code>返回值为字符串</p><p><code>traceback.print_exc()</code>是一个执行函数，直接在控制台打印错误信息</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python, 异常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【转】持续集成 Continuous Integration</title>
      <link href="/2019/10/18/%E9%85%8D%E7%BD%AE/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%20Continuous%20Integration/"/>
      <url>/2019/10/18/%E9%85%8D%E7%BD%AE/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%20Continuous%20Integration/</url>
      
        <content type="html"><![CDATA[<p><a href="https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html" target="_blank" rel="noopener">持续集成 Continuous Integration</a></p><a id="more"></a>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-加快pip安装速度</title>
      <link href="/2019/10/16/Python/Python-%E5%8A%A0%E5%BF%ABpip%E5%AE%89%E8%A3%85%E9%80%9F%E5%BA%A6/"/>
      <url>/2019/10/16/Python/Python-%E5%8A%A0%E5%BF%ABpip%E5%AE%89%E8%A3%85%E9%80%9F%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>PIP安装时使用国内镜像，加快下载速度</p><a id="more"></a><h2 id="0-国内源"><a href="#0-国内源" class="headerlink" title="0 国内源"></a>0 国内源</h2><p>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a></p><p>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a></p><p>华中理工大学：<a href="http://pypi.hustunique.com/" target="_blank" rel="noopener">http://pypi.hustunique.com/</a></p><p>山东理工大学：<a href="http://pypi.sdutlinux.org/" target="_blank" rel="noopener">http://pypi.sdutlinux.org/</a> </p><p>豆瓣：<a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></p><h2 id="1-临时使用"><a href="#1-临时使用" class="headerlink" title="1 临时使用"></a>1 临时使用</h2><p> 可以在使用pip的时候加参数<code>-i https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>例如：</p><p><code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy</code></p><h2 id="2-永久修改"><a href="#2-永久修改" class="headerlink" title="2 永久修改"></a>2 永久修改</h2><p>这样就不用每次都添加国内镜像源地址了</p><p>Linux下，修改<code>~/.pip/pip.conf</code>（没有就创建一个文件夹及文件）</p><p>打开文件，添加内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br></pre></td></tr></table></figure><p> windows下，直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini，</p><p>内容同上</p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo多台电脑同步</title>
      <link href="/2019/10/16/%E9%85%8D%E7%BD%AE/Hexo%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E5%90%8C%E6%AD%A5/"/>
      <url>/2019/10/16/%E9%85%8D%E7%BD%AE/Hexo%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<p>如果换了电脑该如何同步Hexo的源文件？把hexo文件从一个电脑cope到另外一个电脑吗？答案肯定不是这样的，因为这里面有好多依赖包，好几万个文件呢，这样显然不合理。</p><p>本文提供一种多台电脑同步源文件的方法。</p><a id="more"></a><h2 id="0-解决思路"><a href="#0-解决思路" class="headerlink" title="0 解决思路"></a>0 解决思路</h2><p>使用GitHub的分支！在博客对应的仓库中新建一个分支。一个分支用来存放Hexo生成的网站原始的文件，另一个分支用来存放生成的静态网页。</p><h2 id="1-创建分支"><a href="#1-创建分支" class="headerlink" title="1 创建分支"></a>1 创建分支</h2><h3 id="1-1-创建新分支"><a href="#1-1-创建新分支" class="headerlink" title="1.1 创建新分支"></a>1.1 创建新分支</h3><p>命令行操作：</p><p>GitHub操作：</p><p>点击branch按钮，输入新的分支名<code>source</code>，点创建。</p><h3 id="1-2-设置默认分支"><a href="#1-2-设置默认分支" class="headerlink" title="1.2 设置默认分支"></a>1.2 设置默认分支</h3><p>准备在<code>source</code>分支中存放源文件，<code>master</code>中存放生成的网页，因此将<code>source</code>设置为默认分支，方便同步文件。</p><p>在仓库<code>-&gt;Settings-&gt;Branches-&gt;Default branch</code>中将默认分支设为<code>source</code>，save保存</p><h2 id="2-源文件上传到GitHub"><a href="#2-源文件上传到GitHub" class="headerlink" title="2 源文件上传到GitHub"></a>2 源文件上传到GitHub</h2><ol><li>选好一个本地文件夹，执行</li></ol><p><code>git clone git@github.com:Simon-Ace/Simon-Ace.github.io.git(替换成你的仓库)</code></p><ol start="2"><li><p>在克隆到本地的<code>Simon-Ace.github.io</code>中，把除了.git 文件夹外的所有文件都删掉</p></li><li><p>把之前我们写的博客源文件全部复制过来，除了<code>.deploy_git</code></p></li></ol><p>复制过来的源文件应该有一个<code>.gitignore</code>，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br></pre></td></tr></table></figure><p>注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的<code>.git</code>文件夹删掉，因为git不能嵌套上传。</p><ol start="4"><li>提交更改</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit –m "add branch"</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><hr><p>参考文章：</p><p><a href="https://juejin.im/post/5acf22e6f265da23994eeac9" target="_blank" rel="noopener">https://juejin.im/post/5acf22e6f265da23994eeac9</a></p><p><a href="https://www.zhihu.com/question/21193762" target="_blank" rel="noopener">https://www.zhihu.com/question/21193762</a></p>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一台电脑配置多个git账号</title>
      <link href="/2019/10/14/Linux/%E9%85%8D%E7%BD%AE%E5%A4%9A%E4%B8%AAgit%E8%B4%A6%E5%8F%B7/"/>
      <url>/2019/10/14/Linux/%E9%85%8D%E7%BD%AE%E5%A4%9A%E4%B8%AAgit%E8%B4%A6%E5%8F%B7/</url>
      
        <content type="html"><![CDATA[<h2 id="1-清除git全局设置"><a href="#1-清除git全局设置" class="headerlink" title="1 清除git全局设置"></a>1 清除git全局设置</h2><p>如果配置第一个账号的时候使用<code>git config --global</code>设置过，就先要取消掉，否则两个账号肯定会冲突</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 取消global</span></span><br><span class="line">git config --global --unset user.name</span><br><span class="line">git config --global --unset user.email</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-生成新账号的SSH-keys"><a href="#2-生成新账号的SSH-keys" class="headerlink" title="2 生成新账号的SSH keys"></a>2 生成新账号的SSH keys</h2><h3 id="2-1-用-ssh-keygen-命令生成密钥"><a href="#2-1-用-ssh-keygen-命令生成密钥" class="headerlink" title="2.1 用 ssh-keygen 命令生成密钥"></a>2.1 用 <code>ssh-keygen</code> 命令生成密钥</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen -t rsa -C <span class="string">"new email"</span></span></span><br></pre></td></tr></table></figure><p>平时都是直接回车，默认生成 <code>id_rsa</code> 和 <code>id_rsa.pub</code>。这里特别需要注意，出现提示输入文件名的时候(<code>Enter file in which to save the key (~/.ssh/id_rsa): id_rsa_new</code>)要输入与默认配置不一样的文件名，比如：我这里填的是 <code>id_rsa</code>和<code>id_rsa_me</code>。</p><p>如果之前没配置过ssh key，这里用不同邮箱生成两遍即可，注意用不同的文件名</p><p>成功后会出现：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Your identification has been saved in xxx.</span><br><span class="line">Your public key has been saved in xxx.</span><br></pre></td></tr></table></figure><h3 id="2-2-添加到ssh-agent中"><a href="#2-2-添加到ssh-agent中" class="headerlink" title="2.2 添加到ssh-agent中"></a>2.2 添加到ssh-agent中</h3><p>使用<code>ssh-add</code>将 IdentityFile 添加到 ssh-agent中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-add ~/.ssh/id_rsa</span><br><span class="line">ssh-add ~/.ssh/id_rsa_me</span><br></pre></td></tr></table></figure><h3 id="2-3-配置-ssh-config-文件"><a href="#2-3-配置-ssh-config-文件" class="headerlink" title="2.3 配置 ~/.ssh/config 文件"></a>2.3 配置 <code>~/.ssh/config</code> 文件</h3><p>在<code>~/.ssh/</code>下新建<code>config</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The git info <span class="keyword">for</span> company</span></span><br><span class="line">Host git.XXX.com# git别名，写公司的git名字即可</span><br><span class="line">HostName git.XXX.com# git名字，同样写公司的git名字</span><br><span class="line">User git# 写 git 即可</span><br><span class="line">IdentityFile ~/.ssh/id_rsa        #私钥路径，若写错会连接失败</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The git info <span class="keyword">for</span> github</span></span><br><span class="line">Host github.com# git别名，写github的git名字即可</span><br><span class="line">HostName github.com        # git名字，同样写github的git名字</span><br><span class="line">User git# 写 git 即可</span><br><span class="line">IdentityFile ~/.ssh/id_rsa_me#私钥路径，若写错会连接失败</span><br></pre></td></tr></table></figure><h2 id="3-与GitHub链接"><a href="#3-与GitHub链接" class="headerlink" title="3 与GitHub链接"></a>3 与GitHub链接</h2><p>复制刚刚生成的两个ssh公钥到对应的账号中</p><p>文件<code>id_rsa.pub</code>中保存的就是 ssh 公钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pbcopy &lt; ~/.ssh/id_rsa.pub</span><br><span class="line">pbcopy &lt; ~/.ssh/id_rsa_me.pub</span><br></pre></td></tr></table></figure><p>在 github 网站中添加该 ssh 公钥</p><p>验证是否配置成功，以 github 为例，输入 <code>ssh -T git@github.com</code>，若出现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi xxx! You&apos;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure><p>这样的字段，即说明配置成功。另一个同理。</p><blockquote><p>参考链接：</p><p>配置多个git账号的ssh密钥 - 掘金<br><a href="https://juejin.im/post/5befe84d51882557795cc8f9" target="_blank" rel="noopener">https://juejin.im/post/5befe84d51882557795cc8f9</a></p><p>同一台电脑配置多个git账号 · Issue #2 · jawil/notes<br><a href="https://github.com/jawil/notes/issues/2" target="_blank" rel="noopener">https://github.com/jawil/notes/issues/2</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板项目</title>
      <link href="/2000/01/01/%E6%A8%A1%E6%9D%BF%E9%A1%B9%E7%9B%AE/"/>
      <url>/2000/01/01/%E6%A8%A1%E6%9D%BF%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>模板～</p><a id="more"></a><p>正文</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模板 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
