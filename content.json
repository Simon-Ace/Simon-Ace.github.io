{"meta":{"title":"Rookie的博客","subtitle":"Stay hungry. Stay foolish.","description":null,"author":"Simon","url":"https://simon-ace.github.io","root":"/"},"pages":[],"posts":[{"title":"raw.githubusercontent.com无法连接","slug":"raw.githubusercontent.com无法连接","date":"2021-01-23T16:00:00.000Z","updated":"2021-01-23T17:00:15.577Z","comments":true,"path":"2021/01/24/raw.githubusercontent.com无法连接/","link":"","permalink":"https://simon-ace.github.io/2021/01/24/raw.githubusercontent.com无法连接/","excerpt":"由于 DNS 污染导致 raw.githubusercontent.com 无法正常访问，可通过在 hosts 中添加下面一行解决： 1199.232.96.133 raw.githubusercontent.com","text":"由于 DNS 污染导致 raw.githubusercontent.com 无法正常访问，可通过在 hosts 中添加下面一行解决： 1199.232.96.133 raw.githubusercontent.com 一、解决方法查询真实IP 通过IPAddress.com首页，输入raw.githubusercontent.com查询到真实IP地址 如查询到的ip为：199.232.96.133 修改hosts 1sudo vi /etc/hosts 添加以下内容保存 1199.232.96.133 raw.githubusercontent.com 二、其他代理可参考 https://ghproxy.com 更详细使用方法 主要用于 clone github 的项目","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"电商数仓 V2.0 （03 电商数据仓库系统）","slug":"电商数仓 V2.0/电商数仓 V2.0 （03 电商数据仓库系统）","date":"2021-01-23T16:00:00.000Z","updated":"2021-01-24T03:54:14.354Z","comments":true,"path":"2021/01/24/电商数仓 V2.0/电商数仓 V2.0 （03 电商数据仓库系统）/","link":"","permalink":"https://simon-ace.github.io/2021/01/24/电商数仓 V2.0/电商数仓 V2.0 （03 电商数据仓库系统）/","excerpt":"","text":"电商数仓 V2.0 （03 电商数据仓库系统）一、数仓分层1.1 为什么要分层 把复杂问题简单化 将复杂的任务分解成多层来完成，每一层只处理简单的任务，方便定位问题 减少重复开发 规范数据分层，通过的中间层数据，能够减少极大的重复计算，增加一次计算结果的复用性 隔离原始数据 不论是数据的异常还是数据的敏感性，使真实数据与统计数据解耦开 常见分层方式： 1.2 数据集市与数据仓库数据仓库：企业级别的，包含企业内所有的数据 数据集市：可以理解为部门级别的，归属于数据仓库 二、数仓理论简介2.1 范式理论定义 范式可以理解为设计一张数据表的表结构，符合的标准级别，规范和要求 优点 减少数据冗余 保持数据一致性（只改其中一张表就可以了） 缺点 难以应对大数据量的计算，因为会频繁涉及表之间的 join 操作 2.2 关系建模和维度建模数据处理主要分类： 联机事务处理 OLTP（on-line transaction processing） 基于事务型的数据增删改查，如银行交易 联机分析处理 OLAP（On-Line Analytical Processing） 基于分析型的数据处理，如计算pv uv 对比属性 OLTP（如 MySQL） OLAP（如 Hive） 读特性 每次查询只返回少量记录 对大量记录进行汇总 写特性 随机、低延时写入用户的输入 批量导入 使用场景 用户，Java EE项目 内部分析师，为决策提供支持 数据表征 最新数据状态 随时间变化的历史状态 数据规模 GB TB到PB 2.2.1 关系建模 基本严格遵循三范式 表较为零碎 数据冗余低 2.2.2 维度建模 主要应用于OLAP系统中 通常以某一个事实表为中心进行表的组织，主要面向业务 存在数据的冗余，但是能方便的得到数据。 在大规模数据，跨表分析统计查询过程中，不需要多表关联，提升效率 把相关各种表整理成两种：事实表和维度表 分类 星型模型 在一个事实表周围只会围绕一圈维度表 冗余可能会多一点，但是减少了 join 能提升计算效率 雪花模型 相对于星型模型，事实表周围会有多层维度表 会更灵活一些 星座模型 在前两种上的扩展，如果多于一个事实表，就是星座模型 2.3 维度表和事实表2.3.1 维度表定义： 一般是对事实的描述信息。每一张维表对应现实世界中的一个对象或者概念。 例如：用户、商品、日期、地区等 特征： 维表的范围很宽（具有多个属性、列比较多） 跟事实表相比，行数相对较小：通常&lt; 10万条 内容相对固定：编码表 2.3.2 事实表定义： 描述一个业务过程，如下单、支付、退款、评价等。通常包含着可统计的列，如金额、数量等。 特征： 非常的大 内容相对的窄：列数较少 经常发生变化，每天会新增加很多 事实表分类： 1）事务型事实表 以每个事务或事件为单位，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。一旦事务被提交，事实表数据被插入，数据就不再进行更改，其更新方式为增量更新。 2）周期型快照事实表 周期型快照事实表中不会保留所有数据，只保留固定时间间隔的数据，例如每天或者每月的销售额。对中间过程不敏感，只关心到一个时间点时的状态。 3）累积型快照事实表 累计快照事实表用于跟踪业务事实的变化。例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"https://simon-ace.github.io/tags/数仓/"}]},{"title":"CentOS6 yum 404","slug":"Linux/CentOS6 yum 404","date":"2021-01-20T16:00:00.000Z","updated":"2021-01-21T12:44:40.258Z","comments":true,"path":"2021/01/21/Linux/CentOS6 yum 404/","link":"","permalink":"https://simon-ace.github.io/2021/01/21/Linux/CentOS6 yum 404/","excerpt":"","text":"一、问题出现原因突然发现 yum不可用了，错误信息如下： Determining fastest mirrorsYumRepo Error: All mirror URLs are not using ftp, http[s] or file.Eg. Invalid release/repo/arch combination/removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt错误：Cannot find a valid baseurl for repo: base 前面怀疑是服务器的网络问题，经排查网络无异常。拿yum源中的地址确认问题，发现404了，地址已经发发生了改变，原因是CentOS 6已经随着2020年11月的结束进入了EOL（Reaches End of Life），官方便在12月2日正式将CentOS 6相关的软件源移出了官方源，随之而来逐级镜像也会陆续将其删除。 二、解决问题备份文件 12# by rootcp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 替换CentOS-Base.repo文件内容 下面提供了官方Vault源和阿里云Vault镜像，选择其一即可，国内建议使用阿里云Vault镜像，速度会更快。vi /etc/yum.repos.d/CentOS-Base.repo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#阿里云Vault镜像，本例使用的6.10版本，注意修改为你当前的操作系统版本号[base]name=CentOS-6.10 - Base - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/os/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #released updates [updates]name=CentOS-6.10 - Updates - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/updates/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #additional packages that may be useful[extras]name=CentOS-6.10 - Extras - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/extras/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #additional packages that extend functionality of existing packages[centosplus]name=CentOS-6.10 - Plus - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #contrib - packages by Centos Users[contrib]name=CentOS-6.10 - Contrib - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/contrib/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6--官方Vault源，本例使用的6.10版本，注意修改为你当前的操作系统版本号[base]name=CentOS-6.10 - Base - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/os/$basearch/gpgcheck=1gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #released updates [updates]name=CentOS-6.10 - Updates - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/updates/$basearch/gpgcheck=1gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #additional packages that may be useful[extras]name=CentOS-6.10 - Extras - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/extras/$basearch/gpgcheck=1gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #additional packages that extend functionality of existing packages[centosplus]name=CentOS-6.10 - Plus - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #contrib - packages by Centos Users[contrib]name=CentOS-6.10 - Contrib - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/contrib/$basearch/gpgcheck=1enabled=0gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 清除YUM缓存 1yum clean all 重新构建缓存 1yum makecache","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"yum","slug":"yum","permalink":"https://simon-ace.github.io/tags/yum/"}]},{"title":"电商数仓 V2.0 （02 业务数据采集模块）","slug":"电商数仓 V2.0/电商数仓 V2.0 （02 业务数据采集模块）","date":"2021-01-18T16:00:00.000Z","updated":"2021-01-23T16:41:20.030Z","comments":true,"path":"2021/01/19/电商数仓 V2.0/电商数仓 V2.0 （02 业务数据采集模块）/","link":"","permalink":"https://simon-ace.github.io/2021/01/19/电商数仓 V2.0/电商数仓 V2.0 （02 业务数据采集模块）/","excerpt":"","text":"电商数仓 V2.0 （02 业务数据采集模块）一、电商业务简介二、业务数据采集模块2.1 MySQL装一台机器就行 hadoop102 2.2 Sqoop 修改配置sqoop-env.sh 12345678# 目前用于将数据从 MySQL 导入 HDFS 配置下面这俩就可以export HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2# 导入 Hive 要配置 Hive_HOME；导入 HBase 要配置 HBASE_HOME 和 ZOOCFGDIR#export HBASE_HOME=#export HIVE_HOME=#export ZOOCFGDIR= 拷贝 mysql jar 包 1cp mysql-connector-java-5.1.27-bin.jar $SQOOP_HOME/lib 验证安装情况 123456789101112131415161718$ bin/sqoop helpAvailable commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version informationSee 'sqoop help COMMAND' for information on a specific command. 测试连接 mysql 12bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 123456# 能出现表名就是正确的了 2.3 业务数据生成 通过建表语句导入数据 1gmall2020-03-16.sql java 生成业务数据 拷贝 application.properties 和 gmall-mock-db-2020-03-16-SNAPSHOT.jar 修改 application.properties 12345# 先生成 2020-03-10 clear=1（清空之前sql语句生成的数据），再生成 03-11 clear=0# 业务日期mock.date=2020-03-11# 是否重置mock.clear=1 生成数据 1java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar 2.4 同步策略数据同步策略的类型包括：全量表、增量表、新增及变化表 全量表：存储完整的数据。 增量表：存储新增加的数据。 新增及变化表：存储新增加的数据和变化的数据。 特殊表：只需要存储一次。 2.4.1 全量同步策略每日存入一份完整数据，作为一个分区 适用场景：表数据量不大（如小于10万），每天既有新增，也有修改的场景 例子：品牌表、优惠规则、活动、商品分类等 2.4.2 增量同步策略每天存储一份增量数据作为一个分区 适用场景：表数据量大（十万、百万以上），且每天只有新增数据，不会修改原来的数据 例子：退单表、订单状态表、商品评论表等 2.4.3 新增及变化策略每日新增及变化，就是存储创建时间和操作时间都是今天的数据 适用场景：表的数据量大，既会有新增，又会有变化 例子：用户表、订单表、优惠卷领用表 2.4.4 特殊策略某些特殊的维度表，可不必遵循上述同步策略，如一共只存储一次。 1）客观世界维度 没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。 2）日期维度 日期维度可以一次性导入一年或若干年的数据。 3）地区维度 省份表、地区表","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"https://simon-ace.github.io/tags/数仓/"}]},{"title":"电商数仓 V2.0 （01 用户行为采集平台）","slug":"电商数仓 V2.0/电商数仓 V2.0 （01 用户行为采集平台）","date":"2021-01-14T16:00:00.000Z","updated":"2021-01-23T16:40:48.670Z","comments":true,"path":"2021/01/15/电商数仓 V2.0/电商数仓 V2.0 （01 用户行为采集平台）/","link":"","permalink":"https://simon-ace.github.io/2021/01/15/电商数仓 V2.0/电商数仓 V2.0 （01 用户行为采集平台）/","excerpt":"","text":"电商数仓 V2.0 （01 用户行为采集平台） 一、数据仓库概念二、项目需求及架构设计三、数据生成模块执行 123java -classpath logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar com.shuofxz.appclient.AppMain &gt; /dev/null 2&gt;&amp;1# 或者（需要打包后的主类是 AppMain）java -jar logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1 更改系统时间，用于生成不同日期的日志 （这个应该放到 java 代码中改吧） 12# 更改系统时间sudo date -s 2021-01-11 四、数据采集模块Hadoop 配置支持 lzo 压缩（先别搞，需要 lzo 编译的 hadoop 才能用） 需要先安装 lzo 库 1yum install lzo-devel lzop 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/ 修改 etc/hadaoop/core-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt; 将 jar 包 和 core-site.xml 同步到其他机器上 压力测试 Zookeeperflume 采集模块 配置 conf/flume-env.sh 1234# 添加export JAVA_HOME=/opt/module/jdk1.8.0_144# HADOOP_HOME 必须写，否则启动时会报错，找不到 libexport HADOOP_HOME=/opt/module/hadoop-2.7.2 flume 中添加一个配置文件，用于指定 interceptor, channel 等 conf/file-flume-kafka.conf 123456789101112131415161718192021222324252627282930313233a1.sources=r1a1.channels=c1 c2# configure sourcea1.sources.r1.type = TAILDIRa1.sources.r1.positionFile = /opt/module/flume-1.7.0-bin/test/log_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /tmp/logs/app.+a1.sources.r1.fileHeader = truea1.sources.r1.channels = c1 c2#interceptora1.sources.r1.interceptors = i1 i2a1.sources.r1.interceptors.i1.type = com.shuofxz.flume.interceptor.LogETLInterceptor$Buildera1.sources.r1.interceptors.i2.type = com.shuofxz.flume.interceptor.LogTypeInterceptor$Buildera1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = topica1.sources.r1.selector.mapping.topic_start = c1a1.sources.r1.selector.mapping.topic_event = c2# configure channela1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannela1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.channels.c1.kafka.topic = topic_starta1.channels.c1.parseAsFlumeEvent = falsea1.channels.c1.kafka.consumer.group.id = flume-consumera1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannela1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.channels.c2.kafka.topic = topic_eventa1.channels.c2.parseAsFlumeEvent = falsea1.channels.c2.kafka.consumer.group.id = flume-consumer 写一个 java，编写 ETL 和 类型拦截器 修改 conf/log4j.properties 123# 写相对路径貌似有问题，这个路径的logs 文件夹貌似也要提前创建# 疑问：这个为啥要改呢？？？flume.log.dir=/opt/module/flume-1.7.0-bin/logs flume 群起群停脚本 flume-f1.sh 12345678910111213141516171819#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop102 hadoop103 do echo \" --------启动 $i 采集flume-------\" ssh $i \"nohup /opt/module/flume-1.7.0-bin/bin/flume-ng agent --conf /opt/module/flume-1.7.0-bin/conf --conf-file /opt/module/flume-1.7.0-bin/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/flume.log 2&gt;&amp;1 &amp;\" done&#125;;;\"stop\")&#123; for i in hadoop102 hadoop103 do echo \" --------停止 $i 采集flume-------\" ssh $i \"ps -ef | grep file-flume-kafka | grep -v grep |awk '&#123;print \\$2&#125;' | xargs kill\" done&#125;;;esac 注意： 12# 这个参数要加，否则 flume 运行不正常-Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/flume.log 2&gt;&amp;1 注意： flume-env.sh 中需要配置 flume_opts 到 hadoop common 目录，否则启动时报错 Kafka 日志收集 flume 启动后就会往 kafka 里写数据，topic 也会自己创建 测试：执行之前创建的生成日志脚本 gen_logs.sh，如果可以在kafka中接收到新的数据，证明从日志生成 -&gt; flume -&gt; kafka 链路打通了 压力测试 12345# Producerbin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092# Consumerbin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1 查看列表 1bin/kafka-topics.sh --zookeeper hadoop102:2181 --list 消费消息 12345bin/kafka-console-consumer.sh \\--bootstrap-server hadoop102:9092 --from-beginning --topic topic_startbin/kafka-console-consumer.sh \\--bootstrap-server hadoop102:9092 --topic topic_start Kafka 数据存储到 HDFS 配置文件 conf/kafka-flume-hdfs.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768## 组件a1.sources=r1 r2a1.channels=c1 c2a1.sinks=k1 k2## source1a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r1.batchSize = 5000a1.sources.r1.batchDurationMillis = 2000a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.sources.r1.kafka.topics=topic_start# 重新读取数据需要添加，用于重新消费 kafka 的数据#a1.sources.r1.groupId=f2_1#a1.sources.r1.kafka.consumer.auto.offset.reset = earliest## source2a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r2.batchSize = 5000a1.sources.r2.batchDurationMillis = 2000a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.sources.r2.kafka.topics=topic_event#a1.sources.r2.groupId=f2_1#a1.sources.r2.kafka.consumer.auto.offset.reset = earliest## channel1a1.channels.c1.type = filea1.channels.c1.checkpointDir = /opt/module/flume-1.7.0-bin/checkpoint/behavior1a1.channels.c1.dataDirs = /opt/module/flume-1.7.0-bin/data/behavior1/a1.channels.c1.keep-alive = 6## channel2a1.channels.c2.type = filea1.channels.c2.checkpointDir = /opt/module/flume-1.7.0-bin/checkpoint/behavior2a1.channels.c2.dataDirs = /opt/module/flume-1.7.0-bin/data/behavior2/a1.channels.c2.keep-alive = 6## sink1a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%da1.sinks.k1.hdfs.filePrefix = logstart-##sink2a1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%da1.sinks.k2.hdfs.filePrefix = logevent-## 不要产生大量小文件,生产环境rollInterval配置为3600a1.sinks.k1.hdfs.rollInterval = 10a1.sinks.k1.hdfs.rollSize = 134217728a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k2.hdfs.rollInterval = 10a1.sinks.k2.hdfs.rollSize = 134217728a1.sinks.k2.hdfs.rollCount = 0## 控制输出文件是原生文件(hdfs未配置lzo压缩，这里全都注释掉)#a1.sinks.k1.hdfs.fileType = CompressedStream#a1.sinks.k2.hdfs.fileType = CompressedStream#a1.sinks.k1.hdfs.codeC = lzop#a1.sinks.k2.hdfs.codeC = lzop## 拼装a1.sources.r1.channels = c1a1.sinks.k1.channel= c1a1.sources.r2.channels = c2a1.sinks.k2.channel= c2 脚本 flume-f2.sh 12345678910111213141516171819#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop104 do echo \" --------启动 $i 消费flume-------\" ssh $i \"nohup /opt/module/flume-1.7.0-bin/bin/flume-ng agent --conf-file /opt/module/flume-1.7.0-bin/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/logs/f2_log.txt 2&gt;&amp;1 &amp;\" done&#125;;;\"stop\")&#123; for i in hadoop104 do echo \" --------停止 $i 消费flume-------\" ssh $i \"ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '&#123;print \\$2&#125;' | xargs kill\" done&#125;;;esac 集群启动/停止脚本cluster.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#! /bin/bashcase $1 in\"start\")&#123; echo \" -------- 启动 集群 -------\" echo \" -------- 启动 hadoop集群 -------\" /opt/module/hadoop-2.7.2/sbin/start-dfs.sh ssh hadoop103 \"/opt/module/hadoop-2.7.2/sbin/start-yarn.sh\" #启动 Zookeeper集群 /opt/module/zookeeper-3.4.10/bin/zk-all.sh start for (( i=0; i&lt;6; i++ ));do echo \"wait... $i\" sleep 1s; done #启动 Flume采集集群 /opt/module/flume-1.7.0-bin/bin/flume-f1.sh start #启动 Kafka采集集群 /opt/module/kafka_2.12-2.3.1/bin/kafka-all.sh start sleep 6s; #启动 Flume消费集群 /opt/module/flume-1.7.0-bin/bin/flume-f2.sh start &#125;;;\"stop\")&#123; echo \" -------- 停止 集群 -------\" #停止 Flume消费集群 /opt/module/flume-1.7.0-bin/bin/flume-f2.sh stop #停止 Kafka采集集群 /opt/module/kafka_2.12-2.3.1/bin/kafka-all.sh stop sleep 10s; #停止 Flume采集集群 /opt/module/flume-1.7.0-bin/bin/flume-f1.sh stop #停止 Zookeeper集群 /opt/module/zookeeper-3.4.10/bin/zk-all.sh stop echo \" -------- 停止 hadoop集群 -------\" ssh hadoop103 \"/opt/module/hadoop-2.7.2/sbin/stop-yarn.sh\" /opt/module/hadoop-2.7.2/sbin/stop-dfs.sh&#125;;;esac 停止集群 1cluster.sh stop 改系统时间 change_date.sh 1change_date.sh 2020-03-10 1234567#!/bin/bashfor i in hadoop102 hadoop103 hadoop104do echo \"========== Change Date $i ==========\" ssh -t $i \"sudo date -s $1\"done 启动集群 1cluster.sh start 生成日志 1gen_logs.sh 123456#!/bin/bashfor i in hadoop102 hadoop103;do echo \"====== generte $i log =======\" ssh $i \"java -jar /opt/project/data_warehouse_v2/logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar $1 $2 &gt; /dev/null 2&gt;&amp;1 &amp;\"done 查看 hdfs 中的结果 启动后，日志经历 file -&gt; flume -&gt; kafka -&gt; flume -&gt; hdfs，最终会输出到配置的hdfs://origin_data/gmall/log/topic_event/%Y-%m-%d中 注意 如果要删掉原来的重新采集的话，1）kafka 注意删除的时候把 topic 删除完全，去 Zookeeper 中看是否还有该 topic 记录；2）flume 要重新消费","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"https://simon-ace.github.io/tags/数仓/"}]},{"title":"Python 解析xml配置文件","slug":"Python 解析xml配置文件","date":"2020-12-20T16:00:00.000Z","updated":"2020-12-21T01:52:41.880Z","comments":true,"path":"2020/12/21/Python 解析xml配置文件/","link":"","permalink":"https://simon-ace.github.io/2020/12/21/Python 解析xml配置文件/","excerpt":"","text":"https://www.cnblogs.com/hupeng1234/p/7262371.html https://www.cnblogs.com/yyds/p/6627208.html https://blog.csdn.net/liangpingguo/article/details/105164967 https://blog.csdn.net/youZhengChuan/article/details/52996524 https://www.pythonf.cn/read/79523","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"},{"name":"Python","slug":"Python","permalink":"https://simon-ace.github.io/tags/Python/"}]},{"title":"JVM 资料","slug":"JVM/JVM 资料","date":"2020-12-15T16:00:00.000Z","updated":"2020-12-16T07:20:43.395Z","comments":true,"path":"2020/12/16/JVM/JVM 资料/","link":"","permalink":"https://simon-ace.github.io/2020/12/16/JVM/JVM 资料/","excerpt":"","text":"1 JVM 资料整理Java堆分析器 - Eclipse Memory Analyzer Tool(MAT) https://www.jianshu.com/p/de989b94ca3a 内存分析工具MAT的使用入门https://cloud.tencent.com/developer/article/1676945","categories":[{"name":"JVM","slug":"JVM","permalink":"https://simon-ace.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://simon-ace.github.io/tags/JVM/"}]},{"title":"Linux文本编辑命令 sed","slug":"Linux/常用命令/Linux文本编辑命令 sed","date":"2020-12-15T16:00:00.000Z","updated":"2020-12-18T08:41:27.363Z","comments":true,"path":"2020/12/16/Linux/常用命令/Linux文本编辑命令 sed/","link":"","permalink":"https://simon-ace.github.io/2020/12/16/Linux/常用命令/Linux文本编辑命令 sed/","excerpt":"","text":"一、基本介绍二、常用操作2.1 替换用s命令替换 1sed \"s/替换前字符/替换后字符/\" xxx.txt # 仅替换每行第一个匹配的字符 1234567➜ cat NewFile.txthello tom1234hhh ➜ sed \"s/h/p/\" NewFile.txtpello tom1234phh 可以在后面添加g，作用于行内所有匹配的字符 1234➜ sed 's/h/p/g' NewFile.txtpello tom1234ppp 输出到文件 12345# 输出到新文件sed 's/h/p/g' NewFile.txt &gt; nn.txt# 原地替换（-i）sed -i 's/h/p/g' NewFile.txt 2.2 其他d命令：删除匹配行 1sed &apos;2,$d&apos; my.txt p命令：打印命令，类似grep功能 1sed -n &apos;1,/fish/p&apos; my.txt 三、正则匹配3.1 单个匹配在每一行最前面(^)加点东西： 1sed &quot;s/^/#/g&quot; pets.txt 在每一行最后面($)加点东西： 1sed &quot;s/$/ --- /g&quot; pets.txt 正则表达式的一些最基本的东西： ^ 表示一行的开头。如：/^#/ 以#开头的匹配。 $ 表示一行的结尾。如：/}$/ 以}结尾的匹配。 \\&lt; 表示词首。 如 \\&lt;abc 表示以 abc 为首的詞。 \\&gt; 表示词尾。 如 abc\\&gt; 表示以 abc 結尾的詞。 . 表示任何单个字符。 *表示某个字符出现了0次或多次。 [] 空格或字符集合。 如：[abc]表示匹配a或b或c，还有[a-zA-Z]表示匹配所有的26个字符。如果其中有^表示反，如[^a]表示非a的字符。 正规则表达式是一些很牛的事，比如我们要去掉某html中的tags： 如果你这样搞的话，就会有问题: 1sed &quot;s/&lt;.*&gt;//g&quot; html.txt 要解决上面的那个问题，就得像下面这样，其中的”[^&gt;]”指定了除了&gt;的字符重复0次或多次。 1sed &quot;s/&lt;[^&gt;]*&gt;//g&quot; html.txt 我们再来看看指定需要替换第3行的内容： 1sed &quot;3s/my/your/g&quot; pets.txt 下面的命令只替换第3到第6行的文本： 1sed &quot;3,6s/my/your/g&quot; pets.txt 只替换每一行的第一个s： 1sed &quot;s/s/S/1&quot; my.txt 只替换每一行的第二个s： 1sed &quot;s/s/S/2&quot; my.txt 只替换第一行的第3个以后的s： 1sed &quot;s/s/S/3g&quot; my.txt 3.2 多个匹配如果我们需要一次替换多个模式，可参看下面的示例：（第一个模式把第一行到第三行的my替换成your，第二个则把第3行以后的This替换成了That）。 1sed &quot;1,3s/my/your/g; 3,$s/This/That/g&quot; my.txt 上面的命令等价于：（注：下面使用的是sed的-e命令行参数） 1sed -e &quot;1,3s/my/your/g&quot; -e &quot;3,$s/This/That/g&quot; my.txt 我们可以使用&amp;来当做被匹配的变量，然后可以在基本左右加点东西。如下所示： 1sed &quot;s/my/[&amp;]/g&quot; my.txt 3.3 圆括号匹配使用圆括号匹配的示例：（圆括号括起来的正则表达式所匹配的字符串会可以当成变量来使用，sed中使用的是\\1,\\2…） 1sed &quot;s/This is my \\([^,]*\\),.*is \\(.*\\)/\\1:\\2/g&quot; my.txt 上面这个例子中的正则表达式有点复杂，解开如下（去掉转义字符）： 正则为：This is my ([^,]),.*is (.) 匹配为：This is my (cat),……….is (betty) 然后：\\1就是cat，\\2就是betty 相关链接https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html https://man.linuxde.net/sed https://coolshell.cn/articles/9104.html https://zhuanlan.zhihu.com/p/145661854 https://www.cnblogs.com/along21/p/10366886.html","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, sed","slug":"Linux-sed","permalink":"https://simon-ace.github.io/tags/Linux-sed/"}]},{"title":"Linux常用命令","slug":"Linux/Linux常用命令","date":"2020-11-26T16:00:00.000Z","updated":"2020-11-27T07:19:32.667Z","comments":true,"path":"2020/11/27/Linux/Linux常用命令/","link":"","permalink":"https://simon-ace.github.io/2020/11/27/Linux/Linux常用命令/","excerpt":"","text":"一、基础指令 xargs [Linux输出转换命令 xargs](./常用命令/Linux输出转换命令 xargs.md) xargs命令的作用，是将标准输入转为命令行参数。 原因：大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数 12$ echo \"hello world\" | xargs echohello world 二、指令组合输出中间结果 使用 tee 指令，将中间结果进行输出 1find ./ -i \"*.java\" | tee JavaList | grep Spring","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Linux文本文件处理程序 awk","slug":"Linux/常用命令/Linux文本文件处理程序 awk","date":"2020-11-26T16:00:00.000Z","updated":"2020-11-27T09:54:44.816Z","comments":true,"path":"2020/11/27/Linux/常用命令/Linux文本文件处理程序 awk/","link":"","permalink":"https://simon-ace.github.io/2020/11/27/Linux/常用命令/Linux文本文件处理程序 awk/","excerpt":"12","text":"12 一、基础用法awk是处理文本文件的一个应用程序，几乎所有 Linux 系统都自带这个程序。 它依次处理文件的每一行，并读取里面的每一个字段。对于日志、CSV 那样的每行格式相同的文本文件，awk可能是最方便的工具。 awk的基本用法就是下面的形式。 12345# 格式$ awk 动作 文件名# 示例$ awk '&#123;print $0&#125;' demo.txt 上面示例中，demo.txt是awk所要处理的文本文件。前面单引号内部有一个大括号，里面就是每一行的处理动作print $0。其中，print是打印命令，$0代表当前行。因此上面命令是把每一行原样打印出来。 awk会根据空格和制表符，将每一行分成若干字段，依次用$1、$2、$3代表第一个字段、第二个字段、第三个字段等等。 12$ echo 'this is a test' | awk '&#123;print $3&#125;'a 下面，为了便于举例，我们把/etc/passwd文件保存成demo.txt。 12345root:x:0:0:root:/root:/usr/bin/zshdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologinsync:x:4:65534:sync:/bin:/bin/sync 这个文件的字段分隔符是冒号（:），所以要用-F参数指定分隔符为冒号 123456$ awk -F ':' '&#123; print $1 &#125;' demo.txtrootdaemonbinsyssync 二、变量 数字：第几个字段（从1开始） NF：当前行有多少个字段，因此$NF就代表最后一个字段 NR：表示当前处理的是第几行。 FILENAME：当前文件名 FS：字段分隔符，默认是空格和制表符。 RS：行分隔符，用于分割每一行，默认是换行符。 OFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。 ORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。 OFMT：数字输出的格式，默认为％.6g。 变量NF表示当前行有多少个字段，因此$NF就代表最后一个字段；$(NF-1)代表倒数第二个字段。 123&gt; $ echo 'this is a test' | awk '&#123;print $NF, $(NF-1)&#125;' # print中的逗号是以空格分隔的意思&gt; test a&gt; 变量NR表示当前处理的是第几行。 1234567&gt; $ awk -F ':' '&#123;print NR \") \" $1&#125;' demo.txt&gt; 1) root&gt; 2) daemon&gt; 3) bin&gt; 4) sys&gt; 5) sync&gt; 上面代码中，print命令里面，如果原样输出字符，要放在双引号里面 三、函数常用函数如下 toupper()：字符转为大写 tolower()：字符转为小写 length()：返回字符串长度 substr()：返回子字符串 sin()：正弦 cos()：余弦 sqrt()：平方根 rand()：随机数 awk内置函数的完整列表，可以查看手册。 例：toupper 123456$ awk -F ':' '&#123; print toupper($1) &#125;' demo.txtROOTDAEMONBINSYSSYNC 四、条件awk允许指定输出条件，只输出符合条件的行。 输出条件要写在动作的前面。 1$ awk '条件 动作' 文件名 请看下面的例子。print命令前面是一个正则表达式，只输出包含usr的行。 12345$ awk -F ':' '/usr/ &#123;print $1&#125;' demo.txtrootdaemonbinsys 下面的例子只输出奇数行，以及输出第三行以后的行。 12345678910# 输出奇数行$ awk -F ':' 'NR % 2 == 1 &#123;print $1&#125;' demo.txtrootbinsync# 输出第三行以后的行$ awk -F ':' 'NR &gt;3 &#123;print $1&#125;' demo.txtsyssync 下面的例子输出第一个字段等于指定值的行。 123456$ awk -F ':' '$1 == \"root\" &#123;print $1&#125;' demo.txtroot$ awk -F ':' '$1 == \"root\" || $1 == \"bin\" &#123;print $1&#125;' demo.txtrootbin 五、if 语句awk提供了if结构，用于编写复杂的条件。 1234$ awk -F ':' '&#123;if ($1 &gt; \"m\") print $1&#125;' demo.txtrootsyssync 上面代码输出第一个字段的第一个字符大于m的行。 if结构还可以指定else部分。 123456$ awk -F ':' '&#123;if ($1 &gt; \"m\") print $1; else print \"---\"&#125;' demo.txtroot------syssync 六、相关连接 An Awk tutorial by Example, Greg Grothaus 30 Examples for Awk Command in Text Processing, Mokhtar Ebrahim","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Linux输出转换命令 xargs","slug":"Linux/常用命令/Linux输出转换命令 xargs","date":"2020-11-26T16:00:00.000Z","updated":"2020-11-27T07:23:47.334Z","comments":true,"path":"2020/11/27/Linux/常用命令/Linux输出转换命令 xargs/","link":"","permalink":"https://simon-ace.github.io/2020/11/27/Linux/常用命令/Linux输出转换命令 xargs/","excerpt":"1$ echo \"hello world\" | xargs echo","text":"1$ echo \"hello world\" | xargs echo 一、基本用法xargs命令的作用，是将标准输入转为命令行参数。 原因：大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数 如下面 echo 不接受标准输出做参数，可用 xargs 做转换： 12$ echo \"hello world\" | xargs echohello world 二、参数-d 指定分隔符默认情况下，xargs将换行符和空格作为分隔符，把标准输入分解成一个个命令行参数。 1$ echo \"one two three\" | xargs mkdir 上面代码中，mkdir会新建三个子目录，执行mkdir one two three。 -d参数可以更改分隔符 12$ echo -e \"a\\tb\\tc\" | xargs -d \"\\t\" echoa b c 上面的命令指定制表符\\t作为分隔符，所以a\\tb\\tc就转换成了三个命令行参数。echo命令的-e参数表示解释转义字符。 -p -t打印将要执行的命令-p参数打印出要执行的命令，询问用户是否要执行。 12$ echo 'one two three' | xargs -p touchtouch one two three ?... -t参数则是打印出最终要执行的命令，然后直接执行，不需要用户确认。 12$ echo 'one two three' | xargs -t rmrm one two three -I 传递参数起别名如果xargs要将命令行参数传给多个命令，可以使用-I参数。【貌似，会按空格或回车对参数进行分割，然后重复执行命令，而不是当成命令的多个参数】 -I指定每一项命令行参数的替代字符串。 123456789101112$ cat foo.txtonetwothree$ cat foo.txt | xargs -I file sh -c 'echo file; mkdir file'one twothree$ ls one two three 上面代码中，foo.txt是一个三行的文本文件。我们希望对每一项命令行参数，执行两个命令（echo和mkdir），使用-I file表示file是命令行参数的替代字符串。执行命令时，具体的参数会替代掉echo file; mkdir file里面的两个file。 -l -L 指定多少行作为一个命令行参数1234$ echo -e \"a\\nb\\nc\" | xargs -L 1 echoabc -n 指定一行内多项作为一个命令行参数123456$ echo &#123;0..9&#125; | xargs -n 2 echo0 12 34 56 78 9 --max-procs 多线程执行xargs默认只用一个进程执行命令。如果命令要执行多次，必须等上一次执行完，才能执行下一次。 --max-procs参数指定同时用多少个进程并行执行命令。--max-procs 2表示同时最多使用两个进程，--max-procs 0表示不限制进程数。 1$ docker ps -q | xargs -n 1 --max-procs 0 docker kill 上面命令表示，同时关闭尽可能多的 Docker 容器，这样运行速度会快很多","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Shell 编程","slug":"Linux/Shell 编程","date":"2020-11-18T16:00:00.000Z","updated":"2020-11-23T10:34:45.414Z","comments":true,"path":"2020/11/19/Linux/Shell 编程/","link":"","permalink":"https://simon-ace.github.io/2020/11/19/Linux/Shell 编程/","excerpt":"","text":"0 基础第一行 指明脚本应使用的解释器的名字 1#! /bin/bash 编程规范： 大写字母表示常量，小写字母表示变量 1 变量变量 1234567891011121314151617181920# 注意等号两边不能有空格foo=\"yes\"echo $foo# 有空格的字符串需要用引号包围b=\"abc efg\" # 使用其他变量的值c=\"hhh $b\"# 将执行命令的结果赋值d=$(ls -la) d1=`ls -la` # ``等价 $()# 算数扩展，注意是两个括号e=$((5*7)) # 使用&#123;&#125;限定变量名的范围f=aa.txtg=$&#123;f&#125;1 环境变量 123export xx=xxx # 设置环境变量source xxx_file # 让文件中的环境变量立即生效echo $xx # 输出变量的值 位置参数变量 1234$n # $0为命令本身；$1-$9代表第一到第九个参数；$&#123;10&#125;十以上的用大括号括起来$* # 代表命令行中所有参数，并把所有参数看成一个整体$@ # 代表命令行中所有参数，但把每个参数区分对待？$# # 参数个数 预定义变量 123$$ # 当前进程号$! # 后台运行的最后一个进程的进程号$? # 最后一次执行命令的返回状态，0代表成功，其他都是失败 可自定义 2 条件判断运算符 123$((m+n)) # $(()) 中间写运算式$[m+n] # 推荐这种方式expr m + n # 不推荐 条件判断 test 1234567891011# 写法一test expression# 写法二[ expression ]# 写法三[[ expression ]] # 推荐使用这种写法，包含前两种的用法，且还支持模式匹配# 数字判断(( expression )) 字符串判断 1234567[ -n string ] # 如果字符串string的长度大于零，则为真[ -z string ] # 如果字符串string的长度为零，则为真[ string1 = string2 ] # 如果string1和string2相同，则为真[ string1 == string2 ] # 等同于[ string1 = string2 ][ string1 != string2 ] # 如果string1和string2不相同，则为真[ string1 '&gt;' string2 ] # 如果按照字典顺序string1排列在string2之后，则为真[ string1 '&lt;' string2 ] # 如果按照字典顺序string1排列在string2之前，则为真 整数判断 1(( m &gt; n )) # 可以直接用 &gt; &lt; == &gt;= &lt;= !=，空格都要有！ 逻辑判断 1[[ expr1 &amp;&amp; expr2 ]] # &amp;&amp; || ! 3 流控制if 1234567if [[ $x = 5 ]]; then # 等号两边有空格，也可以用 == 代替 echo \"x=5\"elif [[ $x = 10 ]]; then echo \"x=10\"else echo \"no\"fi case 1234567891011121314case $变量名 in\"值 1\"）如果变量的值等于值 1，则执行程序 1;;\"值 2\"）如果变量的值等于值 2，则执行程序 2;;*）如果变量的值都不是以上的值，则执行此程序;;esac while 1234567#! /bin/basha=5while [[ $a&gt;0 ]]; do echo $a a=$(( a-1 ))doneecho finish for 12345678for (( i=0; i&lt;5; i++ )); do echo $idone# =========================for i in A B C D; do echo $idone","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"shell, linux, 教程","slug":"shell-linux-教程","permalink":"https://simon-ace.github.io/tags/shell-linux-教程/"}]},{"title":"Spark","slug":"Hadoop/Spark","date":"2020-11-04T16:00:00.000Z","updated":"2020-12-01T02:26:10.621Z","comments":true,"path":"2020/11/05/Hadoop/Spark/","link":"","permalink":"https://simon-ace.github.io/2020/11/05/Hadoop/Spark/","excerpt":"","text":"Spark一、SparkCoreRDD 创建 从集合中创建 12345val listRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))listRdd.foreach(println)val arrayRDD: RDD[Int] = sc.parallelize(Array(1, 2, 3, 4))arrayRDD.foreach(println) 由外部存储系统的数据集创建 1val lines: RDD[String] = sc.textFile(\"in\") RDD 转换算子Value 类型map123val listRdd: RDD[Int] = sc.makeRDD(1 to 10)val mulRdd: RDD[Int] = listRdd.map(_ * 2)mulRdd.collect().foreach(println) mapPartitions对每一个分区中的数据批处理。相当于只给每个分区的数据，只发送一次计算；而 map 的实现会给每个数据发送一次计算，增加了网络传输消耗；但是 mapPartitions 由于以整个分区为单位，可能会造成 OOM 12345val listRdd: RDD[Int] = sc.makeRDD(1 to 10)val mapParRdd: RDD[Int] = listRdd.mapPartitions(datas =&gt; &#123; datas.map(_ * 2)&#125;)mapParRdd.collect().foreach(println) mapPartitionsWithIndex1234567val listRdd: RDD[Int] = sc.makeRDD(1 to 10,3)val tupleRdd: RDD[(Int, String)] = listRdd.mapPartitionsWithIndex &#123; case (num, datas) =&gt; &#123; datas.map((_, \"partition_num: \" + num)) &#125;&#125;tupleRdd.collect().foreach(println) flatMap扁平化，变成一个一个单独的元素 123val listRdd: RDD[List[Int]] = sc.makeRDD(Array(List(1, 2), List(3, 4)))val flatRdd: RDD[Int] = listRdd.flatMap(datas =&gt; datas)flatRdd.collect().foreach(println) glom将同一个分区的元素，放到一个数组里 12345val listRdd: RDD[Int] = sc.makeRDD(1 to 16, 4)val glomRdd: RDD[Array[Int]] = listRdd.glom()glomRdd.collect().foreach(array =&gt; &#123; println(array.mkString(\",\"))&#125;) groupBy同一个分区的放到一个迭代对象中。结果 tuple 中，第一个元素是 key，后面是 iterator 123456val listRdd: RDD[Int] = sc.makeRDD(1 to 9)val groupRdd: RDD[(Int, Iterable[Int])] = listRdd.groupBy(i =&gt; i % 2)groupRdd.collect().foreach(println)--------------------(0,CompactBuffer(2, 4, 6, 8))(1,CompactBuffer(1, 3, 5, 7, 9)) filter按条件筛选 123val listRdd: RDD[Int] = sc.makeRDD(1 to 9)val filterRdd: RDD[Int] = listRdd.filter(_ % 2 == 0)filterRdd.collect().foreach(println) sample抽样。 参数介绍：withReplacement，是否重复抽样（可重复，泊松抽样；不可重复，伯努利抽样） fraction，打分？（可重复下，需≥0，代表大概可重复的次数；不可重复下，需[0,1]，代表大概抽取比例） 1234val listRdd: RDD[Int] = sc.makeRDD(1 to 10)// val sampleRdd: RDD[Int] = listRdd.sample(false, 0.7, 333)val sampleRdd: RDD[Int] = listRdd.sample(true, 4, 333)sampleRdd.collect().foreach(println) distinct去重 注意：distinct 计算后，原数据分区会被打乱，是因为中间进行了 shuffle 操作。同时也因为 shuffle 导致必须等待所有分区都计算完成后才能进行下一个操作；而没有 shuffle 操作的算子，执行完一个分区的操作后就可以继续进行下一个操作。 1234val listRdd: RDD[Int] = sc.makeRDD(List(1, 2, 1, 1, 3, 4, 6, 4, 3))val disRdd: RDD[Int] = listRdd.distinct()val disRdd: RDD[Int] = listRdd.distinct(2) // 设置去重后的分区数disRdd.collect().foreach(println) coalease缩减分区。实际为合并分区，即将其中某几个分区合并；若要扩大分区，需要添加 shuffle 参数 1234val listRdd: RDD[Int] = sc.makeRDD(1 to 16, 4)println(\"before: \", listRdd.partitions.size)val coalRdd: RDD[Int] = listRdd.coalesce(3)println(\"after: \", coalRdd.partitions.size) repartition对 coalease 的封装，shuffle = true 1rdd.repartition(2) sortBy排序，可自己设置排序规则 1234val listRdd: RDD[Int] = sc.makeRDD(List(3, 5, 1, 7, 2))val sortRdd: RDD[Int] = listRdd.sortBy(x =&gt; x)// val sortRdd: RDD[Int] = listRdd.sortBy(x =&gt; x%3)sortRdd.collect().foreach(println) 双 Value 类型union合并两个 Rdd 1val rdd3 = rdd1.union(rdd2) subtract去除相同元素，不同的会保留 1val rdd3 = rdd1.subtract(rdd2) intersection求交集后返回 1val rdd3 = rdd1.intersection(rdd2) cartesian笛卡尔积 1val rdd3 = rdd1.cartesian(rdd2) zip将两个 rdd 对应元素组合在一起（tuple？key-value？）。两个 rdd 分区数量和元素数量必须都相同；会把分区中的拆成一个一个的元素，组合的元素还在原来的分区里。 12345val rdd1: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 2)val rdd2: RDD[String] = sc.makeRDD(Array(\"a\", \"b\", \"c\", \"d\"), 2)val zipRdd: RDD[(Int, String)] = rdd1.zip(rdd2)zipRdd.collect().foreach(println)zipRdd.saveAsTextFile(\"output\") Key-Value 类型partitionBy根据 key 进行重新分区（因此 rdd 需要是 kv 的形式），也可自定义分区类 123val arrayRdd: RDD[(Int, String)] = sc.makeRDD(Array((1, \"aaa\"), (2, \"bbb\"), (3, \"ccc\"), (4, \"ddd\")), 2)val parRdd: RDD[(Int, String)] = arrayRdd.partitionBy(new org.apache.spark.HashPartitioner(3))parRdd.saveAsTextFile(\"output\") Rdd Action 行动算子综合练习二、SparkSQLSpark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。 Rdd → DataFrame → DataSet DataFrame：在 Rdd 的基础上，装饰了表结构，让每一个字段包含意义 DataSet：在 DataFrame 基础上，装饰了读取操作，让数据的读取像操作对象一样简单 1bin/spark-shell DataFrame创建1234567&gt; val df = spark.read.json(\"/opt/module/spark-2.3.2-local/mydata/user.json\")&gt; df.show==========# user.json&#123;\"name\":\"123\", \"age\":20&#125;&#123;\"name\":\"456\", \"age\":20&#125;&#123;\"name\":\"789\", \"age\":20&#125; SQL 风格语法 单个 Session 内 View 可见 12&gt; df.createTempView(\"user\")&gt; spark.sql(\"select * from user\").show() 创建全局表 12&gt; df.createGlobalTempView(\"user_g\")&gt; spark.newSession().sql(\"SELECT * FROM global_temp.user_g\").show() DSL 风格语法以对象的方式来操作数据 1234&gt; df.select(\"name\").show()&gt; df.select($\"name\", $\"age\" + 1).show()&gt; df.filter($\"age\" &gt; 21).show()&gt; df.groupBy(\"age\").count().show() Rdd 转为 DataFrame12345&gt; case class People(name:String, age:Int)&gt; val rdd1 = sc.makeRDD(List((\"zhangsan\", 20), (\"lisi\", 14)))&gt; val peopleRdd = rdd1.map(t=&gt;&#123;People(t._1, t._2)&#125;)&gt; val df = peopleRdd.toDF&gt; df.show DataFrame 转为 Rdd注意这里面转换之后，并不会还原成 People 结构，而只是一个 Row 对象。这是因为 DataFrame 本身不存数据的类型 1&gt; df.rdd DataSetDataset是具有强类型的数据集合，需要提供对应的类型信息。 解决 DataFrame 中取数只能通过下标来取的问题（啥意思？？） 创建12&gt; case class People(name:String, age:Int)&gt; val caseClassDS = Seq(People(\"Andy\", 21)).toDS() Rdd 转换为 DataSetRdd + 结构 → DataFrame；DataFram + 类型 → DataSet Rdd + 结构 + 类型 → DataSet 1234&gt; case class Person(name: String, age: Long)&gt; val mapRdd = rdd.map(t=&gt;&#123;Person(t._1, t._2)&#125;)&gt; val ds = mapRdd.toDS&gt; ds.show DataSet 转换为 Rdd转换回来仍保留着类型 1&gt; ds.rdd 三、SparkStreaming","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Spark, 教程","slug":"Spark-教程","permalink":"https://simon-ace.github.io/tags/Spark-教程/"}]},{"title":"Hive 常用操作 & 练习","slug":"Hadoop/Hive 常用操作 & 练习","date":"2020-10-20T16:00:00.000Z","updated":"2020-10-21T02:51:56.681Z","comments":true,"path":"2020/10/21/Hadoop/Hive 常用操作 & 练习/","link":"","permalink":"https://simon-ace.github.io/2020/10/21/Hadoop/Hive 常用操作 & 练习/","excerpt":"","text":"一、常用操作二、练习","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"},{"name":"练习","slug":"练习","permalink":"https://simon-ace.github.io/tags/练习/"}]},{"title":"console 日志输出到文件","slug":"Linux/console 日志输出到文件","date":"2020-10-08T16:00:00.000Z","updated":"2020-11-22T03:56:40.104Z","comments":true,"path":"2020/10/09/Linux/console 日志输出到文件/","link":"","permalink":"https://simon-ace.github.io/2020/10/09/Linux/console 日志输出到文件/","excerpt":"1some_command 2&gt;&amp;1 | tee output.txt","text":"1some_command 2&gt;&amp;1 | tee output.txt 在Linux中，如果想将一个程序在控制台中的输出字符输出到文件中，不保留控制台内的文字，可以用下面命令： 1some_command &gt; output.txt 命令结果会输出到output.txt中，换成&gt;&gt;可以追加到文件末尾 但如果想输出到文件同时，保留控制台的内容，需要使用tee命令，示例如下： 1some_command | tee output.txt 有时会发现上述命令后屏幕有输出，但文件内容为空，此时可能是由于some_command输出的字符从std error文件描述符输出，需要先将std error的输出导向到std output： 1some_command 2&gt;&amp;1 | tee output.txt 其中，2代表std error，1代表std output，&gt;&amp;是linux中fd到fd的重定向操作符。","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"SpringBoot 集成 Prometheus","slug":"SpringBoot集成Prometheus","date":"2020-09-22T16:00:00.000Z","updated":"2020-09-25T12:14:05.812Z","comments":true,"path":"2020/09/23/SpringBoot集成Prometheus/","link":"","permalink":"https://simon-ace.github.io/2020/09/23/SpringBoot集成Prometheus/","excerpt":"","text":"一、添加依赖 Maven pom.xml 12345678910111213&lt;!-- 第一条必须加，否则会导致 Could not autowire. No beans of 'xxxx' type found 的错误 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt; Gradle build.gradle 123implementation &apos;org.springframework.boot:spring-boot-starter-actuator&apos;compile &apos;io.micrometer:micrometer-registry-prometheus&apos;compile &apos;io.micrometer:micrometer-core&apos; 打开 Prometheus 监控接口 application.properties 1234server.port=8088spring.application.name=springboot2-prometheusmanagement.endpoints.web.exposure.include=*management.metrics.tags.application=$&#123;spring.application.name&#125; 可以直接运行程序，访问http://localhost:8088/actuator/prometheus可以看到下面的内容： 12345678910111213141516171819202122# HELP jvm_buffer_total_capacity_bytes An estimate of the total capacity of the buffers in this pool# TYPE jvm_buffer_total_capacity_bytes gaugejvm_buffer_total_capacity_bytes&#123;id=&quot;direct&quot;,&#125; 90112.0jvm_buffer_total_capacity_bytes&#123;id=&quot;mapped&quot;,&#125; 0.0# HELP tomcat_sessions_expired_sessions_total # TYPE tomcat_sessions_expired_sessions_total countertomcat_sessions_expired_sessions_total 0.0# HELP jvm_classes_unloaded_classes_total The total number of classes unloaded since the Java virtual machine has started execution# TYPE jvm_classes_unloaded_classes_total counterjvm_classes_unloaded_classes_total 1.0# HELP jvm_buffer_count_buffers An estimate of the number of buffers in the pool# TYPE jvm_buffer_count_buffers gaugejvm_buffer_count_buffers&#123;id=&quot;direct&quot;,&#125; 11.0jvm_buffer_count_buffers&#123;id=&quot;mapped&quot;,&#125; 0.0# HELP system_cpu_usage The &quot;recent cpu usage&quot; for the whole system# TYPE system_cpu_usage gaugesystem_cpu_usage 0.0939447637893599# HELP jvm_gc_max_data_size_bytes Max size of old generation memory pool# TYPE jvm_gc_max_data_size_bytes gaugejvm_gc_max_data_size_bytes 2.841116672E9# 此处省略超多字... 二、Prometheus 安装与配置使用 docker 运行 Prometheus（仅初始测试） 1docker run --name prometheus -d -p 9090:9090 prom/prometheus:latest 写配置文件prometheus.yml 12345678910111213141516171819202122232425262728293031# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] # demo job - job_name: 'springboot-actuator-prometheus-test' # job name metrics_path: '/actuator/prometheus' # 指标获取路径 scrape_interval: 5s # 间隔 basic_auth: # Spring Security basic auth username: 'actuator' password: 'actuator' static_configs: - targets: ['docker.for.mac.localhost:18080'] # 实例的地址，默认的协议是http （这里开始有问题，直接写 localhost 是访问容器内的地址，而不是宿主机的。可通过在网页上方 status -&gt; targets 查看对应的服务情况 运行 docker 1docker run -d -p 9090:9090 -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml 访问 http://localhost:9090，可看到如下界面 点击 Insert metric at cursor ，即可选择监控指标；点击 Graph ，即可让指标以图表方式展示；点击Execute 按钮，即可看到指标图 三、Grafana 安装和配置1、启动 1$ docker run -d --name=grafana -p 3000:3000 grafana/grafana 2、登录 访问 http://localhost:3000/login ，初始账号/密码为：admin/admin 3、配置数据源 点击左侧齿轮Configuration中Add Data Source，会看到如下界面： 这里我们选择Prometheus 当做数据源，这里我们就配置一下Prometheus 的访问地址，点击 Save &amp; Test 4、创建监控 Dashboard 点击导航栏上的 + 按钮，并点击Dashboard，将会看到类似如下的界面 点击+ Add new panel 四、自定义监控指标1、创建 Prometheus 监控管理类PrometheusCustomMonitor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import io.micrometer.core.instrument.Counter;import io.micrometer.core.instrument.DistributionSummary;import io.micrometer.core.instrument.MeterRegistry;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;import java.util.concurrent.atomic.AtomicInteger;@Componentpublic class PrometheusCustomMonitor &#123; private Counter requestErrorCount; private Counter orderCount; private DistributionSummary amountSum; private AtomicInteger failCaseNum; private final MeterRegistry registry; @Autowired public PrometheusCustomMonitor(MeterRegistry registry) &#123; this.registry = registry; &#125; @PostConstruct private void init() &#123; requestErrorCount = registry.counter(\"requests_error_total\", \"status\", \"error\"); orderCount = registry.counter(\"order_request_count\", \"order\", \"test-svc\"); amountSum = registry.summary(\"order_amount_sum\", \"orderAmount\", \"test-svc\"); failCaseNum = registry.gauge(\"fail_case_num\", new AtomicInteger(0)); &#125; public Counter getRequestErrorCount() &#123; return requestErrorCount; &#125; public Counter getOrderCount() &#123; return orderCount; &#125; public DistributionSummary getAmountSum() &#123; return amountSum; &#125; public AtomicInteger getFailCaseNum() &#123; return failCaseNum; &#125;&#125; 2、新增/order接口 当 flag=&quot;1&quot;时，抛异常，模拟下单失败情况。在接口中统计order_request_count和order_amount_sum 12345678910111213141516171819202122232425262728import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;import java.util.Random;@RestControllerpublic class TestController &#123; @Resource private PrometheusCustomMonitor monitor; @RequestMapping(\"/order\") public String order(@RequestParam(defaultValue = \"0\") String flag) throws Exception &#123; // 统计下单次数 monitor.getOrderCount().increment(); if (\"1\".equals(flag)) &#123; throw new Exception(\"出错啦\"); &#125; Random random = new Random(); int amount = random.nextInt(100); // 统计金额 monitor.getAmountSum().record(amount); monitor.getFailCaseNum().set(amount); return \"下单成功, 金额: \" + amount; &#125;&#125; 3、新增全局异常处理器GlobalExceptionHandler 统计下单失败次数requests_error_total 123456789101112131415161718import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;@ControllerAdvicepublic class GlobalExceptionHandler &#123; @Resource private PrometheusCustomMonitor monitor; @ResponseBody @ExceptionHandler(value = Exception.class) public String handle(Exception e) &#123; monitor.getRequestErrorCount().increment(); return \"error, message: \" + e.getMessage(); &#125;&#125; 4、测试 启动项目，访问http://localhost:8080/order和http://localhost:8080/order?flag=1模拟下单成功和失败的情况，然后我们访问http://localhost:8080/actuator/prometheus，可以看到我们自定义指标已经被 /prometheus 端点暴露出来 12345678910# HELP requests_error_total # TYPE requests_error_total counterrequests_error_total&#123;application=&quot;springboot-actuator-prometheus-test&quot;,status=&quot;error&quot;,&#125; 41.0# HELP order_request_count_total # TYPE order_request_count_total counterorder_request_count_total&#123;application=&quot;springboot-actuator-prometheus-test&quot;,order=&quot;test-svc&quot;,&#125; 94.0# HELP order_amount_sum # TYPE order_amount_sum summaryorder_amount_sum_count&#123;application=&quot;springboot-actuator-prometheus-test&quot;,orderAmount=&quot;test-svc&quot;,&#125; 53.0order_amount_sum_sum&#123;application=&quot;springboot-actuator-prometheus-test&quot;,orderAmount=&quot;test-svc&quot;,&#125; 2701.0 5、使用 Prometheus 监控 重新运行 docker 1docker run -d -p 9090:9090 -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml 选择对应指标后可以看到数据变化 6、使用 Grafana 展示 在 Dashboard 界面选择对应的监控指标即可 参考资料： Metric types | Prometheus IntelliJ IDEA创建第一个Spring Boot项目_Study Notes-CSDN博客 Spring Boot 使用 Micrometer 集成 Prometheus 监控 Java 应用性能 【springboot 2.0】 Micrometer Application Monitoring【官方文档】 Spring Boot 微服务应用集成Prometheus + Grafana 实现监控告警 ★ Monitoring Java Spring Boot applications with Prometheus: Part 1 | by Arush Salil | Kubernauts 【放弃这个教程？】client java 不支持 springboot 2.x，最高支持 1.5 Spring Boot 参考指南（端点）_风继续吹 - SegmentFault 思否","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"SpringBoot, Prometheus","slug":"SpringBoot-Prometheus","permalink":"https://simon-ace.github.io/tags/SpringBoot-Prometheus/"}]},{"title":"虚拟机 Hadoop 环境配置","slug":"Hadoop/虚拟机Hadoop环境配置","date":"2020-09-16T16:00:00.000Z","updated":"2021-01-15T07:57:21.093Z","comments":true,"path":"2020/09/17/Hadoop/虚拟机Hadoop环境配置/","link":"","permalink":"https://simon-ace.github.io/2020/09/17/Hadoop/虚拟机Hadoop环境配置/","excerpt":"","text":"一、安装 CentOS 6 &amp; 基本配置 win10通过VMware安装CentOS6.5 - 简书https://www.jianshu.com/p/9d5b9757a1ef 1、关闭防火墙 12service iptables stopchkconfig iptables off 2、创建普通用户 12useradd Acepasswd 123 3、创建软件存储文件夹，并更改所有权 12mkdir /opt/software /opt/modulechown Ace:Ace /opt/software /opt/module 4、用户添加到 sudoers 12vi /etc/sudoersAce ALL=(ALL) NOPASSWD:ALL 5、改 Hosts 12345#!/bin/bashfor ((i=101;i&lt;105;i++))do echo \"192.168.87.$i hadoop$i\" &gt;&gt; /etc/hostsdone 6、改静态 ip vim /etc/sysconfig/network-scripts/ifcfg-eth0 123456789DEVICE=eth0TYPE=EthernetONBOOT=yesBOOTPROTO=staticIPADDR=192.168.87.100PREFIX=24GATEWAY=192.168.87.2DNS1=192.168.87.2NAME=\"System eth0\" 【创建新虚拟机，下面的都要做一遍，可以写脚本解决（看下面，推荐）】 6、改 ip 地址（同上6） vim /etc/sysconfig/network-scripts/ifcfg-eth0 1IPADDR=192.168.87.100 # 改成对应的 7、改主机名 vim /etc/sysconfig/network 1HOSTNAME=hadoopxxx 8、删除多余网卡 vim /etc/udev/rules.d/70-persistent-net.rules 12# 第一行删掉（只保留一个网卡就行，注释也删掉，手动删的话记得和后面脚本的行数要对应上）# 第二行最后 NAME=\"eth1\" 改为 NAME=\"eth0\" 9、拍快照，克隆 【脚本】 分发脚本 xsync vim xsync 123456789101112131415161718192021#!/bin/bash# 获取输入参数个数，如果没有参数，直接退出pcount=$#if ((pcount==0)); thenecho no args;exit;fip1=$1fname=`basename $p1`echo fname=$fnamedirname=`cd -P $(dirname $p1); pwd`echo dirname=$dirnameuser=`whoami`for((i=102;i&lt;105;i++)); do echo \"------------- hadoop$i ------------\" rsync -avlP $dirname/$fname hadoop$i:$dirnamedone 移动到bin目录下，sudo mv xsync /bin 安装rsync，sudo yum install -y rsync 改权限，chmod +x xsync 执行相同命令脚本 123456789101112#!/bin/bashpcount=$#if ((pcount==0)); thenecho no args;exit;fifor i in hadoop102 hadoop103 hadoop104do echo \"==== $i $1 ====\" ssh $i \"$1\"done 移动到/bin，改权限 自动配置网络脚本 123456789101112131415161718#!/bin/bashid=$1# sed -i \"s/before replace/after replace/\"sudo sed -i \"s/192.168.87.101/192.168.87.$id/\" /etc/sysconfig/network-scripts/ifcfg-eth0sudo sed -i \"s/hadoop101/hadoop$id/\" /etc/sysconfig/networkfile=/etc/udev/rules.d/70-persistent-net.rules# count \"SUBSYSTEM\" word numbernu=$(grep -c SUBSYSTEM $file)if(($nu &gt; 1));then # delete 8th line sed -i '8d' $filefised -i 's/eth1/eth0/' $filereboot 改权限，chmod +x change_network 二、安装 JAVA 和 Hadoop1、下载/上传 java 和 hadoop 的包到 /opt/software 2、解压 java 和 hadoop 到 /opt/module 3、安装（配置环境变量） 12345678910sudo vim /etc/profile# 在末尾添加# JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin# HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 4、测试安装情况 执行下面命令后，能出现版本号即为成功 12$ java -version$ hadoop version 5、使用 xsync 同步到多个机器上 三、Hadoop 配置Apache Hadoop 3.2.1 – Hadoop: Setting up a Single Node Cluster. 安装插件 1234567$ sudo yum install ssh$ sudo yum install pdsh # pdsh 可能默认找不到$ wget http://mirrors.mit.edu/epel/6/i386/epel-release-6-8.noarch.rpm$ rpm -Uvh epel-release-6-8.noarch.rpm$ yum install pdsh 环境配置 vim etc/hadoop/hadoop-env.sh 1export JAVA_HOME=/your-java-home-path 3.1 本地运行模式1234$ mkdir input$ cp etc/hadoop/*.xml input$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'$ cat output/* 3.2 伪分布式1、配置 vim etc/hadoop/core-site.xml 12345678910111213&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 暂时不配置 yarn 了 etc/hadoop/yarn-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 2、设置免密码登录 123$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys$ chmod 0600 ~/.ssh/authorized_keys 3、执行 Format the filesystem: 1$ bin/hdfs namenode -format Start NameNode daemon and DataNode daemon: 1$ sbin/start-dfs.sh The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs). Browse the web interface for the NameNode; by default it is available at: NameNode - http://localhost:9870/ Make the HDFS directories required to execute MapReduce jobs: 12$ bin/hdfs dfs -mkdir /user$ bin/hdfs dfs -mkdir /user/&lt;username&gt; Copy the input files into the distributed filesystem: 12$ bin/hdfs dfs -mkdir input$ bin/hdfs dfs -put etc/hadoop/*.xml input Run some of the examples provided: 1$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them: 12$ bin/hdfs dfs -get output output$ cat output/* or View the output files on the distributed filesystem: 1$ bin/hdfs dfs -cat output/* When you’re done, stop the daemons with: 1$ sbin/stop-dfs.sh 3.3 完全分布式同步两个软件，/etc/profile 3.3.1 集群配置 集群部署规划 NN 1个； 2NN 1个；RM 1个；DN 3个、NM 3个 —— 最少共需六台机器，但是开不起那么多个虚拟机，因此按下表进行合并配置 hadoop102 hadoop103 hadoop104 HDFS NameNode, DataNode DataNode SecondaryNameNode, DataNode YARN NodeManager ResourceManager, NodeManager NodeManager 配置集群 1）核心配置文件 配置etc/hadoop/core-site.xml 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 2）HDFS配置文件 配置``etc/hadoop/hadoop-env.sh` 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置etc/hadoop/hdfs-site.xml 12345678910&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; 3）YARN配置文件 配置etc/hadoop/yarn-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置etc/hadoop/yarn-site.xml 1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt; 4）MapReduce配置文件 配置etc/hadoop/mapred-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置etc/hadoop/mapred-site.xml 12345678910$ cp mapred-site.xml.template mapred-site.xml~~&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;~~ 在集群上分发配置好的Hadoop配置文件 1$ xsync /opt/module/hadoop-2.7.2/ 3.3.2 集群单点启动1）如果集群是第一次启动，需要格式化NameNode 1$ hadoop namenode -format 2）在 hadoop102 上启动 NameNode 1$ hadoop-daemon.sh start namenode 3）在 hadoop102、hadoop103、hadoop104 上分别启动DataNode 1$ hadoop-daemon.sh start datanode 4）在 hadoop104 上启动 secondarynamenode 1$ hadoop-daemon.sh start secondarynamenode 5）查看服务启动情况 jps 12345678910111213[hadoop102]$ jps4182 Jps2842 DataNode2747 NameNode[hadoop103]$ jps1712 DataNode2215 Jps[hadoop104]$ jps1680 DataNode2266 Jps2171 SecondaryNameNode 3.3.3 集群一键启动 配置机器间 ssh 无密登录 「方法1：共用一个秘钥」 123456# 生成秘钥$ ssh-keygen -t rsa# 将秘钥添加到本机的 authorized_keys 中，实现本机无密登录$ ssh-copy-id hadoop102# 共享同一个秘钥，实现集群机器间无密登录$ xsync ~/.ssh 「方法2：每个机器单独生成秘钥」 123456# 在每个机器上生成秘钥（每个机器执行一遍）$ ssh-keygen -t rsa# 把每个机器的秘钥分发到别的机器上（每个机器执行一遍）$ ssh-copy-id hadoop102$ ssh-copy-id hadoop103$ ssh-copy-id hadoop104 添加机器名，配置etc/hadoop/slaves，并同步到其他机器（xsync） 123hadoop102hadoop103hadoop104 启动 12[hadoop102]$ start-dfs.sh[hadoop103]$ start-yarn.sh # 应该在ResouceManager所在的机器上启动YARN 测试（单词计数） 1234567891011121314151617181920212223# 创建一个文件夹 wcinput，里面放一个文件，添加计数文件中的内容，如：qwedddsmo123qwe# 将这个文件夹上传到 hdfs$ hadoop fs -put wcinput/ /# 执行 MapReduce 程序$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /output# 查看执行结果[Ace@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /outputFound 2 items-rw-r--r-- 3 Ace supergroup 0 2020-09-27 10:15 /output/_SUCCESS-rw-r--r-- 3 Ace supergroup 24 2020-09-27 10:15 /output/part-r-00000[Ace@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /output/part-r-00000123 1ddd 1qwe 2smo 1 停止 12[hadoop102]$ stop-dfs.sh[hadoop103]$ stop-yarn.sh 3.3.4 配置历史服务器 &amp; 日志聚集 配置etc/hadoop/mapred-site.xml，添加： 12345678910&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop104:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop104:19888&lt;/value&gt;&lt;/property&gt; 配置etc/hadoop/yarn-site.xml 12345678910&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; [分发配置] 启动 1234[hadoop102]$ start-dfs.sh[hadoop103]$ start-yarn.sh[hadoop104]$ mr-jobhistory-daemon.sh start historyserver$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /output1 查看 打开 yarn web http://hadoop103:8088/ 打开 history，再点 log 就能看到任务具体的日志信息了 ![image-20201008173041985](../../../../../Library/Application Support/typora-user-images/image-20201008173041985.png) 3.3.5 集群时间同步1）检查 ntp 是否安装 查看 ntp 包（切换到 root 用户） 123[root@hadoop102 ~]# rpm -qa | grep ntpntp-4.2.6p5-15.el6.centos.x86_64ntpdate-4.2.6p5-15.el6.centos.x86_64 如果没有这两个服务要安装一下 1yum install -y ntp 先停止 ntp 服务 1234567# 查看 ntpd 服务是否在运行$ service ntpd status# 如果在运行先关闭$ service ntpd stop$ chkconfig ntpd off# 再查看一下 ntpd 运行情况$ chkconfig --list ntpd 2）修改ntp配置文件/etc/ntp.conf 123456789101112# 修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# 修改2（集群在局域网中，不使用其他互联网上的时间），将下面四行注释掉server 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst# 添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）server 127.127.1.0fudge 127.127.1.0 stratum 10 3）修改/etc/sysconfig/ntpd文件 12增加内容如下（让硬件时间与系统时间一起同步）SYNC_HWCLOCK=yes 4）重新启动ntpd服务 12$ service ntpd status$ service ntpd start 5）设置ntpd服务开机启动 1[hadoop102]$ chkconfig ntpd on 6）其他机器配置（必须root用户） 在其他机器配置10分钟与时间服务器同步一次 12$ crontab -e*/10 * * * * /usr/sbin/ntpdate hadoop102 测试（修改任意机器时间），十分钟后查看机器是否与时间服务器同步 1$ date -s \"2017-9-11 11:11:11\" 7）若主机时间不对 123456$ service ntpd stop$ ntpdate us.pool.ntp.org$ service ntpd start# 或者直接$ sudo ntpdate -u pool.ntp.org 3.4 常用端口记录 Hadoop默认端口应用一览_在路上的学习者-CSDN博客 Hadoop常用端口号_baiBenny的博客-CSDN博客 组件 节点 默认端口 配置 用途说明 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口，可查看 HDFS 存储内容 HBase Master 16000 Master RPC Port（远程通信调用） Master 16010 Master Web Port Regionserver 16020 Regionserver RPC Port Regionserver 16030 Regionserver Web Port Spark 4040 查看 Spark Job 3.5 HA 配置两个 NameNode 配置 JournalNode 用于将 Active NN 的数据 同步到 Standby NN 上「解决元数据同步的问题」 配置 Zookeeper，解决主备 NN 切换的问题，防止脑裂 在 NN 上启动 failoverController（zkfc），作为 Zookeeper 的客户端，实现与 zk 集群的交互和监测 四、Zookeeper 配置4.1 本地模式1、安装前准备 安装Jdk 拷贝Zookeeper安装包到Linux系统下 解压到指定目录 1tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2、配置修改 修改conf/zoo_sample.cfg 1cp zoo_sample.cfg zoo.cfg 修改 zoo.cfg 文件 1dataDir=/opt/module/zookeeper-3.4.10/zkData 并创建 zkData 文件夹 3、操作Zookeeper 启动Zookeeper 1$ bin/zkServer.sh start 查看进程是否启动 123$ jps4020 Jps4001 QuorumPeerMain 查看状态： 1234$ bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: standalone 停止Zookeeper 1$ bin/zkServer.sh stop 4.2 集群模式1、配置 修改 zoo.cfg 文件 123456server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888# 2888 为集群间通信端口号# 3888 为选举端口号# server.2/3/4 为id，不相同即可 创建 zkData/myid，每个机器写不同的，要和前面的对应上 1234567$ vim zkData/myid# hadoop1022# hadoop1033# hadoop1044 配置 bin/zkEnv.sh 1234567# 替换前ZOO_LOG_DIR=\".\"# 替换后ZOO_LOG_DIR=\"/opt/module/zookeeper-3.4.10/logs\"# 添加 JAVA_HOME（远程启动的时候才是必要的，因为会丢失环境变量）export JAVA_HOME=/opt/module/jdk1.8.0_144 同步 xsync 2、启动 12# 每个机器都要单独启动./bin/zkServer.sh start 仅启动一台机器时： 1234$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgError contacting service. It is probably not running. 启动两台机器（超过半数）： 1234$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader / follower 3、集群脚本 12345678910111213141516171819202122232425262728#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== start $i zookeeper ====\" ssh $i \"/opt/module/zookeeper-3.4.10/bin/zkServer.sh start\" done&#125;;;\"stop\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== stop $i zookeeper ====\" ssh $i \"/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop\" done&#125;;;\"status\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== status $i zookeeper ====\" ssh $i \"/opt/module/zookeeper-3.4.10/bin/zkServer.sh status\" done&#125;;;esac 4.3 客户端操作 启动 1$ bin/zkCli.sh 执行 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建-s 含有序列-e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 五、Kakfa 配置5.1 环境准备 在 hadoop102 103 104 上均安装 Kafka jar 包下载 https://kafka.apache.org/downloads 命名中有两个版本号，第一个为 scala 版本，第二个是 kafka 版本 5.2 集群配置 修改 config/server.properties 1234567891011# broker的全局唯一编号，不能重复broker.id=0# 删除topic功能使能（kafka 2.x 版本 不需要配置）delete.topic.enable=true# kafka运行时数据存放的路径log.dirs=/opt/module/kafka_2.12-2.3.1/data# 配置连接Zookeeper集群地址zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 分发安装包到所有物理机上 xsync 将 hadoop103 104 上config/server.properties 中的 broker.id=x进行修改 单点启动 / 停止 依次在 hadoop102、hadoop103、hadoop104 节点上启动/停止 kafka，执行下面的命令 12345678# 启动# 前台运行$ bin/kafka-server-start.sh config/server.properties# 后台运行$ bin/kafka-server-start.sh -daemon config/server.properties# 停止$ bin/kafka-server-stop.sh 群起 / 群停 由于 Kafka 中没有给集群启动停止的脚本，需要自己写kk-all.sh 需要注意：要先在 ~/.bashrc 中配置 java 环境变量（xsync） 123# JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin 12345678910111213141516171819202122#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== start $i kafka ====\" # ssh $i \"source /etc/profile\" ssh $i \"/opt/module/kafka_2.12-2.3.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.12-2.3.1/config/server.properties\" done&#125;;;\"stop\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== stop $i kafka ====\" # ssh $i \"source /etc/profile\" ssh $i \"/opt/module/kafka_2.12-2.3.1/bin/kafka-server-stop.sh\" done&#125;;;esac 启动/停止 Kafka：（记得先启动 Zookeeper） 1234567$ ./kk-all.sh start$ jps1637 QuorumPeerMain6071 Kafka6538 Jps$ ./kk-all.sh stop 5.3 命令行操作 查看当前服务器中的所有 topic 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list 创建 topic 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first --replication-factor 定义副本数；--partitions 定义分区数 删除 topic 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first 查看某个 Topic 的详情 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first 发送消息 123$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first&gt;hello world&gt;atguigu atguigu 消费消息 1$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first --from-beginning 会把 first 主题中以往所有的数据都读取出来 六、Spark 配置6.1 环境准备下载地址：https://archive.apache.org/dist/spark/ 有两种版本，hadoop 版下载就能用，不依赖其他组价；without-hadoop 需要依赖已有的 hadoop 组件 spark-2.3.2-bin-hadoop2.7.tgzspark-2.3.2-bin-without-hadoop.tgz 6.2 本地模式 Local Jar 解压后进入 Spark 根目录执行（一个算 PI 的程序）： 123456bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master local[2] \\./examples/jars/spark-examples_2.11-2.3.2.jar 100bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.11-2.3.2.jar 100 输出结果： 12# 一大堆迭代过程 Pi is roughly 3.1424043142404314 可以通过访问 http://hadoop102:4040 查看任务运行情况 「问题」：运行结束这个页面就关闭了，不能查历史任务执行情况 「解决」：添加 Spark History Server Spark-shell 进入 Spark-shell，bin/spark-shell 1234567# 创建两个文件，里面输入几行单词$ vim 1.txt # xxxxx$ vim 2.txt # xxxxx# 进入 spark-shell 执行$ bin/spark-shell&gt; sc.textFile(\"input\").flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).collect 6.3 Standalone 模式构建一个由 Master + Slave 构成的 Spark 集群，Spark 运行在集群中。 这个要和 Hadoop 中的 Standalone 区别开来.这里的 Standalone 是指只用 Spark 来搭建一个集群, 不需要借助其他的框架.是相对于 Yarn 和 Mesos 来说的. 6.3.1 Spark server配置1、进入配置文件目录conf，配置spark-evn.sh 12cd conf/cp spark-env.sh.template spark-env.sh 在 spark-env.sh 文件中配置如下内容: 12SPARK_MASTER_HOST=hadoop102SPARK_MASTER_PORT=7077 # 默认端口就是7077, 可以省略不配 2、修改 slaves 文件，添加 worker 节点 12345cp slaves.template slaves# 在slaves文件中配置如下内容:hadoop201hadoop202hadoop203 3、修改 sbin/spark-config.sh，添加 JAVA_HOME （防止 JAVA_HOME is not set 报错） 1export JAVA_HOME=/opt/module/jdk1.8.0_144 4、分发spark-standalone 5、启动 Spark 集群 1sbin/start-all.sh 6、网页查看信息：http://hadoop102:8080/ 7、测试 1234567bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://hadoop102:7077 \\--executor-memory 1G \\--total-executor-cores 6 \\--executor-cores 2 \\./examples/jars/spark-examples_2.11-2.3.2.jar 100 6.3.2 spark-history-server在 Spark-shell 没有退出之前，看到正在执行的任务的日志情况:http://hadoop102:4040. 但是退出之后，执行的所有任务记录全部丢失 所以需要配置任务的历史服务器, 方便在任何需要的时候去查看日志。 配置spark-default.conf文件，开启 Log 1cp spark-defaults.conf.template spark-defaults.conf 在 spark-defaults.conf 文件中, 添加如下内容: 12spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop102:9000/spark-job-log 注意: hdfs://hadoop201:9000/spark-job-log 目录必须提前存在, 名字随意 修改spark-env.sh文件，添加如下配置 1export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log\" 分发配置文件 启动历史服务 需要先启动 HDFS $HADOOP_HOME/sbin/start-dfs.sh 然后再启动: sbin/start-history-server.sh ui 地址: http://hadoop102:18080 6.4 Yarn 模式6.4.1 spark server 配置 修改 ${HADOOP_HOME}/etc/hadoop/yarn-site.xml（仅虚拟机中配置，防止内存不够） 12345678910&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默&gt;认是true --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默&gt;认是true --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 修改 conf/spark-env.sh，分发 1YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop 测试（注意 master、deploy-mode 参数的变化） 12345678$ start-dfs.sh$ start-yarn.shbin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\./examples/jars/spark-examples_2.11-2.3.2.jar 100 spark-shell 1$ bin/spark-shell --master yarn Yarn：http://hadoop103:8088 6.4.2 spark-history-server$HADOOP_HOME/etc/hadoop/yarn-site.xml 中添加 12345&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop104:19888/jobhistory/logs&lt;/value&gt; &lt;!-- 填 yarn historyserver 的物理机 --&gt;&lt;/property&gt; $SPARK_HOME/conf/spark-defaults.conf 12345spark.yarn.historyServer.address hadoop102:18080 # spark history Server 物理机spark.history.ui.port 18080spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop102:9000/spark-job-logspark.history.fs.logDirectory hdfs://hadoop102:9000/spark-job-log 相关服务 spark: sbin/start-all.sh HDFS: [hadoop102]$ start-dfs.sh Yarn: [hadoop103]$ start-yarn.sh Yarn-history: [hadoop104]$ mr-jobhistory-daemon.sh start historyserver Spark-history: [hadoop102] $ sbin/start-history-server.sh 【可以正常展示了】 相关文档解释：spark深入：配置文件与日志 - Super_Orco - 博客园 查看方式 通过 YARN 查询 http://hadoop103:8088/ 直接在 spark history server 中查询 http://hadoop102:18080/ 6.5 WordCount 程序略 七、Hive 配置7.1 单机默认配置下载地址：http://archive.apache.org/dist/hive/ 安装部署： 修改 conf/hive-env.sh.template 12345678$ mv hive-env.sh.template hive-env.sh$ vim hive-env.sh~~export HADOOP_HOME=/opt/module/hadoop-2.7.2export HIVE_CONF_DIR=/opt/module/hive-2.3.0-bin/conf~~ hadoop 相关配置 123456789# 启动 hdfs yarn$ start-dfs.sh$ start-yarn.sh# 创建 hive warehouse（存数据的地方）$ hadoop fs -mkdir /tmp$ hadoop fs -mkdir -p /user/hive/warehouse$ hadoop fs -chmod g+w /tmp$ hadoop fs -chmod g+w /user/hive/warehouse Hive 基本操作 1234567891011121314151617$ bin/hive# 查看数据库hive&gt; show databases;# 打开默认数据库 hive&gt; use default;# 显示 default 数据库中的表 hive&gt; show tables;# 创建一张表hive&gt; create table student(id int, name string);# 查看表的结构 hive&gt; desc student;# 向表中插入数据hive&gt; insert into student values(1000,\"ss\");# 查询表中数据hive&gt; select * from student;# 退出 hive hive&gt; quit; 7.2 修改默认数据库（derby -&gt; MySQL）derby 只支持单个客户端连接，仅适用于简单测试。更换成关系型数据库（如MySQL），可支持多客户端连接。 7.2.1 安装 MySQL 卸载原有的，安装新的 MySQL 12345678910111213141516171819# 卸载原有的$ su - # 切换到 root 用户$ rpm -qa | grep mysql mysql-libs-5.1.73-7.el6.x86_64$ rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64# 安装新的 MySQL-server$ rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpmcat /root/.mysql_secret # 记住默认的root登录密码OEXaQuS8IWkG19Xs$ service mysql status$ service mysql start# 安装 MySQL-client$ rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm# 修改 root 密码$ mysql -uroot -pOEXaQuS8IWkG19Xsmysql&gt;SET PASSWORD=PASSWORD('123456');mysql&gt;exit 配置 MySQL 远程登录 12345678910111213141516$ mysql -uroot -p123456mysql&gt; use mysql;mysql&gt; show tables;mysql&gt; select User, Host, Password from user;# 修改 user 表，把 Host 表内容修改为%mysql&gt; update user set host='%' where host='localhost';# 其他的都删掉mysql&gt; delete from user where host='hadoop102';mysql&gt; delete from user where host='127.0.0.1';mysql&gt; delete from user where host='::1';mysql&gt; flush privileges;mysql&gt; quit;# 都做完切换回原来的用户 7.2.2 修改 hive 元数据库 拷贝驱动 1$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive-2.3.0-bin/lib/ 配置 Metastore 到 MySQL 创建 conf/hive-site.xml 官方配置文档AdminManual Metastore Administration - Apache Hive - Apache Software Foundation 123456789101112131415161718192021222324&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt; &lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 启动 12345678# 初试化hive库$ bin/schematool -initSchema -dbType mysql# 启动metastore节点$ nohup bin/hive --service metastore &amp;# 启动hiveserver2（可选？） $ nohup bin/hive --service hiveserver2 &amp;# 命令行启动$ bin/hive 7.3 Beeline 连接 Hive学习之路 （四）Hive的连接3种连接方式 - 扎心了，老铁 - 博客园https://www.cnblogs.com/qingyunzong/p/8715925.html 7.4 常用交互命令1、-e不进入 hive 的交互窗口执行 sql 语句 1$ bin/hive -e \"select id from student;\" 2、-f执行脚本中 sql 语句 创建 hivef.sql 文件 123$ touch hivef.sqlselect *from student;$ bin/hive -f xxx/hivef.sql &gt; xxx/result.txt 7.5 常用参数配置 查询后信息显示配置 conf/hive-site.xml 12345678910&lt;!-- 可以显示表头列名 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 显示当前数据库名 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 八、HBase 配置8.1 集群配置 conf/hbase-env.sh 1234567# 1、注释掉下面两行# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+export HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m\"export HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSiz e=128m\"# 2、使用单独的 Zookeeperexport HBASE_MANAGES_ZK=false conf/hbase-site.sh 1234567891011121314151617181920&lt;!-- 都要根据自己的机器进行配置 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt; 单独启动 123456789101112# 启动 HDFS、Zookeeper$ start-dfs.sh$ $&#123;Zookeeper_BASE&#125;/bin/zkServer.sh start # 每个机器都要单独启动# HBase$ bin/hbase-daemon.sh start master # 在其中一台启动$ bin/hbase-daemon.sh start regionserver # 都要启动$ bin/hbase-daemon.sh stop master # 在其中一台启动$ bin/hbase-daemon.sh stop regionserver # 都要启动# 可以在 hadoop102:16010 查看ui界面 群起 / 群停 12345678910111213141516# 启动 HDFS、Zookeeper$ start-dfs.sh$ $&#123;Zookeeper_BASE&#125;/bin/zkServer.sh start # 每个机器都要单独启动# 配置 conf/regionservers，添加所有 regionserver 的 hosthadoop102hadoop103hadoop104# 群起 / 群停$ bin/start-hbase.sh$ bin/stop-hbase.sh# regionservers$ bin/hbase-daemons.sh start regionserver$ bin/hbase-daemons.sh stop regionserver 进入 shell 12# 如果有 kerberos 认证，需要先 kinit 一下$ bin/hbase shell 8.2 常用操作使用 help 查看各种命令使用方式 12345678# 列出所有指令&gt; help# 查看某一组命令的帮助&gt; help 'COMMAND_GROUP'# 查看单个命令帮助&gt; help 'COMMAND' 8.2.1 namespaceCommands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables list_namespace 12# 查看所有库名&gt; list_namespace create_namespace 1&gt; create_namespace 'school' delete_namespace 12# 注意：只能删除空库&gt; delete_namespace 'school' list_namespace_tables 12# 列出库中所有表&gt; list_namespace_tables 'school' 8.2.2 DDLCommands: alter, alter_async, alter_status, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, locate_region, show_filters list 12# list 列出所有表&gt; list create 12345678# create '库名:表名', &#123; NAME =&gt; '列族名1', 属性名 =&gt; 属性值&#125;, &#123;NAME =&gt; '列族名2', 属性名 =&gt; 属性值&#125;, …&gt; create 'school:student', &#123;NAME=&gt;'info'&#125;&gt; create 'school:student', &#123;NAME=&gt;'info', VERSIONS=&gt;5&#125;# 如果你只需要创建列族，而不需要定义列族属性，那么可以采用以下快捷写法：# create'表名','列族名1' ,'列族名2', …# 不写库名，默认 namespace 为 default&gt; create 'student','info' desc 1&gt; desc 'student' disable 123# 停用表，防止对表进行写数据；在修改或删除表之前要 disable&gt; disable 'student'&gt; is_disable 'student' enable 123# 启用表&gt; enable 'student'&gt; is_enable 'student' alter 12# 需要先 disable&gt; alter 'student', &#123;NAME =&gt; 'info', VERSIONS =&gt; '5'&#125; drop 12# 需要先 disable&gt; drop 'student' count 12# 查看行数&gt; count 'student' truncate 12# 删除表数据&gt; truncate 'student' 8.2.3 DMLCommands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve scan 1234# 查看数据&gt; scan 'student', &#123;limit =&gt; 5&#125;# 查看每行最近十次修改的数据&gt; scan 'student', &#123;RAW =&gt; true, VERSIONS =&gt; 10&#125; put 12# put '表名', '行键', '列族:列名', '值'&gt; put 'student', '1001', 'info:name', 'Nick' get 1&gt; get 'student','1001' delete 1234# 删除某rowkey的全部数据：&gt; deleteall 'student', '1001'# 删除某rowkey的某一列数据：&gt; delete 'student', '1002', 'info:sex' 8.2.4 其他操作 flush 12# 将内存数据落盘&gt; flush 'student'","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hadoop, 教程","slug":"Hadoop-教程","permalink":"https://simon-ace.github.io/tags/Hadoop-教程/"}]},{"title":"vim 常用操作","slug":"Linux/vim 常用操作","date":"2020-09-16T16:00:00.000Z","updated":"2020-11-22T03:56:40.105Z","comments":true,"path":"2020/09/17/Linux/vim 常用操作/","link":"","permalink":"https://simon-ace.github.io/2020/09/17/Linux/vim 常用操作/","excerpt":"12# 取消高亮:noh","text":"12# 取消高亮:noh 取消搜索后高亮 1234# no high light search:nohlsearch# 简写:noh 移动 12w # 移至下一单词b # 移至上一单词 剪切、复制、粘贴、删除 1234567dd # 删除当前行5dd # 删除5行yy # 复制当前行5yy # 复制5行p # 粘贴 设置 tab 键长度 1:set tabstop=4 开启自动缩进 12:set autoindentctrl+d # 停止自动缩进","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"vim, 教程","slug":"vim-教程","permalink":"https://simon-ace.github.io/tags/vim-教程/"}]},{"title":"构建Hadoop的Docker编译环境","slug":"Hadoop/构建 Hadoop 的 Docker 编译环境","date":"2020-08-31T16:00:00.000Z","updated":"2020-09-02T02:47:19.693Z","comments":true,"path":"2020/09/01/Hadoop/构建 Hadoop 的 Docker 编译环境/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Hadoop/构建 Hadoop 的 Docker 编译环境/","excerpt":"","text":"一、配置 docker 环境 参考链接：Hadoop安装之一：使用Docker编译64位的Hadoop - 简书 1. 制作 CentOS 7 基础镜像（可选）Docker Hub上已经提供了CentOS7的官方镜像，但并未激活 Systemd（用来启动守护进程），制作一个启动 Systemd 的镜像。（这里编译Hadoop其实用不到systemd） Dockerfile 1234567891011121314151617181920212223242526# 镜像来源FROM centos:7# 镜像创建者MAINTAINER \"you\" &lt;your@email.here&gt;# 设置一个环境变量ENV container docker# 运行命令# 设置systemdRUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == \\systemd-tmpfiles-setup.service ] || rm -f $i; done); \\rm -f /lib/systemd/system/multi-user.target.wants/*;\\rm -f /etc/systemd/system/*.wants/*;\\rm -f /lib/systemd/system/local-fs.target.wants/*; \\rm -f /lib/systemd/system/sockets.target.wants/*udev*; \\rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \\rm -f /lib/systemd/system/basic.target.wants/*;\\rm -f /lib/systemd/system/anaconda.target.wants/*;# 挂载一个本地文件夹VOLUME [ \"/sys/fs/cgroup\" ]# 设置容器启动时的执行命令CMD [\"/usr/sbin/init\"] 生成镜像 1docker build -t centos7-systemd . 2. 安装 Oracle Java 参考链接使用yum卸载、安装jdk_不做小白的博客-CSDN博客 注意不要使用 openjdk，会导致编译 hive 时出现问题 启动刚刚生成的镜像 从官网下载 oracle java jdk-8u202-linux-x64.tar.gz 安装 Java，配置环境变量 12345678910111213141516171819mkdir /usr/local/javacp jdk-8u202-linux-x64.tar.gz /usr/local/javacd /usr/local/javatar -xzvf jdk-8u202-linux-x64.tar.gz# 配置环境变量vim /etc/profile~~export JAVA_HOME=/usr/local/java/jdk1.8.0_202export JRE_HOME=/usr/local/java/jdk1.8.0_202/jre export PATH=$PATH:/usr/local/java/jdk1.8.0_202/bin export CLASSPATH=./:/usr/local/java/jdk1.8.0_202/lib:/usr/local/java/jdk1.8.0_202/jre/lib~~source /etc/profile# 检查 JAVA 是否安装成功java -version 保存镜像 1docker commit 容器id 镜像名 3. 制作编译镜像 编译脚本 1234567891011121314151617181920$ vi compile.sh#!/bin/bash# 设置默认编译版本(支持传参)version=$&#123;1:-2.7.3&#125;# 进入源代码目录cd /hadoop-$version-src# 开始编译echo -e \"\\n\\ncompile hadoop $version...\"mvn clean package -Pdist,native -DskipTests -Dtar# 输出结果if [[ $? -eq 0]]; then echo -e \"\\n\\ncompile hadoop $version success!\\n\\n\"else echo -e \"\\n\\ncompile hadoop $version fail!\\n\\n\"fi Dockerfile（其中有不少安装包不是必要的） 123456789101112131415161718192021222324252627# 镜像来源(第二步生成的本地镜像)FROM centos7-systemd-java# 镜像创建者MAINTAINER \"you\" &lt;your@email.here&gt;# 运行命令安装环境依赖# 使用 -y 同意全部询问RUN yum update -y &amp;&amp; \\ yum groupinstall -y \"Development Tools\" &amp;&amp; \\ yum install -y wget \\ protobuf-devel \\ protobuf-compiler \\ maven \\ cmake \\ pkgconfig \\ openssl-devel \\ zlib-devel \\ gcc \\ automake \\ autoconf \\ make # 复制编辑脚本文件到镜像中COPY compile.sh /root/compile.sh# 设置脚本文件的可运行权限RUN chmod +x /root/compile.sh 生成镜像 1sudo docker build -t centos7-hadoop-compiler . 二、编译源码 hive（大概10分钟） 1mvn clean package -Pdist -DskipTests hadoop（大概15分钟） 1mvn clean package -Pdist,native -DskipTests -Dtar 也可以使用 docker image 中的脚本编译 12$ export VERSION=2.7.3$ sudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src --privileged=true centos7-hadoop-complier /root/compile.sh $VERSION 要添加 privileged 参数！ [docker]privileged参数_追寻神迹-CSDN博客 使用该参数，container内的root拥有真正的root权限。否则，container内的root只是外部的一个普通用户权限。 总结 1234567891011121314151617181920212223242526272829# pull docker imagedocker pull shuofxz/hadoop-compiler:1.0# === HADOOP ===# hadoop download link# new versionhttps://hadoop.apache.org/releases.html# old versionhttps://archive.apache.org/dist/hadoop/common/# compile command (about 15 minutes to complete)mvn package -Pdist,native -DskipTests -Dtar# compile with script file$ export VERSION=2.7.3$ sudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src --privileged=true shuofxz/hadoop-compiler:1.0 /root/hadoop-compile.sh $VERSION# === HIVE ===# hive download link# select corresponding branch src file to downloadhttps://github.com/apache/hive# compile command (about 10 minutes to complete)mvn clean package -Pdist -DskipTests# compile with script file$ export VERSION=2.3.0$ sudo docker run -v $(pwd)/hive-rel-release-$VERSION:/hive-rel-release-$VERSION --privileged=true shuofxz/hadoop-compiler:1.0 /root/hive-compile.sh $VERSION hadoop-2.7.0-src.tar.gz release-2.3.0.tar.gz 已包括各种库的 image，可以直接编译 hadoop（不好用） GitHub - kiwenlau/compile-hadoop: Compile Hadoop in Docker containerhttps://github.com/kiwenlau/compile-hadoop","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"docker,教程","slug":"docker-教程","permalink":"https://simon-ace.github.io/tags/docker-教程/"}]},{"title":"Linux压缩命令 tar","slug":"Linux/常用命令/Linux压缩命令 tar","date":"2020-08-31T16:00:00.000Z","updated":"2021-01-12T07:18:38.035Z","comments":true,"path":"2020/09/01/Linux/常用命令/Linux压缩命令 tar/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Linux/常用命令/Linux压缩命令 tar/","excerpt":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx","text":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx 一、常用压缩参数必选参数，压缩解压都要用到其中一个： -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 可选参数： -z：有gzip属性的 -j： 有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 -C：解压时指定文件夹 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 二、举个栗子压缩 123456789101112131415# 将目录里所有jpg文件打包成tar.jpgtar -cvf jpg.tar *.jpg # 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gztar -czvf jpg.tar.gz *.jpg# 打包文件夹tar -czvf xxx.tar.gz /etc/folder# rar格式的压缩，需要先下载 rar for linuxrar a jpg.rar *.jpg # zip格式的压缩，需要先下载 zip for linuxzip jpg.zip *.jpg# zip 文件夹zip -r folder.zip folder 解压 12345678910111213141516# 解压 tar包tar -xvf file.tar # 指定文件夹tar -xvf file.tar -C /home/xxx# 解压tar.gztar -xzvf file.tar.gz # 解压 tar.bz2tar -xjvf file.tar.bz2 # 解压rarunrar e file.rar # 解压zipunzip file.zip 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压 1234tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Docker 批量操作","slug":"Docker 批量操作","date":"2020-08-27T16:00:00.000Z","updated":"2020-09-01T10:04:59.753Z","comments":true,"path":"2020/08/28/Docker 批量操作/","link":"","permalink":"https://simon-ace.github.io/2020/08/28/Docker 批量操作/","excerpt":"1docker rmi $(docker images | grep \"none\" | awk '&#123;print $3&#125;')","text":"1docker rmi $(docker images | grep \"none\" | awk '&#123;print $3&#125;') 列出所有的容器 ID 1docker ps -aq 停止所有的容器 1docker stop $(docker ps -aq) 删除所有的容器 1docker rm $(docker ps -aq) 删除所有的镜像 1docker rmi $(docker images -q) 删除指定名称镜像 1docker rmi $(docker images | grep \"none\" | awk '&#123;print $3&#125;') 复制文件 12docker cp mycontainer:/opt/file.txt /opt/local/docker cp /opt/local/file.txt mycontainer:/opt/ 现在的docker有了专门清理资源(container、image、网络)的命令。 docker 1.13 中增加了 docker system prune的命令，针对container、image可以使用docker container prune、docker image prune命令。 删除所有不使用的镜像 123docker image prune --force --all# ordocker image prune -f -a 删除所有停止的容器 1docker container prune -f","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://simon-ace.github.io/tags/docker/"}]},{"title":"Cron语法","slug":"Linux/cron语法","date":"2020-08-16T16:00:00.000Z","updated":"2020-11-26T04:22:48.797Z","comments":true,"path":"2020/08/17/Linux/cron语法/","link":"","permalink":"https://simon-ace.github.io/2020/08/17/Linux/cron语法/","excerpt":"1234567* * * * * * *秒 分钟 小时 天 月 星期 年# -------------------# linux crontab 只有5个 * * * * * 分钟 小时 天 月 星期","text":"1234567* * * * * * *秒 分钟 小时 天 月 星期 年# -------------------# linux crontab 只有5个 * * * * * 分钟 小时 天 月 星期 1 表达式详解一个cron表达式有至少6个（也可能7个）有空格分隔的时间元素。 按顺序依次为 1 秒（0~59） 2 分钟（0~59） 3 小时（0~23） 4 天（0~31） 5 月（0~11） 6 星期（1~7 1=SUN 或 SUN，MON，TUE，WED，THU，FRI，SAT） 7 年份（1970－2099） 每个元素格式： 一个具体值（如6） 一个连续区间（9-12） 一个列表(1,3,5) 特殊字符 通配符（*），所有可能的值 空符号（？），表示不指定值 由于”月份中的日期”和”星期中的日期”这两个元素互斥的,必须要对其中一个设置? 增量符（/） 如第二位12/10 表示从第12分钟开始，每10分钟（它和“12，22，32…”） 最后（L） 仅被用于天（月）和天（星期）两个子表达式，它是单词“last”的缩写 “6L”表示这个月的倒数第６天 平日（W） 仅能用于日域中，它用来指定离指定日的最近的一个工作日（1-5） 日域中的 15W 意味着 “离该月15号的最近一个平日 2 例子123456789101112130 0 10,14,16 * * ? 每天上午10点，下午2点，4点0 0/30 9-17 * * ? 朝九晚五工作时间内每半小时0 0 12 ? * WED 表示每个星期三中午12点0 0 12 * * ? 每天中午12点触发0 15 10 ? * * 每天上午10:15触发0 15 10 * * ? 2005 2005年的每天上午10:15触发0 * 14 * * ? 在每天下午2点到下午2:59期间的每1分钟触发0 0/5 14,18 * * ? 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发0 0-5 14 * * ? 在每天下午2点到下午2:05期间的每1分钟触发0 10,44 14 ? 3 WED 每年三月的星期三的下午2:10和2:44触发0 15 10 ? * MON-FRI 周一至周五的上午10:15触发0 15 10 L * ? 每月最后一日的上午10:15触发0 15 10 ? * 6L 每月的最后一个星期五上午10:15触发","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"cron","slug":"cron","permalink":"https://simon-ace.github.io/tags/cron/"}]},{"title":"Hexo常用命令","slug":"Hexo常用命令","date":"2020-08-12T16:00:00.000Z","updated":"2020-08-13T10:18:27.861Z","comments":true,"path":"2020/08/13/Hexo常用命令/","link":"","permalink":"https://simon-ace.github.io/2020/08/13/Hexo常用命令/","excerpt":"123456# 生成静态文件hexo g# 启动服务hexo s# 部署hexo d","text":"123456# 生成静态文件hexo g# 启动服务hexo s# 部署hexo d 常用命令 生成静态文件 123hexo generate# 简写hexo g 启动服务预览文章 12345hexo server# 简写hexo s# 指定端口hexo server -p 5000 一键部署 123hexo deploy# 简写hexo d","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"","slug":"Docker部署Vue项目","date":"2020-08-12T07:02:40.098Z","updated":"2020-08-12T07:02:43.859Z","comments":true,"path":"2020/08/12/Docker部署Vue项目/","link":"","permalink":"https://simon-ace.github.io/2020/08/12/Docker部署Vue项目/","excerpt":"","text":"[手把手系列之]Docker 部署 vue 项目 - 掘金https://juejin.im/post/6844903837774397447","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-07-10T03:23:10.782Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2020/07/10/hello-world/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Mac安装node","slug":"Mac安装node","date":"2020-07-09T16:00:00.000Z","updated":"2020-07-10T06:24:18.048Z","comments":true,"path":"2020/07/10/Mac安装node/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/Mac安装node/","excerpt":"Mac安装及降级node版本","text":"Mac安装及降级node版本 1 安装最新版Node 安装HomeBrew 1/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 安装Node 1brew install node 验证Node是否安装成功 输入下面两条指令看是否可以都输出版本号 12node -vnpm -v 2 降级Node由于开发需要或版本兼容性，需要安装低版本的Node，按下面的方式操作 卸载Node 如果你是按前面的方法安装的Node，则用下面的命令卸载 1brew uninstall node 查看可用的Node版本 1brew search node 输出结果： 12==&gt; Formulaelibbitcoin-node node node-sass node@12 nodebrew nodenv llnode node-build node@10 node_exporter nodeenv 安装你需要的版本 12# 这里安装v12版本brew install node@12 连接Node 12brew link node@12# 这一步可能会报错, 按照提示执行命令就ok了, 比如我最后执行的是brew link --overwrite --force node@12 检查Node是否安装成功 1node -v","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"iTerm2配置","slug":"配置/iTerm2配置","date":"2020-07-09T16:00:00.000Z","updated":"2020-07-10T10:23:28.607Z","comments":true,"path":"2020/07/10/配置/iTerm2配置/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/配置/iTerm2配置/","excerpt":"iTerm2配置 Oh-my-zsh安装，主题配置","text":"iTerm2配置 Oh-my-zsh安装，主题配置 1 安装iTerm2iTerm2 是一款完全免费的，专为 Mac OS 用户打造的命令行应用。直接在官网上 http://iterm2.com/ 下载并安装即可。 设置为默认终端 2 安装 oh-my-zshbash是mac中terminal自带的shell，把它换成oh-my-zsh，这个的功能要多得多。拥有语法高亮，命令行tab补全，自动提示符，显示Git仓库状态等功能。 1sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 解决权限问题 如果安装完重启iterm之后，出现下面的提示： 12345[oh-my-zsh] Insecure completion-dependent directories detected:drwxrwxrwx 7 hans admin 238 2 9 10:13 /usr/local/share/zshdrwxrwxrwx 6 hans admin 204 10 1 2017 /usr/local/share/zsh/site-functions...... 解决方法： 12chmod 755 /usr/local/share/zshchmod 755 /usr/local/share/zsh/site-functions 3 配置主题4 Vim配置设置鼠标滚动","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"Mac使用代理ssh远程连接服务器 & keep alive","slug":"配置/Mac使用代理ssh远程连接服务器 & keep alive","date":"2020-07-09T16:00:00.000Z","updated":"2020-11-25T09:52:52.298Z","comments":true,"path":"2020/07/10/配置/Mac使用代理ssh远程连接服务器 & keep alive/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/配置/Mac使用代理ssh远程连接服务器 & keep alive/","excerpt":"123456# 直接连接ssh -p 端口号 服务器用户名@ip地址# eg: ssh -p 22 userkunyu@119.29.37.63# 通过代理连接ssh -o ProxyCommand=\"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p\" 需要访问的服务器的用户名@需要访问的服务器ip","text":"123456# 直接连接ssh -p 端口号 服务器用户名@ip地址# eg: ssh -p 22 userkunyu@119.29.37.63# 通过代理连接ssh -o ProxyCommand=\"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p\" 需要访问的服务器的用户名@需要访问的服务器ip 1 直接连接12# ssh -p 端口号 服务器用户名@ip地址ssh -p 22 userkunyu@119.29.37.63 2 通过代理连接 直接连接 12# ssh -o ProxyCommand=\"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p\" 需要访问的服务器的用户名@需要访问的服务器ipssh -o ProxyCommand=\"nc -X 5 -x 192.168.0.255:9999 %h %p\" user_name@192.168.77.200 使用SSH配置文件 1sudo vi ~/.ssh/config 12Host * ProxyCommand nc -X 5 -x 192.168.0.255:9999 %h %p 配置好了之后就可以和直接连接一样使用 1ssh uesr@ip Mac下SSH跳点连接及代理连接_Dawnworld-CSDN博客_mac ssh 代理https://blog.csdn.net/thundon/article/details/46858957 3 Keep alive http://bluebiu.com/blog/iterm2-ssh-session-idle.html 方案一： 在本机 vim ~/.ssh/config 123# 在开头添加Host * ServerAliveInterval 60 我觉得60秒就好了，而且基本去连的机器都保持，所以配置了*，如果有需要针对某个机器，可以自行配置为需要的serverHostName。 方案二： 单次连接 添加下面的参数 1ssh -o ServerAliveInterval=30 user@host","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"Linux后台执行命令 nohup","slug":"Linux/常用命令/Linux后台执行命令 nohup","date":"2019-10-21T16:00:00.000Z","updated":"2020-11-27T06:48:34.581Z","comments":true,"path":"2019/10/22/Linux/常用命令/Linux后台执行命令 nohup/","link":"","permalink":"https://simon-ace.github.io/2019/10/22/Linux/常用命令/Linux后台执行命令 nohup/","excerpt":"当在终端工作时，可能一个持续运行的作业占住屏幕输出，或终端退出时导致命令结束。为了避免这些问题，可以将这些进程放到后台运行，且不受终端关闭的影响，可使用下面的方法： 1nohup command &gt; myout.file 2&gt;&amp;1 &amp;","text":"当在终端工作时，可能一个持续运行的作业占住屏幕输出，或终端退出时导致命令结束。为了避免这些问题，可以将这些进程放到后台运行，且不受终端关闭的影响，可使用下面的方法： 1nohup command &gt; myout.file 2&gt;&amp;1 &amp; 1 后台执行命令1.1 命令&amp;在命令后面加上&amp;实现后台运行（控制台关掉(退出帐户时)，作业就会停止运行） 1command &amp; 例：python run.py &amp; 1.2 命令nohupnohup命令可以在你退出帐户之后继续运行相应的进程。nohup就是不挂起的意思( no hang up) 1nohup command &amp; 例：nohup run.py &amp; 2 kill进程执行后台任务命令后，会返回一个进程号，可通过这个进程号kill掉进程。 1kill -9 进程号 3 输出重定向由于使用前面的命令将任务放到后台运行，因此任务的输出也不打印到屏幕上了，所以需要将输出重定向到文件中，以方便查看输出内容。 将输出重定向到 file（覆盖） 1command1 &gt; file1 将输出重定向到 file（追加） 1command1 &gt;&gt; file1 将 stdout 和 stderr 合并后重定向到 file 2&gt;1代表什么，2与&gt;结合代表错误重定向，而1则代表错误重定向到一个文件1，而不代表标准输出；换成2&gt;&amp;1，&amp;与1结合就代表标准输出了，就变成错误重定向到标准输出. 1command1 &gt; file1 2&gt;&amp;1 完整写法： 1nohup command &gt;out.file 2&gt;&amp;1 &amp; 4 其他 nohup执行python程序时，print无法输出 这是因为python的输出有缓冲，导致nohup.out并不能够马上看到输出 python 有个-u参数，使得python不启用缓冲 nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Linux统计文件数目 wc","slug":"Linux/常用命令/Linux统计文件夹下的文件数目 wc","date":"2019-10-21T16:00:00.000Z","updated":"2020-11-27T06:49:00.812Z","comments":true,"path":"2019/10/22/Linux/常用命令/Linux统计文件夹下的文件数目 wc/","link":"","permalink":"https://simon-ace.github.io/2019/10/22/Linux/常用命令/Linux统计文件夹下的文件数目 wc/","excerpt":"1$ ls -l | grep &quot;^-&quot; | wc -l","text":"1$ ls -l | grep &quot;^-&quot; | wc -l 1 统计文件夹下的文件数目 统计当前目录下文件的个数（不包括目录） 1$ ls -l | grep &quot;^-&quot; | wc -l 统计当前目录下文件的个数（包括子目录） 1$ ls -lR| grep &quot;^-&quot; | wc -l 查看某目录下文件夹(目录)的个数（包括子目录） 1$ ls -lR | grep &quot;^d&quot; | wc -l 命令原理： ls -l 详细输出该文件夹下文件信息 ls -lR是列出所有文件，包括子目录 grep &quot;^-&quot; 过滤ls的输出信息，只保留一般文件；只保留目录是grep &quot;^d&quot; wc -l 统计输出信息的行数","categories":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Python打印更详细的异常信息","slug":"Python打印更详细的异常信息","date":"2019-10-20T16:00:00.000Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2019/10/21/Python打印更详细的异常信息/","link":"","permalink":"https://simon-ace.github.io/2019/10/21/Python打印更详细的异常信息/","excerpt":"打印Python异常信息的几种方式","text":"打印Python异常信息的几种方式 1 简单的异常信息1234try: a = 1/0except Exception as e: print(e) 打印最简单的message信息： 1division by zero 2 更完整的信息12345678910import tracebacktry: a = 1/0except Exception as e: print('str(e):\\t', e) print('repr(e):\\t', repr(e)) print('traceback.format_exc():\\n%s' % traceback.format_exc()) #字符串 traceback.print_exc() #执行函数 输出： 123456789101112str(e): division by zerorepr(e): ZeroDivisionError(&apos;division by zero&apos;)traceback.format_exc():Traceback (most recent call last): File &quot;/Users/ace/Play/test/异常信息.py&quot;, line 4, in &lt;module&gt; a = 1/0ZeroDivisionError: division by zeroTraceback (most recent call last): File &quot;/Users/ace/Play/test/异常信息.py&quot;, line 4, in &lt;module&gt; a = 1/0ZeroDivisionError: division by zero traceback.format_exc()和traceback.print_exc()都可以打印完整的错误信息 traceback.format_exc()返回值为字符串 traceback.print_exc()是一个执行函数，直接在控制台打印错误信息","categories":[{"name":"Python","slug":"Python","permalink":"https://simon-ace.github.io/categories/Python/"}],"tags":[{"name":"Python, 异常","slug":"Python-异常","permalink":"https://simon-ace.github.io/tags/Python-异常/"}]},{"title":"【转】持续集成 Continuous Integration","slug":"配置/持续集成 Continuous Integration","date":"2019-10-17T16:00:00.000Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2019/10/18/配置/持续集成 Continuous Integration/","link":"","permalink":"https://simon-ace.github.io/2019/10/18/配置/持续集成 Continuous Integration/","excerpt":"持续集成 Continuous Integration","text":"持续集成 Continuous Integration","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"Hexo多台电脑同步","slug":"Hexo多台电脑同步","date":"2019-10-15T16:00:00.000Z","updated":"2020-08-13T09:06:57.490Z","comments":true,"path":"2019/10/16/Hexo多台电脑同步/","link":"","permalink":"https://simon-ace.github.io/2019/10/16/Hexo多台电脑同步/","excerpt":"如果换了电脑该如何同步Hexo的源文件？把hexo文件从一个电脑cope到另外一个电脑吗？答案肯定不是这样的，因为这里面有好多依赖包，好几万个文件呢，这样显然不合理。 本文提供一种多台电脑同步源文件的方法。","text":"如果换了电脑该如何同步Hexo的源文件？把hexo文件从一个电脑cope到另外一个电脑吗？答案肯定不是这样的，因为这里面有好多依赖包，好几万个文件呢，这样显然不合理。 本文提供一种多台电脑同步源文件的方法。 0 解决思路使用GitHub的分支！在博客对应的仓库中新建一个分支。一个分支用来存放Hexo生成的网站原始的文件，另一个分支用来存放生成的静态网页。 1 创建分支1.1 创建新分支命令行操作： GitHub操作： 点击branch按钮，输入新的分支名source，点创建。 1.2 设置默认分支准备在source分支中存放源文件，master中存放生成的网页，因此将source设置为默认分支，方便同步文件。 在仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为source，save保存 2 源文件上传到GitHub 选好一个本地文件夹，执行 git clone git@github.com:Simon-Ace/Simon-Ace.github.io.git(替换成你的仓库) 在克隆到本地的Simon-Ace.github.io中，把除了.git 文件夹外的所有文件都删掉 把之前我们写的博客源文件全部复制过来，除了.deploy_git 复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的.git文件夹删掉，因为git不能嵌套上传。 提交更改 123git add .git commit –m \"add branch\"git push 参考文章： https://juejin.im/post/5acf22e6f265da23994eeac9 https://www.zhihu.com/question/21193762","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"Python-加快pip安装速度","slug":"tutorials/Python-加快pip安装速度","date":"2019-10-15T16:00:00.000Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2019/10/16/tutorials/Python-加快pip安装速度/","link":"","permalink":"https://simon-ace.github.io/2019/10/16/tutorials/Python-加快pip安装速度/","excerpt":"PIP安装时使用国内镜像，加快下载速度","text":"PIP安装时使用国内镜像，加快下载速度 0 国内源清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/ 1 临时使用 可以在使用pip的时候加参数-i https://pypi.tuna.tsinghua.edu.cn/simple 例如： pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy 2 永久修改这样就不用每次都添加国内镜像源地址了 Linux下，修改~/.pip/pip.conf（没有就创建一个文件夹及文件） 打开文件，添加内容： 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.com windows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini， 内容同上","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"一台电脑配置多个git账号","slug":"Linux/配置多个git账号","date":"2019-10-13T16:00:00.000Z","updated":"2020-11-22T03:56:40.106Z","comments":true,"path":"2019/10/14/Linux/配置多个git账号/","link":"","permalink":"https://simon-ace.github.io/2019/10/14/Linux/配置多个git账号/","excerpt":"1 清除git全局设置如果配置第一个账号的时候使用git config --global设置过，就先要取消掉，否则两个账号肯定会冲突 123# 取消globalgit config --global --unset user.namegit config --global --unset user.email","text":"1 清除git全局设置如果配置第一个账号的时候使用git config --global设置过，就先要取消掉，否则两个账号肯定会冲突 123# 取消globalgit config --global --unset user.namegit config --global --unset user.email 2 生成新账号的SSH keys2.1 用 ssh-keygen 命令生成密钥1$ ssh-keygen -t rsa -C \"new email\" 平时都是直接回车，默认生成 id_rsa 和 id_rsa.pub。这里特别需要注意，出现提示输入文件名的时候(Enter file in which to save the key (~/.ssh/id_rsa): id_rsa_new)要输入与默认配置不一样的文件名，比如：我这里填的是 id_rsa和id_rsa_me。 如果之前没配置过ssh key，这里用不同邮箱生成两遍即可，注意用不同的文件名 成功后会出现： 12Your identification has been saved in xxx.Your public key has been saved in xxx. 2.2 添加到ssh-agent中使用ssh-add将 IdentityFile 添加到 ssh-agent中 12ssh-add ~/.ssh/id_rsassh-add ~/.ssh/id_rsa_me 2.3 配置 ~/.ssh/config 文件在~/.ssh/下新建config文件 1234567891011# The git info for companyHost git.XXX.com # git别名，写公司的git名字即可HostName git.XXX.com # git名字，同样写公司的git名字User git # 写 git 即可IdentityFile ~/.ssh/id_rsa #私钥路径，若写错会连接失败# The git info for github Host github.com # git别名，写github的git名字即可HostName github.com # git名字，同样写github的git名字User git # 写 git 即可IdentityFile ~/.ssh/id_rsa_me #私钥路径，若写错会连接失败 3 与GitHub链接复制刚刚生成的两个ssh公钥到对应的账号中 文件id_rsa.pub中保存的就是 ssh 公钥 12pbcopy &lt; ~/.ssh/id_rsa.pubpbcopy &lt; ~/.ssh/id_rsa_me.pub 在 github 网站中添加该 ssh 公钥 验证是否配置成功，以 github 为例，输入 ssh -T git@github.com，若出现 1Hi xxx! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 这样的字段，即说明配置成功。另一个同理。 参考链接： 配置多个git账号的ssh密钥 - 掘金https://juejin.im/post/5befe84d51882557795cc8f9 同一台电脑配置多个git账号 · Issue #2 · jawil/noteshttps://github.com/jawil/notes/issues/2","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"git","slug":"git","permalink":"https://simon-ace.github.io/tags/git/"}]}]}