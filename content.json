{"meta":{"title":"Rookie的博客","subtitle":"Stay hungry. Stay foolish.","description":null,"author":"Simon","url":"https://simon-ace.github.io","root":"/"},"pages":[],"posts":[{"title":"Java 开发 Hadoop 程序","slug":"Hadoop/Java 开发 Hadoop 程序","date":"2021-05-24T16:00:00.000Z","updated":"2021-05-25T11:44:07.062Z","comments":true,"path":"2021/05/25/Hadoop/Java 开发 Hadoop 程序/","link":"","permalink":"https://simon-ace.github.io/2021/05/25/Hadoop/Java 开发 Hadoop 程序/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546@RequestMapping(value = \"/updatenmtest\", method = RequestMethod.GET) public String updateNmTest() throws Exception &#123; YarnConfiguration conf = new YarnConfiguration(); YarnClient yarnClient = YarnClient.createYarnClient(); String yarnSitePath = \"/disk1/eadop/hadoop-2.8.5/etc/hadoop/yarn-site.xml\"; String coreSitePath = \"/disk1/eadop/hadoop-2.8.5/etc/hadoop/core-site.xml\"; conf.addResource(new Path(yarnSitePath)); conf.addResource(new Path(coreSitePath)); // kerberos System.setProperty(\"java.security.krb5.conf\", \"/etc/krb5.conf\" ); conf.setBoolean(\"hadoop.security.authorization\", true); conf.set(\"hadoop.security.authentication\", \"kerberos\"); ArrayList&lt;String&gt; nodeIdList = new ArrayList&lt;&gt;(); UserGroupInformation.setConfiguration(conf); UserGroupInformation ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(\"yarn/hd047.corp.yodao.com@YOUDAO.163.COM\", \"/disk1/eadhadoop/yarn.keytab\"); ugi.doAs(new PrivilegedAction&lt;Object&gt;() &#123; @SneakyThrows @Override public Object run() &#123; RMAdminCLI rmAdminCLI = new RMAdminCLI(conf); String cmd = \"-updateNodeResource aliyun038.analysis.aliyun.corp.yodao.com:45454 512000 64\"; String[] cmdList = cmd.split(\" \"); rmAdminCLI.run(cmdList); // YarnClient yarnClient = YarnClient.createYarnClient(); yarnClient.init(conf); yarnClient.start(); List&lt;NodeReport&gt; nodeReports = yarnClient.getNodeReports(); nodeReports.forEach(eachNode -&gt; &#123; String s = eachNode.getNodeId().toString(); nodeIdList.add(s); &#125;); // ((NodeReportPBImpl) nodeReports.get(0)).getNodeId().toString() yarnClient.stop(); return null; &#125; &#125;); return nodeIdList.toString(); &#125; 参考： Java访问kerberos认证的HDFS文件 使用hdfsclient遇到的kerberos解决方法","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://simon-ace.github.io/tags/Java/"}]},{"title":"依赖冲突","slug":"JAVA/依赖冲突","date":"2021-05-24T16:00:00.000Z","updated":"2021-05-25T11:41:43.218Z","comments":true,"path":"2021/05/25/JAVA/依赖冲突/","link":"","permalink":"https://simon-ace.github.io/2021/05/25/JAVA/依赖冲突/","excerpt":"","text":"问题描述： 引入多个 dependency 后可能会出现依赖冲突，如： 1234SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/C:/Users/admin/.m2/repository/org/slf4j/slf4j-log4j12/1.6.4/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/C:/Users/admin/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 123456789101112131415161718192021222324Caused by: org.apache.catalina.LifecycleException: Failed to start component [NonLoginAuthenticator[StandardEngine[Tomcat].StandardHost[localhost].TomcatEmbeddedContext[]]] at org.apache.catalina.util.LifecycleBase.handleSubClassException(LifecycleBase.java:440) [tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:198) [tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.core.StandardPipeline.startInternal(StandardPipeline.java:176) ~[tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5063) ~[tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.29.jar:9.0.29] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_261] at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.29.jar:9.0.29] at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[na:1.8.0_261] at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.29.jar:9.0.29] ... 29 common frames omittedCaused by: java.lang.NoSuchMethodError: javax.servlet.ServletContext.getVirtualServerName()Ljava/lang/String; at org.apache.catalina.authenticator.AuthenticatorBase.startInternal(AuthenticatorBase.java:1220) ~[tomcat-embed-core-9.0.29.jar:9.0.29] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [tomcat-embed-core-9.0.29.jar:9.0.29] ......The method&apos;s class, javax.servlet.ServletContext, is available from the following locations: jar:file:/Users/wangshuo/.gradle/caches/modules-2/files-2.1/javax.servlet/servlet-api/2.5/5959582d97d8b61f4d154ca9e495aafd16726e34/servlet-api-2.5.jar!/javax/servlet/ServletContext.class jar:file:/Users/wangshuo/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/9.0.29/207dc9ca4215853d96ed695862f9873001f02a4b/tomcat-embed-core-9.0.29.jar!/javax/servlet/ServletContext.class 解决： 看报错找到冲突的包，exclude 123456789101112131415161718192021222324dependencies &#123; compile &apos;com.youdao.analysis:json-utils:1.0.2&apos; implementation &apos;org.springframework.boot:spring-boot-starter-web&apos; compileOnly &apos;org.projectlombok:lombok&apos; annotationProcessor &apos;org.projectlombok:lombok&apos; compile &apos;com.youdao.analysis:http-utils:1.1.0&apos; compile &apos;com.youdao.analysis:outlog-shade:3.2.7-2-g0aeb335-3&apos; implementation &apos;com.googlecode.log4jdbc:log4jdbc:1.2&apos; testImplementation(&apos;org.springframework.boot:spring-boot-starter-test&apos;) &#123; exclude group: &apos;org.junit.vintage&apos;, module: &apos;junit-vintage-engine&apos; &#125; compile &apos;io.springfox:springfox-swagger2:2.8.0&apos; compile &apos;io.springfox:springfox-swagger-ui:2.8.0&apos; compile(&apos;org.apache.hadoop:hadoop-common:2.8.5&apos;)&#123; exclude module: &apos;servlet-api&apos; &#125; compile(&apos;org.apache.hadoop:hadoop-yarn-client:2.8.5&apos;)&#123; exclude module: &apos;servlet-api&apos; &#125;&#125;configurations.all &#123; exclude module: &apos;slf4j-log4j12&apos;&#125; 参考： 解决Gradle中jar冲突 Gradle 依赖&amp;解决依赖冲突","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"依赖","slug":"依赖","permalink":"https://simon-ace.github.io/tags/依赖/"}]},{"title":"Yarn内存参数调优","slug":"Hadoop/Yarn内存参数调优","date":"2021-05-20T16:00:00.000Z","updated":"2021-05-21T10:40:29.526Z","comments":true,"path":"2021/05/21/Hadoop/Yarn内存参数调优/","link":"","permalink":"https://simon-ace.github.io/2021/05/21/Hadoop/Yarn内存参数调优/","excerpt":"","text":"YARN 内存参数终极详解 Hadoop YARN中内存和CPU两种资源的调度和隔离（配置篇） YARN 内存参数终极详解","categories":[{"name":"Yarn","slug":"Yarn","permalink":"https://simon-ace.github.io/categories/Yarn/"}],"tags":[{"name":"模板","slug":"模板","permalink":"https://simon-ace.github.io/tags/模板/"}]},{"title":"RPC 调用","slug":"JAVA/RPC 调用","date":"2021-05-20T16:00:00.000Z","updated":"2021-05-21T10:41:02.481Z","comments":true,"path":"2021/05/21/JAVA/RPC 调用/","link":"","permalink":"https://simon-ace.github.io/2021/05/21/JAVA/RPC 调用/","excerpt":"","text":"【RPC】远程接口调用实例 【Java】java实现的远程调用例子 rpc原理 JAVA RPC：从上手到爱不释手 Java实现简单的RPC框架(美团面试) Hadoop RPC机制详解 对hadoop RPC的理解","categories":[{"name":"Java","slug":"Java","permalink":"https://simon-ace.github.io/categories/Java/"}],"tags":[{"name":"Java, RPC","slug":"Java-RPC","permalink":"https://simon-ace.github.io/tags/Java-RPC/"}]},{"title":"Prometheus 监控入门","slug":"其他/Prometheus 监控入门","date":"2021-04-27T16:00:00.000Z","updated":"2021-06-08T06:38:44.959Z","comments":true,"path":"2021/04/28/其他/Prometheus 监控入门/","link":"","permalink":"https://simon-ace.github.io/2021/04/28/其他/Prometheus 监控入门/","excerpt":"","text":"一、服务暴露接口需要暴露出 metrics 接口 二、Prometheus 对接在 prometheus 中配置服务的 ip:port 三、PromQL3.1 查询时间序列匹配模式 完全匹配 通过使用label=value可以选择那些标签满足表达式定义的时间序列 反之使用label!=value则可以根据标签匹配排除时间序列 正则匹配 使用label=~regx表示选择那些标签符合正则表达式定义的时间序列 反之使用label!~regx进行排除 3.2 范围查询时间范围通过时间范围选择器[]进行定义。例如，通过以下表达式可以选择最近5分钟内的所有样本数据： 1http_requests_total&#123;&#125;[5m] 3.3 时间位移想查询，5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据呢? 这个时候我们就可以使用位移操作，位移操作的关键字为offset。 可以使用offset时间位移操作： 12http_request_total&#123;&#125; offset 5mhttp_request_total&#123;&#125;[1d] offset 1d 3.4 聚合对同一特征多个label的序列，进行聚合操作 12345678# 查询系统所有http请求的总量sum(http_request_total)# 按照mode计算主机CPU的平均使用时间avg(node_cpu) by (mode)# 按照主机查询各个主机的CPU使用率sum(sum(irate(node_cpu&#123;mode!=&apos;idle&apos;&#125;[5m])) / sum(irate(node_cpu[5m]))) by (instance) 例子： gauge 类型数据，可以用xxx_over_time()函数来做一段时间的数据聚合操作 quantile_over_time(0.3, yarn_label_used_capacity[1d]) prometheus 函数语法介绍 一篇文章带你理解和使用Prometheus的指标 ★Prometheus Book 三、Grafana 展示 &amp; 报警 grafana配置仪表盘 Prometheus运维七 详解可视化Grafana工具","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"prometheus","slug":"prometheus","permalink":"https://simon-ace.github.io/tags/prometheus/"}]},{"title":"Linux扫描磁盘","slug":"Linux/常用命令/Linux扫描磁盘","date":"2021-04-11T16:00:00.000Z","updated":"2021-04-12T13:47:18.555Z","comments":true,"path":"2021/04/12/Linux/常用命令/Linux扫描磁盘/","link":"","permalink":"https://simon-ace.github.io/2021/04/12/Linux/常用命令/Linux扫描磁盘/","excerpt":"","text":"https://www.jianshu.com/p/c75ce4964062","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Flink 入门到实战","slug":"Hadoop/Flink 入门到实战","date":"2021-03-16T16:00:00.000Z","updated":"2021-03-20T03:31:25.664Z","comments":true,"path":"2021/03/17/Hadoop/Flink 入门到实战/","link":"","permalink":"https://simon-ace.github.io/2021/03/17/Hadoop/Flink 入门到实战/","excerpt":"","text":"一、基础概念二、Flink 流处理 API2.1 Environment获取执行环境：本地？集群？ 2.2 Source从不同的数据源获取数据 集合 文件 Kafka 自定义 Source 2.3 Transform 转换算子2.3.1 基本转换算子 map flatMap filter 2.3.2 聚合算子DataStream 需要现分组才能做聚合操作 先 keyBy 得到 KeyedStream，再调用 reduce、sum 方法 keyBy 根据 key 进行分组 Rolling Aggregation sum()、min()、max()、minBy()、maxBy() 滚动聚合，来一个数据更新一次结果 reduce 自定义聚合 2.3.3 多流转换算子 Split 和 Select（已废弃） split 将 DataStream 中的数据拆到一个 SplitStream 中的两部分 Select 将 SplitStream 中的数据，提取成多个 DataStream Connect 和 CoMap 与 Split、Select 相对，将两条Stream合并为一个Stream connect 将两个 stream 合并成 ConnectedStreams 中的两部分 CoMap 将ConnectedStreams 中的两部分，合并为一个 DataSteam Union 对两个或者两个以上的DataStream进行Union操作 与 Connect 区别 Connect 的数据类型可以不同，Connect 只能合并两个流 Union可以合并多条流，Union的数据结构必须是一样的 2.3.4 算子转换关系 2.4 支持的数据类型2.5 实现 UDF 函数2.6 数据重分区2.7 Sink2.7.1 Kafka Sink pom 依赖 12345678910111213141516171819202122232425262728&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.shuofxz&lt;/groupId&gt; &lt;artifactId&gt;FlinkTutorial&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; Java 代码 1234567891011121314151617181920212223242526272829303132333435363738394041package com.shuofxz.sink;import com.shuofxz.beans.SensorReading;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;import java.util.Properties;public class kafkaSink &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 从文件读取数据// DataStream&lt;String&gt; inputStream = env.addSource( new FlinkKafkaConsumer011&lt;String&gt;(\"sensor\", new SimpleStringSchema(), properties)); Properties properties = new Properties(); properties.setProperty(\"bootstrap.servers\", \"hadoop102:9092\"); properties.setProperty(\"group.id\", \"consumer-group\"); properties.setProperty(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.setProperty(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.setProperty(\"auto.offset.reset\", \"latest\"); // 从kafka读取数据 DataStream&lt;String&gt; inputStream = env.addSource( new FlinkKafkaConsumer011&lt;String&gt;(\"sensor\", new SimpleStringSchema(), properties)); // 转换成SensorReading类型 DataStream&lt;String&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])).toString(); &#125;); dataStream.addSink( new FlinkKafkaProducer011&lt;String&gt;(\"hadoop102:9092\", \"sinktest\", new SimpleStringSchema())); env.execute(); &#125;&#125; 启动 zookeeper、kafka 服务 启动 kafka 生产者 &amp; 消费者 新建kafka生产者console 1$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sensor 新建kafka消费者console 1$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sinktest 运行Flink程序，在kafka生产者console输入数据，查看kafka消费者console的输出结果 输入(kafka生产者console) 12&gt;sensor_1,1547718199,35.8&gt;sensor_6,1547718201,15.4 输出(kafka消费者console) 12SensorReading&#123;id='sensor_1', timestamp=1547718199, temperature=35.8&#125;SensorReading&#123;id='sensor_6', timestamp=1547718201, temperature=15.4&#125; 这里Flink的作用相当于pipeline了。 2.7.2 Redis Sink2.7.3 ES Sink2.7.4 JDBC Sink三、Flink Window3.1 Window 概述将无限数据流切割为有限数据块的操作。Window将一个无限的stream拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作。 3.1.1 类型 时间窗口（Time Window） 滚动时间窗口 滑动时间窗口 会话窗口 计数窗口（Count Window） 滚动计数窗口 滑动计数窗口 TimeWindow：按照时间生成Window CountWindow：按照指定的数据条数生成一个Window，与时间无关 滚动窗口(Tumbling Windows) 依据固定的窗口长度对数据进行切分 时间对齐，窗口长度固定，没有重叠 滑动窗口(Sliding Windows) 可以按照固定的长度向后滑动固定的距离 滑动窗口由固定的窗口长度和滑动间隔组成 可以有重叠(是否重叠和滑动距离有关系) 滑动窗口是固定窗口的更广义的一种形式，滚动窗口可以看做是滑动窗口的一种特殊情况（即窗口大小和滑动间隔相等） 会话窗口(Session Windows) 由一系列事件组合一个指定时间长度的timeout间隙组成，也就是一段时间没有接收到新数据就会生成新的窗口 特点：时间无对齐 3.2 Window API四、时间语义 &amp; Watermark4.1 Flink中的时间语义 Event Time：事件创建时间 Ingestion Time：数据进入Flink的时间 Processing Time：执行操作算子的本地系统时间，与机器相关 Event Time是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。 4.2 Watermark4.2.1 概念及作用出现原因：用于处理网络延迟、分布式延迟等造成的数据乱序问题 工作方式： 伪装成一个普通数据插入到数据流中，基本只包含时间信息 Flink 读到 watermark 证明该 watermark 中时间点前的数据已全部到达，可以关闭对应的 bucket 进行处理 为了能够尽量包容延迟数据，会将 watermark 的时间比实际收到的数据时间慢一些（如2-3s），这样就可以把几秒内的延迟数据包容进来了 猜测：如果延迟时间过长，超过了 watermark 设置的时间，就会被丢弃 五、Flink 状态管理状态：相当于是之前task留下来的数据，用于和新的数据流数据进行计算用的？","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://simon-ace.github.io/tags/Flink/"}]},{"title":"matplotlib 图例中文乱码","slug":"Python/matplotlib 图例中文乱码","date":"2021-03-04T16:00:00.000Z","updated":"2021-03-05T06:25:33.249Z","comments":true,"path":"2021/03/05/Python/matplotlib 图例中文乱码/","link":"","permalink":"https://simon-ace.github.io/2021/03/05/Python/matplotlib 图例中文乱码/","excerpt":"","text":"1、下载中文字体（黑体，看准系统版本） https://www.fontpalace.com/font-details/SimHei/ 2、解压之后在系统当中安装好，我的是Mac，打开字体册就可以安装了，Windows的在网上搜一下吧 3、找到matplotlib字体文件夹，例如：matplotlib/mpl-data/fonts/ttf，将SimHei.ttf拷贝到ttf文件夹下面 123456# 执行下面的找到文件夹&gt;&gt;&gt; import matplotlib&gt;&gt;&gt; print(matplotlib.matplotlib_fname())/Users/xxx/Library/Python/3.8/lib/python/site-packages/matplotlib/mpl-data/matplotlibrc# 即前缀就是 /Users/xxx/Library/Python/3.8/lib/python/site-packages/matplotlib 4、修改配置文件matplotlibrc （上一步看到的位置），修改下面三项配置 12345font.family : sans-serif font.sans-serif : SimHei, Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif axes.unicode_minus : False #作用就是解决负号'-'显示为方块的问题 5、重新加载字体，在Python中运行如下代码即可： 12from matplotlib.font_manager import _rebuild_rebuild() #reload一下","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"python","slug":"python","permalink":"https://simon-ace.github.io/tags/python/"}]},{"title":"Vue 工程创建","slug":"vue/Vue 工程创建","date":"2021-02-28T16:00:00.000Z","updated":"2021-03-10T02:59:07.926Z","comments":true,"path":"2021/03/01/vue/Vue 工程创建/","link":"","permalink":"https://simon-ace.github.io/2021/03/01/vue/Vue 工程创建/","excerpt":"","text":"一、初始化项目1.1 Vue UI 创建 命令行输入，会跳出一个浏览器页面： 1vue ui 选择一个存放路径 输入项目名称 进入「创建新项目」界面 如果之前有保存配置，这里可以选择之前的 没有保存过，就选手动 进入「功能」界面 添加一些常用的功能 一般会需要：Babel、Router、Vuex、Linter/Formatter、使用配置文件 进入「配置」页面 Linter/formatter 选择 standard 最后可以把配置保存 生成项目大概需要5-10min 二、安装插件 &amp; 依赖2.1 插件 推荐安装的插件： vue-cli-plugin-element 想要生成的包小，就用「import on demand」；如果想开发省事，就「Fully import」 2.2 依赖 运行依赖 axios 开发依赖 less-loader less 三、生产优化 移除 console 输出 查看打包报告 能够看到各种环境的占比； 或者使用命令行 1vue-cli-service build --report 分开 prod 和 dev 的配置文件 使用 CDN 加载 减小包的大小 路由懒加载 基于Vue、ElementUI的换肤解决方案","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://simon-ace.github.io/tags/vue/"}]},{"title":"注解和反射","slug":"JAVA/注解和反射","date":"2021-02-22T16:00:00.000Z","updated":"2021-02-26T04:35:18.507Z","comments":true,"path":"2021/02/23/JAVA/注解和反射/","link":"","permalink":"https://simon-ace.github.io/2021/02/23/JAVA/注解和反射/","excerpt":"","text":"一、注解学前疑惑： 干啥用的？ 类似于注释，给编译器看的？看的什么东西呢。加载的时候会有什么不同？ 虽然语法上有点像 python 的装饰器，但是作用似乎不一样 1.1 什么是注解 可以对程序作出解释（类似于注释，是给编译器看的） 可以被其他程序（如编译器等）读取 1.2 元注解用于注解其他注解的注解，即对注解进行一些说明。 共有4个 —— @Target, @Retention, @Document, @Inherited @Target 描述注解的使用范围，作用于方法、类等 @Retention 表示注解在什么地方还有效 SOURCE &lt; CLASS &lt; RUNTIME 默认用 RUNTIME @Document 表示是否将注解生成在 javadoc 中 @Inherited 说明子类可以继承父类注解 1.3 自定义注解123456@Retention(RetentionPolicy.RUNTIME)@Target(value = ElementType.METHOD)@interface MyAnnotation &#123; String name() default \"\"; int age() default 0;&#125; 使用 @interface 声明注解 其中的每个方法，实际为一个配置参数 如果只有一个参数成员，一般参数名为 value 默认值用 default 二、反射机制学前疑惑： 什么是反射，有什么作用？ 为什么不直接new出来对象调用呢？ 2.1 基础概念2.1.1 相关知识静态语言 &amp; 动态语言： 动态语言 【在运行时检查】 在运行时可以改变结构的语言，即变量、函数等可以在运行时改变类型（JS、Python 等） 静态语言 【在编译期检查】 相对应的，不能改变类型，声明了类型就不许改变（Java、C++ 等） 强类型 &amp; 弱类型： 强类型 【不会隐式做语言类型转换】 弱类型 【做隐式类型转换】 2.1.2 反射概念 反射机制允许程序在执行期间获取类本身的各种定义，如属性、方法等 与new对象的方式相反： new 对象：import 包 -&gt; new 对象 -&gt; 取得实例化对象 反射：实例化对象 -&gt; getCalss() 方法 -&gt; 得到完整的”包类“名称 由于和new对象的过程相反，所以有「反射」的叫法 2.1.3 反射的作用 在运行时判断对象所属的类 在运行时构造对象 在运行时调用对象的成员变量和方法 。。。 2.1.4 反射优点和缺点优点： 动态创建对象和编译 缺点： 性能低 反射类似于解释操作，告诉JVM我需要什么，然后JVM来做，会比我们直接操作要慢 2.2 反射2.2.1 获取反射对象 一个类在内存中只有一个Class对象 一个类被加载后，整个类结构都会封装在 Class 对象中 1Class c1 = Class.forName(\"com.shuofxz.reflection.User\"); 2.2.2 Class 是什么 Class 本身也是一个类 Class 对象只能由系统（JVM）创建 特征 一个类只会在 JVM 中有一个 Class 实例 每个类的实例都知道自己是由哪个 Class 实例生成 用处 通过 Class 实例可以获得一个类中所有被加载的结构 通过 Class 可以动态加载运行类（即反射） 获取 Class 类的实例 12345678910// 1) 从类获得Class c1 = Person.class;// 2) 从实例获得Class c2 = person.getClass();// 3) 通过 包名+类名 获得Class c3 = Class.forName(\"com.shuofxz.Person\");// 4) ClassLoader 类加载 与 ClassLoader 详细请看 JVM 篇 方法区：从 .class 文件中加载类数据到方法区（每个类数据包括静态变量、静态方法、常量池、代码等） 堆：以方法区类数据作为模板，生成出类对象；再由「类对象」生成出「对象」 栈：程序执行，按照代码赋值对象，执行方法 三、注解与反射 注解可以通过反射机制起作用 获取到一个类实例后，通过反射的方式拿到所写的注解信息 再通过类本身信息 + 注解的信息 完成一些操作 如操作数据库的时候，会给类属性对应一个数据库列的名字，就可以用注解来对应 相当于一个语法糖？ 将一些本该由更多代码实现的对应信息，通过注解简单的方式实现了 作用 简化代码 结偶 比方原来需要通过继承或者实现方式获得的属性和方法，通过注解方式结偶","categories":[{"name":"Java","slug":"Java","permalink":"https://simon-ace.github.io/categories/Java/"}],"tags":[{"name":"Java, 注解, 反射","slug":"Java-注解-反射","permalink":"https://simon-ace.github.io/tags/Java-注解-反射/"}]},{"title":"线程池","slug":"JAVA/线程池","date":"2021-02-22T16:00:00.000Z","updated":"2021-05-18T12:41:05.803Z","comments":true,"path":"2021/02/23/JAVA/线程池/","link":"","permalink":"https://simon-ace.github.io/2021/02/23/JAVA/线程池/","excerpt":"","text":"Java线程池实现原理及其在美团业务中的实践使用线程池 廖雪峰","categories":[{"name":"Java","slug":"Java","permalink":"https://simon-ace.github.io/categories/Java/"}],"tags":[{"name":"Java, 线程","slug":"Java-线程","permalink":"https://simon-ace.github.io/tags/Java-线程/"}]},{"title":"IDEA 常用快捷键","slug":"IDEA 常用快捷键","date":"2021-02-07T16:00:00.000Z","updated":"2021-02-08T07:59:04.082Z","comments":true,"path":"2021/02/08/IDEA 常用快捷键/","link":"","permalink":"https://simon-ace.github.io/2021/02/08/IDEA 常用快捷键/","excerpt":"","text":"清除无效 import 12ctrl + alt + octrl + option + o","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"idea, 快捷键","slug":"idea-快捷键","permalink":"https://simon-ace.github.io/tags/idea-快捷键/"}]},{"title":"git 使用看这一篇就够了","slug":"Linux/git 使用看这一篇就够了","date":"2021-02-07T16:00:00.000Z","updated":"2021-04-16T09:49:03.847Z","comments":true,"path":"2021/02/08/Linux/git 使用看这一篇就够了/","link":"","permalink":"https://simon-ace.github.io/2021/02/08/Linux/git 使用看这一篇就够了/","excerpt":"","text":"一、基础概念区域 工作区 暂存区 版本库 git 对象 二、基本操作2.1 初始化仓库1git init 三、进阶操作 同步另一个分支的修改 在 A、B 分支之前处于一个位置，之后A修改了，变成A’，想把B同步到A’的位置 1234# 先切换到 B 分支git checkout B# 合并 A1 的修改git merge A 删除某次 commit 3.1 git reset git reset ：回滚到某次提交。 git reset --soft：此次提交之后的修改会被退回到暂存区。 git reset --hard：此次提交之后的修改不做任何保留，git status 查看工作区是没有记录的。 123git log // 查询要回滚的 commit_idgit reset --hard commit_id // HEAD 就会指向此次的提交记录git push origin HEAD --force // 强制推送到远端 3.2 rebase 修改之前的commit信息（只允许在推到远程之前做） 1git rebase -i &lt;要改的commit 之前的一次commit id&gt; 【原理】 在要做变基的节点上，创建出了新的一条支，然后在新的这个支上作修改。","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"git","slug":"git","permalink":"https://simon-ace.github.io/tags/git/"}]},{"title":"SpringBoot 入门","slug":"Spring/SpringBoot入门","date":"2021-02-04T16:00:00.000Z","updated":"2021-02-23T03:25:52.608Z","comments":true,"path":"2021/02/05/Spring/SpringBoot入门/","link":"","permalink":"https://simon-ace.github.io/2021/02/05/Spring/SpringBoot入门/","excerpt":"","text":"一、基础入门1.1 Spring 与 SpringBootSpringBoot 出现原因：整合各种 Spring 框架，简化配置，快速上手开发。【约定大于配置】 1.2 SpringBoot 第一个程序1.2.1 maven 设置pom.xml中添加 123456789101112&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.2.2 主类MainApplication.java 123456@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class, args); &#125;&#125; HelloController.java 12345678@RestControllerpublic class HelloController &#123; @RequestMapping(\"/hello\") public String hello() &#123; return \"Hello springboot !\"; &#125;&#125; 启动即可 完全参考官方文档即可，写的很清楚 官方文档 1.3 了解自动配置原理（重点） SpringBoot 底层整合了 Sping, SpringMVC等 二、核心功能2.1 配置文件2.2 Web 开发2.2.1 请求参数处理 REST 风格 GET-查询 DELETE-删除 PUT-修改 POST-保存 请求方法注解 1234// 原始写法@RequestMapping(value = \"/hello\", method = RequestMethod.POST)// 简略写法@PostMapping(\"/hello\") 2.2.2 响应参数处理","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://simon-ace.github.io/tags/SpringBoot/"}]},{"title":"Presto","slug":"Hadoop/Presto","date":"2021-02-03T16:00:00.000Z","updated":"2021-02-04T02:36:41.954Z","comments":true,"path":"2021/02/04/Hadoop/Presto/","link":"","permalink":"https://simon-ace.github.io/2021/02/04/Hadoop/Presto/","excerpt":"","text":"Presto一、 简介1.1 概念 是什么：即席查询框架 为什么出现： hive等大数据仓库查数据太慢 原来的数据库（仓库）无法做到跨源联合查询 presto特征： 基于内存 数据量支持 GB~PB 查询，查询在几秒到几分钟 可联合多个数据源进行join查询 只是一个查询引擎，不存储数据 1.2 架构 1.3 优缺点优点： 基于内存，减少落盘次数，计算快 能够连接多个数据源，进行跨数据源join 缺点： 因为是放在内存计算，当连表查询时，会产生一堆的临时表（落盘？），反而比hive还慢","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Presto, 教程","slug":"Presto-教程","permalink":"https://simon-ace.github.io/tags/Presto-教程/"}]},{"title":"raw.githubusercontent.com无法连接","slug":"raw.githubusercontent.com无法连接","date":"2021-01-23T16:00:00.000Z","updated":"2021-01-23T17:00:15.577Z","comments":true,"path":"2021/01/24/raw.githubusercontent.com无法连接/","link":"","permalink":"https://simon-ace.github.io/2021/01/24/raw.githubusercontent.com无法连接/","excerpt":"由于 DNS 污染导致 raw.githubusercontent.com 无法正常访问，可通过在 hosts 中添加下面一行解决： 1199.232.96.133 raw.githubusercontent.com","text":"由于 DNS 污染导致 raw.githubusercontent.com 无法正常访问，可通过在 hosts 中添加下面一行解决： 1199.232.96.133 raw.githubusercontent.com 一、解决方法查询真实IP 通过IPAddress.com首页，输入raw.githubusercontent.com查询到真实IP地址 如查询到的ip为：199.232.96.133 修改hosts 1sudo vi /etc/hosts 添加以下内容保存 1199.232.96.133 raw.githubusercontent.com 二、其他代理可参考 https://ghproxy.com 更详细使用方法 主要用于 clone github 的项目","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"电商数仓 V2.0 （03 电商数据仓库系统）","slug":"电商数仓 V2.0/电商数仓 V2.0 （03 电商数据仓库系统）","date":"2021-01-23T16:00:00.000Z","updated":"2021-02-03T01:31:59.926Z","comments":true,"path":"2021/01/24/电商数仓 V2.0/电商数仓 V2.0 （03 电商数据仓库系统）/","link":"","permalink":"https://simon-ace.github.io/2021/01/24/电商数仓 V2.0/电商数仓 V2.0 （03 电商数据仓库系统）/","excerpt":"","text":"电商数仓 V2.0 （03 电商数据仓库系统）一、数仓分层1.1 为什么要分层 把复杂问题简单化 将复杂的任务分解成多层来完成，每一层只处理简单的任务，方便定位问题 减少重复开发 规范数据分层，通过的中间层数据，能够减少极大的重复计算，增加一次计算结果的复用性 隔离原始数据 不论是数据的异常还是数据的敏感性，使真实数据与统计数据解耦开 常见分层方式： 1.2 数据集市与数据仓库数据仓库：企业级别的，包含企业内所有的数据 数据集市：可以理解为部门级别的，归属于数据仓库 二、数仓理论简介2.1 范式理论定义 范式可以理解为设计一张数据表的表结构，符合的标准级别，规范和要求 优点 减少数据冗余 保持数据一致性（只改其中一张表就可以了） 缺点 难以应对大数据量的计算，因为会频繁涉及表之间的 join 操作 2.2 关系建模和维度建模数据处理主要分类： 联机事务处理 OLTP（on-line transaction processing） 基于事务型的数据增删改查，如银行交易 联机分析处理 OLAP（On-Line Analytical Processing） 基于分析型的数据处理，如计算pv uv 对比属性 OLTP（如 MySQL） OLAP（如 Hive） 读特性 每次查询只返回少量记录 对大量记录进行汇总 写特性 随机、低延时写入用户的输入 批量导入 使用场景 用户，Java EE项目 内部分析师，为决策提供支持 数据表征 最新数据状态 随时间变化的历史状态 数据规模 GB TB到PB 2.2.1 关系建模 基本严格遵循三范式 表较为零碎 数据冗余低 2.2.2 维度建模 主要应用于OLAP系统中 通常以某一个事实表为中心进行表的组织，主要面向业务 存在数据的冗余，但是能方便的得到数据。 在大规模数据，跨表分析统计查询过程中，不需要多表关联，提升效率 把相关各种表整理成两种：事实表和维度表 分类 星型模型 在一个事实表周围只会围绕一圈维度表 冗余可能会多一点，但是减少了 join 能提升计算效率 雪花模型 相对于星型模型，事实表周围会有多层维度表 会更灵活一些 星座模型 在前两种上的扩展，如果多于一个事实表，就是星座模型 2.3 维度表和事实表2.3.1 维度表定义： 一般是对事实的描述信息。每一张维表对应现实世界中的一个对象或者概念。 例如：用户、商品、日期、地区等 特征： 维表的范围很宽（具有多个属性、列比较多） 跟事实表相比，行数相对较小：通常&lt; 10万条 内容相对固定：编码表 2.3.2 事实表定义： 描述一个业务过程，如下单、支付、退款、评价等。通常包含着可统计的列，如金额、数量等。 特征： 非常的大 内容相对的窄：列数较少 经常发生变化，每天会新增加很多 事实表分类： 1）事务型事实表 以每个事务或事件为单位，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。一旦事务被提交，事实表数据被插入，数据就不再进行更改，其更新方式为增量更新。 2）周期型快照事实表 周期型快照事实表中不会保留所有数据，只保留固定时间间隔的数据，例如每天或者每月的销售额。对中间过程不敏感，只关心到一个时间点时的状态。 3）累积型快照事实表 累计快照事实表用于跟踪业务事实的变化。例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。 2.4 数据仓库建模2.4.1 ODS层 保存着最原始的数据，起到数据备份的作用 采用数据压缩，减少磁盘占用（压缩到 1/10 ~ 1/20） 创建分区表，避免之后全表扫描 原始表之间关系： 绿色：维度表 深色：事实表 白色：会进行维度退化的表 2.4.2 DWD层1、在这里使用维度建模，一般采用星型模型，展现出来为星座模型 2、维度建模大致分为以下四个步骤： 选择业务过程 -&gt; 声明粒度 -&gt; 确认维度 -&gt; 确认事实 选择业务过程 挑选感兴趣的业务线，一条业务线对应一张事实表 声明最小粒度 无论统计啥都能从这个粒度进行计算 确认维度 谁、何处、何时 维度表：会根据需求进行维度退化 确认事实 即确定「度量值」，个数、次数、件数等信息 3、维度建模到这里已完毕 4、DWD 层以业务过程为驱动，后面DWS、DWT则以需求为驱动 时间 用户 地区 商品 优惠券 活动 编码 度量值 订单 √ √ √ √ 件数/金额 订单详情 √ √ √ √ 件数/金额 支付 √ √ √ 金额 加购 √ √ √ 件数/金额 收藏 √ √ √ 个数 评价 √ √ √ 个数 退款 √ √ √ 件数/金额 优惠券领用 √ √ √ 个数 2.4.3 DWS层主要作用：将维度表和关联的事实表进行组合（join），每个维度表生成一个大宽表，避免后续重复join计算 粒度：统计各主题对象当天的行为 每个表包含哪些字段：以维度表为基准，关联相关事实表的度量值（如下图） 以用户表为例： 2.4.4 DWT层作用：统计事情从第一次发生至今的累积度量值 例： 2.4.5 ADS层根据实际业务需要在前面几张表中进行统计，如 pv，uv，新增，留存等。 三、ODS 层搭建 保持数据原貌不做任何修改，起到备份数据的作用（即直接以一串string的原始格式存进来，不作任何解析） 数据采用LZO压缩，减少磁盘存储空间。100G数据可以压缩到10G以内。 创建分区表，防止后续的全表扫描，在企业开发中大量使用分区表。 创建外部表。在企业开发中，除了自己用的临时表，创建内部表外，绝大多数场景都是创建外部表。 3.1 创建数据库12&gt; create database gmall;&gt; use gmall; 3.2 用户行为数据3.2.1 启动日志表 创建表 12345678USE gmall;DROP TABLE IF EXISTS ods_start_log;CREATE EXTERNAL TABLE ods_start_log (`line` string)PARTITIONED BY (`dt` string)STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/warehouse/gmall/ods/ods_start_log'; 加载数据 load 操作是剪切操作，相当于从 hdfs 的一个路径，移动到另一个路径 12load data inpath '/origin_data/gmall/log/topic_start/2020-03-10' into table gmall.ods_start_log partition(dt='2020-03-10');-- 注意：时间格式都配置成YYYY-MM-DD格式，这是Hive默认支持的时间格式 查看数据 1select * from ods_start_log where dt='2020-03-10' limit 2; 创建 lzo 索引 1hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=2020-03-10 3.2.2 事件日志表 创建表 12345678USE gmall;DROP TABLE IF EXISTS ods_event_log;CREATE EXTERNAL TABLE ods_event_log(`line` string)PARTITIONED BY (`dt` string)STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/warehouse/gmall/ods/ods_event_log'; 加载数据 1load data inpath '/origin_data/gmall/log/topic_event/2020-03-10' into table gmall.ods_event_log partition(dt='2020-03-10'); 查看数据 1select * from ods_event_log where dt=\"2020-03-10\" limit 2; 创建索引 1hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2020-03-10 3.2.3 数据导入脚本hdfs_to_ods_log.sh 12345678910111213141516171819202122232425#!/bin/bash# 定义变量方便修改APP=gmallHIVE=/opt/module/hive-2.3.0-bin/bin/hive# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n \"$1\" ] ;then do_date=$1else do_date=`date -d \"-1 day\" +%F`fi echo \"=== 日志日期为 $do_date ===\"sql=\"load data inpath '/origin_data/gmall/log/topic_start/$do_date' overwrite into table $&#123;APP&#125;.ods_start_log partition(dt='$do_date');load data inpath '/origin_data/gmall/log/topic_event/$do_date' overwrite into table $&#123;APP&#125;.ods_event_log partition(dt='$do_date');\"$HIVE -e \"$sql\"hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=$do_datehadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=$do_date 3.3 业务数据从 MySQL 导入 HDFS 的数据 3.3.1 创建表一堆表，以 订单表 为例 1234567891011121314151617181920212223-- 增量及更新hive (gmall)&gt;use gmall;drop table if exists ods_order_info;create external table ods_order_info ( `id` string COMMENT '订单号', `final_total_amount` decimal(10,2) COMMENT '订单金额', `order_status` string COMMENT '订单状态', `user_id` string COMMENT '用户id', `out_trade_no` string COMMENT '支付流水号', `create_time` string COMMENT '创建时间', `operate_time` string COMMENT '操作时间', `province_id` string COMMENT '省份ID', `benefit_reduce_amount` decimal(10,2) COMMENT '优惠金额', `original_total_amount` decimal(10,2) COMMENT '原价金额', `feight_fee` decimal(10,2) COMMENT '运费') COMMENT '订单表'PARTITIONED BY (`dt` string)row format delimited fields terminated by '\\t'STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'location '/warehouse/gmall/ods/ods_order_info/'; 其他表的创建略。 3.3.2 数据导入脚本hdfs_to_ods_db.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!/bin/bashAPP=gmallHIVE=/opt/module/hive-2.3.0-bin/bin/hive# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n \"$2\" ] ;then do_date=$2else do_date=`date -d \"-1 day\" +%F`fisql1=\" load data inpath '/origin_data/$APP/db/order_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_order_info partition(dt='$do_date');load data inpath '/origin_data/$APP/db/order_detail/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_order_detail partition(dt='$do_date');load data inpath '/origin_data/$APP/db/sku_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_sku_info partition(dt='$do_date');load data inpath '/origin_data/$APP/db/user_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_user_info partition(dt='$do_date');load data inpath '/origin_data/$APP/db/payment_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_payment_info partition(dt='$do_date');load data inpath '/origin_data/$APP/db/base_category1/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_category1 partition(dt='$do_date');load data inpath '/origin_data/$APP/db/base_category2/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_category2 partition(dt='$do_date');load data inpath '/origin_data/$APP/db/base_category3/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_category3 partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/base_trademark/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_trademark partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/activity_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_activity_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/activity_order/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_activity_order partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/cart_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_cart_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/comment_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_comment_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/coupon_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_coupon_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/coupon_use/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_coupon_use partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/favor_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_favor_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/order_refund_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_order_refund_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/order_status_log/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_order_status_log partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/spu_info/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_spu_info partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/activity_rule/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_activity_rule partition(dt='$do_date'); load data inpath '/origin_data/$APP/db/base_dic/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_dic partition(dt='$do_date'); \"sql2=\" load data inpath '/origin_data/$APP/db/base_province/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_province;load data inpath '/origin_data/$APP/db/base_region/$do_date' OVERWRITE into table $&#123;APP&#125;.ods_base_region;\"case $1 in\"first\")&#123; $HIVE -e \"$sql1\" $HIVE -e \"$sql2\"&#125;;;\"all\")&#123; $HIVE -e \"$sql1\"&#125;;;esac 执行： 12hdfs_to_ods_db.sh first 2020-03-10hdfs_to_ods_db.sh all 2020-03-11 测试： 1select * from ods_order_detail where dt='2020-03-11'; 四、DWD 层搭建4.1 用户行为数据 将 JSON 数据解析为标准的列数据 创建 UDF 函数（解析json列） 创建 UDTF 函数（一进多出，将一行数据拆分成多行） 4.2 业务数据 进行维度建模 join多张表，形成一个大宽表 拉链表 用于处理缓慢变化的数据 如果每日全量记录，太浪费 拉链表，有变化的行才会记录 两个特殊的列 start_time, end_time 分别记录这条的生效和结束时间，有变化时就会产生一个新的记录行 五、DWS 层搭建","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"https://simon-ace.github.io/tags/数仓/"}]},{"title":"CentOS6 yum 404","slug":"Linux/CentOS6 yum 404","date":"2021-01-20T16:00:00.000Z","updated":"2021-01-21T12:44:40.258Z","comments":true,"path":"2021/01/21/Linux/CentOS6 yum 404/","link":"","permalink":"https://simon-ace.github.io/2021/01/21/Linux/CentOS6 yum 404/","excerpt":"","text":"一、问题出现原因突然发现 yum不可用了，错误信息如下： Determining fastest mirrorsYumRepo Error: All mirror URLs are not using ftp, http[s] or file.Eg. Invalid release/repo/arch combination/removing mirrorlist with no valid mirrors: /var/cache/yum/x86_64/6/base/mirrorlist.txt错误：Cannot find a valid baseurl for repo: base 前面怀疑是服务器的网络问题，经排查网络无异常。拿yum源中的地址确认问题，发现404了，地址已经发发生了改变，原因是CentOS 6已经随着2020年11月的结束进入了EOL（Reaches End of Life），官方便在12月2日正式将CentOS 6相关的软件源移出了官方源，随之而来逐级镜像也会陆续将其删除。 二、解决问题备份文件 12# by rootcp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 替换CentOS-Base.repo文件内容 下面提供了官方Vault源和阿里云Vault镜像，选择其一即可，国内建议使用阿里云Vault镜像，速度会更快。vi /etc/yum.repos.d/CentOS-Base.repo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#阿里云Vault镜像，本例使用的6.10版本，注意修改为你当前的操作系统版本号[base]name=CentOS-6.10 - Base - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/os/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #released updates [updates]name=CentOS-6.10 - Updates - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/updates/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #additional packages that may be useful[extras]name=CentOS-6.10 - Extras - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/extras/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #additional packages that extend functionality of existing packages[centosplus]name=CentOS-6.10 - Plus - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6 #contrib - packages by Centos Users[contrib]name=CentOS-6.10 - Contrib - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos-vault/6.10/contrib/$basearch/gpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos-vault/RPM-GPG-KEY-CentOS-6--官方Vault源，本例使用的6.10版本，注意修改为你当前的操作系统版本号[base]name=CentOS-6.10 - Base - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/os/$basearch/gpgcheck=1gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #released updates [updates]name=CentOS-6.10 - Updates - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/updates/$basearch/gpgcheck=1gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #additional packages that may be useful[extras]name=CentOS-6.10 - Extras - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/extras/$basearch/gpgcheck=1gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #additional packages that extend functionality of existing packages[centosplus]name=CentOS-6.10 - Plus - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/centosplus/$basearch/gpgcheck=1enabled=0gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 #contrib - packages by Centos Users[contrib]name=CentOS-6.10 - Contrib - vault.centos.orgfailovermethod=prioritybaseurl=http://vault.centos.org/6.10/contrib/$basearch/gpgcheck=1enabled=0gpgkey=http://vault.centos.org/RPM-GPG-KEY-CentOS-6 清除YUM缓存 1yum clean all 重新构建缓存 1yum makecache","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"yum","slug":"yum","permalink":"https://simon-ace.github.io/tags/yum/"}]},{"title":"电商数仓 V2.0 （02 业务数据采集模块）","slug":"电商数仓 V2.0/电商数仓 V2.0 （02 业务数据采集模块）","date":"2021-01-18T16:00:00.000Z","updated":"2021-02-01T10:34:18.528Z","comments":true,"path":"2021/01/19/电商数仓 V2.0/电商数仓 V2.0 （02 业务数据采集模块）/","link":"","permalink":"https://simon-ace.github.io/2021/01/19/电商数仓 V2.0/电商数仓 V2.0 （02 业务数据采集模块）/","excerpt":"","text":"电商数仓 V2.0 （02 业务数据采集模块）一、电商业务简介二、业务数据采集模块2.1 MySQL装一台机器就行 hadoop102 2.2 Sqoop 修改配置sqoop-env.sh 12345678# 目前用于将数据从 MySQL 导入 HDFS 配置下面这俩就可以export HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2# 导入 Hive 要配置 Hive_HOME；导入 HBase 要配置 HBASE_HOME 和 ZOOCFGDIR#export HBASE_HOME=#export HIVE_HOME=#export ZOOCFGDIR= 拷贝 mysql jar 包 1cp mysql-connector-java-5.1.27-bin.jar $SQOOP_HOME/lib 验证安装情况 123456789101112131415161718$ bin/sqoop helpAvailable commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version informationSee 'sqoop help COMMAND' for information on a specific command. 测试连接 mysql 12bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 123456# 能出现表名就是正确的了 2.3 业务数据生成 通过建表语句导入数据 1gmall2020-03-16.sql java 生成业务数据 拷贝 application.properties 和 gmall-mock-db-2020-03-16-SNAPSHOT.jar 修改 application.properties 12345# 先生成 2020-03-10 clear=1（清空之前sql语句生成的数据），再生成 03-11 clear=0# 业务日期mock.date=2020-03-11# 是否重置mock.clear=1 生成数据 1java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar 2.4 同步策略数据同步策略的类型包括：全量表、增量表、新增及变化表 全量表：存储完整的数据。 增量表：存储新增加的数据。 新增及变化表：存储新增加的数据和变化的数据。 特殊表：只需要存储一次。 2.4.1 全量同步策略每日存入一份完整数据，作为一个分区 适用场景：表数据量不大（如小于10万），每天既有新增，也有修改的场景 例子：品牌表、优惠规则、活动、商品分类等 2.4.2 增量同步策略每天存储一份增量数据作为一个分区 适用场景：表数据量大（十万、百万以上），且每天只有新增数据，不会修改原来的数据 例子：退单表、订单状态表、商品评论表等 2.4.3 新增及变化策略每日新增及变化，就是存储创建时间和操作时间都是今天的数据 适用场景：表的数据量大，既会有新增，又会有变化 例子：用户表、订单表、优惠卷领用表 2.4.4 特殊策略某些特殊的维度表，可不必遵循上述同步策略，如一共只存储一次。 1）客观世界维度 没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。 2）日期维度 日期维度可以一次性导入一年或若干年的数据。 3）地区维度 省份表、地区表 2.5 业务数据导入 HDFS 使用 sqoop 工具 时间处理：T + 1模式，即后一天导入前一天的数据 导表类型： 全量：select * from table where 1=1（1=1 是为了适应 Sqoop 中 and $CONDITIONS的语法） 增量：select * from table where createtime paytime = $cur_date 新增和变化：``select * from table where createtime or operatetime = $cur_date` 执行脚本： 内容略 执行： 参数： 参数1：first or all，表示第一次或每日，first会导入两个地区表，all就不导入了 参数2：日期 mysql_to_hdfs.sh first 2020-03-10 mysql_to_hdfs.sh all 2020-03-11 一天的数据导入大概20分钟","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"https://simon-ace.github.io/tags/数仓/"}]},{"title":"电商数仓 V2.0 （01 用户行为采集平台）","slug":"电商数仓 V2.0/电商数仓 V2.0 （01 用户行为采集平台）","date":"2021-01-14T16:00:00.000Z","updated":"2021-01-23T16:40:48.670Z","comments":true,"path":"2021/01/15/电商数仓 V2.0/电商数仓 V2.0 （01 用户行为采集平台）/","link":"","permalink":"https://simon-ace.github.io/2021/01/15/电商数仓 V2.0/电商数仓 V2.0 （01 用户行为采集平台）/","excerpt":"","text":"电商数仓 V2.0 （01 用户行为采集平台） 一、数据仓库概念二、项目需求及架构设计三、数据生成模块执行 123java -classpath logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar com.shuofxz.appclient.AppMain &gt; /dev/null 2&gt;&amp;1# 或者（需要打包后的主类是 AppMain）java -jar logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1 更改系统时间，用于生成不同日期的日志 （这个应该放到 java 代码中改吧） 12# 更改系统时间sudo date -s 2021-01-11 四、数据采集模块Hadoop 配置支持 lzo 压缩（先别搞，需要 lzo 编译的 hadoop 才能用） 需要先安装 lzo 库 1yum install lzo-devel lzop 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/ 修改 etc/hadaoop/core-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt; org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt; 将 jar 包 和 core-site.xml 同步到其他机器上 压力测试 Zookeeperflume 采集模块 配置 conf/flume-env.sh 1234# 添加export JAVA_HOME=/opt/module/jdk1.8.0_144# HADOOP_HOME 必须写，否则启动时会报错，找不到 libexport HADOOP_HOME=/opt/module/hadoop-2.7.2 flume 中添加一个配置文件，用于指定 interceptor, channel 等 conf/file-flume-kafka.conf 123456789101112131415161718192021222324252627282930313233a1.sources=r1a1.channels=c1 c2# configure sourcea1.sources.r1.type = TAILDIRa1.sources.r1.positionFile = /opt/module/flume-1.7.0-bin/test/log_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /tmp/logs/app.+a1.sources.r1.fileHeader = truea1.sources.r1.channels = c1 c2#interceptora1.sources.r1.interceptors = i1 i2a1.sources.r1.interceptors.i1.type = com.shuofxz.flume.interceptor.LogETLInterceptor$Buildera1.sources.r1.interceptors.i2.type = com.shuofxz.flume.interceptor.LogTypeInterceptor$Buildera1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = topica1.sources.r1.selector.mapping.topic_start = c1a1.sources.r1.selector.mapping.topic_event = c2# configure channela1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannela1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.channels.c1.kafka.topic = topic_starta1.channels.c1.parseAsFlumeEvent = falsea1.channels.c1.kafka.consumer.group.id = flume-consumera1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannela1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.channels.c2.kafka.topic = topic_eventa1.channels.c2.parseAsFlumeEvent = falsea1.channels.c2.kafka.consumer.group.id = flume-consumer 写一个 java，编写 ETL 和 类型拦截器 修改 conf/log4j.properties 123# 写相对路径貌似有问题，这个路径的logs 文件夹貌似也要提前创建# 疑问：这个为啥要改呢？？？flume.log.dir=/opt/module/flume-1.7.0-bin/logs flume 群起群停脚本 flume-f1.sh 12345678910111213141516171819#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop102 hadoop103 do echo \" --------启动 $i 采集flume-------\" ssh $i \"nohup /opt/module/flume-1.7.0-bin/bin/flume-ng agent --conf /opt/module/flume-1.7.0-bin/conf --conf-file /opt/module/flume-1.7.0-bin/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/flume.log 2&gt;&amp;1 &amp;\" done&#125;;;\"stop\")&#123; for i in hadoop102 hadoop103 do echo \" --------停止 $i 采集flume-------\" ssh $i \"ps -ef | grep file-flume-kafka | grep -v grep |awk '&#123;print \\$2&#125;' | xargs kill\" done&#125;;;esac 注意： 12# 这个参数要加，否则 flume 运行不正常-Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/flume.log 2&gt;&amp;1 注意： flume-env.sh 中需要配置 flume_opts 到 hadoop common 目录，否则启动时报错 Kafka 日志收集 flume 启动后就会往 kafka 里写数据，topic 也会自己创建 测试：执行之前创建的生成日志脚本 gen_logs.sh，如果可以在kafka中接收到新的数据，证明从日志生成 -&gt; flume -&gt; kafka 链路打通了 压力测试 12345# Producerbin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092# Consumerbin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1 查看列表 1bin/kafka-topics.sh --zookeeper hadoop102:2181 --list 消费消息 12345bin/kafka-console-consumer.sh \\--bootstrap-server hadoop102:9092 --from-beginning --topic topic_startbin/kafka-console-consumer.sh \\--bootstrap-server hadoop102:9092 --topic topic_start Kafka 数据存储到 HDFS 配置文件 conf/kafka-flume-hdfs.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768## 组件a1.sources=r1 r2a1.channels=c1 c2a1.sinks=k1 k2## source1a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r1.batchSize = 5000a1.sources.r1.batchDurationMillis = 2000a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.sources.r1.kafka.topics=topic_start# 重新读取数据需要添加，用于重新消费 kafka 的数据#a1.sources.r1.groupId=f2_1#a1.sources.r1.kafka.consumer.auto.offset.reset = earliest## source2a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r2.batchSize = 5000a1.sources.r2.batchDurationMillis = 2000a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.sources.r2.kafka.topics=topic_event#a1.sources.r2.groupId=f2_1#a1.sources.r2.kafka.consumer.auto.offset.reset = earliest## channel1a1.channels.c1.type = filea1.channels.c1.checkpointDir = /opt/module/flume-1.7.0-bin/checkpoint/behavior1a1.channels.c1.dataDirs = /opt/module/flume-1.7.0-bin/data/behavior1/a1.channels.c1.keep-alive = 6## channel2a1.channels.c2.type = filea1.channels.c2.checkpointDir = /opt/module/flume-1.7.0-bin/checkpoint/behavior2a1.channels.c2.dataDirs = /opt/module/flume-1.7.0-bin/data/behavior2/a1.channels.c2.keep-alive = 6## sink1a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%da1.sinks.k1.hdfs.filePrefix = logstart-##sink2a1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%da1.sinks.k2.hdfs.filePrefix = logevent-## 不要产生大量小文件,生产环境rollInterval配置为3600a1.sinks.k1.hdfs.rollInterval = 10a1.sinks.k1.hdfs.rollSize = 134217728a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k2.hdfs.rollInterval = 10a1.sinks.k2.hdfs.rollSize = 134217728a1.sinks.k2.hdfs.rollCount = 0## 控制输出文件是原生文件(hdfs未配置lzo压缩，这里全都注释掉)#a1.sinks.k1.hdfs.fileType = CompressedStream#a1.sinks.k2.hdfs.fileType = CompressedStream#a1.sinks.k1.hdfs.codeC = lzop#a1.sinks.k2.hdfs.codeC = lzop## 拼装a1.sources.r1.channels = c1a1.sinks.k1.channel= c1a1.sources.r2.channels = c2a1.sinks.k2.channel= c2 脚本 flume-f2.sh 12345678910111213141516171819#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop104 do echo \" --------启动 $i 消费flume-------\" ssh $i \"nohup /opt/module/flume-1.7.0-bin/bin/flume-ng agent --conf-file /opt/module/flume-1.7.0-bin/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume-1.7.0-bin/logs/f2_log.txt 2&gt;&amp;1 &amp;\" done&#125;;;\"stop\")&#123; for i in hadoop104 do echo \" --------停止 $i 消费flume-------\" ssh $i \"ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '&#123;print \\$2&#125;' | xargs kill\" done&#125;;;esac 集群启动/停止脚本cluster.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#! /bin/bashcase $1 in\"start\")&#123; echo \" -------- 启动 集群 -------\" echo \" -------- 启动 hadoop集群 -------\" /opt/module/hadoop-2.7.2/sbin/start-dfs.sh ssh hadoop103 \"/opt/module/hadoop-2.7.2/sbin/start-yarn.sh\" #启动 Zookeeper集群 /opt/module/zookeeper-3.4.10/bin/zk-all.sh start for (( i=0; i&lt;6; i++ ));do echo \"wait... $i\" sleep 1s; done #启动 Flume采集集群 /opt/module/flume-1.7.0-bin/bin/flume-f1.sh start #启动 Kafka采集集群 /opt/module/kafka_2.12-2.3.1/bin/kafka-all.sh start sleep 6s; #启动 Flume消费集群 /opt/module/flume-1.7.0-bin/bin/flume-f2.sh start &#125;;;\"stop\")&#123; echo \" -------- 停止 集群 -------\" #停止 Flume消费集群 /opt/module/flume-1.7.0-bin/bin/flume-f2.sh stop #停止 Kafka采集集群 /opt/module/kafka_2.12-2.3.1/bin/kafka-all.sh stop sleep 10s; #停止 Flume采集集群 /opt/module/flume-1.7.0-bin/bin/flume-f1.sh stop #停止 Zookeeper集群 /opt/module/zookeeper-3.4.10/bin/zk-all.sh stop echo \" -------- 停止 hadoop集群 -------\" ssh hadoop103 \"/opt/module/hadoop-2.7.2/sbin/stop-yarn.sh\" /opt/module/hadoop-2.7.2/sbin/stop-dfs.sh&#125;;;esac 停止集群 1cluster.sh stop 改系统时间 change_date.sh 1change_date.sh 2020-03-10 1234567#!/bin/bashfor i in hadoop102 hadoop103 hadoop104do echo \"========== Change Date $i ==========\" ssh -t $i \"sudo date -s $1\"done 启动集群 1cluster.sh start 生成日志 1gen_logs.sh 123456#!/bin/bashfor i in hadoop102 hadoop103;do echo \"====== generte $i log =======\" ssh $i \"java -jar /opt/project/data_warehouse_v2/logcollecter-1.0-SNAPSHOT-jar-with-dependencies.jar $1 $2 &gt; /dev/null 2&gt;&amp;1 &amp;\"done 查看 hdfs 中的结果 启动后，日志经历 file -&gt; flume -&gt; kafka -&gt; flume -&gt; hdfs，最终会输出到配置的hdfs://origin_data/gmall/log/topic_event/%Y-%m-%d中 注意 如果要删掉原来的重新采集的话，1）kafka 注意删除的时候把 topic 删除完全，去 Zookeeper 中看是否还有该 topic 记录；2）flume 要重新消费","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"数仓","slug":"数仓","permalink":"https://simon-ace.github.io/tags/数仓/"}]},{"title":"Python 解析xml配置文件","slug":"Python/Python 解析xml配置文件","date":"2020-12-20T16:00:00.000Z","updated":"2020-12-21T01:52:41.880Z","comments":true,"path":"2020/12/21/Python/Python 解析xml配置文件/","link":"","permalink":"https://simon-ace.github.io/2020/12/21/Python/Python 解析xml配置文件/","excerpt":"","text":"https://www.cnblogs.com/hupeng1234/p/7262371.html https://www.cnblogs.com/yyds/p/6627208.html https://blog.csdn.net/liangpingguo/article/details/105164967 https://blog.csdn.net/youZhengChuan/article/details/52996524 https://www.pythonf.cn/read/79523","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"},{"name":"Python","slug":"Python","permalink":"https://simon-ace.github.io/tags/Python/"}]},{"title":"JVM 资料","slug":"JVM/JVM 资料","date":"2020-12-15T16:00:00.000Z","updated":"2020-12-16T07:20:43.395Z","comments":true,"path":"2020/12/16/JVM/JVM 资料/","link":"","permalink":"https://simon-ace.github.io/2020/12/16/JVM/JVM 资料/","excerpt":"","text":"1 JVM 资料整理Java堆分析器 - Eclipse Memory Analyzer Tool(MAT) https://www.jianshu.com/p/de989b94ca3a 内存分析工具MAT的使用入门https://cloud.tencent.com/developer/article/1676945","categories":[{"name":"JVM","slug":"JVM","permalink":"https://simon-ace.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://simon-ace.github.io/tags/JVM/"}]},{"title":"Linux文本编辑命令 sed","slug":"Linux/常用命令/Linux文本编辑命令 sed","date":"2020-12-15T16:00:00.000Z","updated":"2020-12-18T08:41:27.363Z","comments":true,"path":"2020/12/16/Linux/常用命令/Linux文本编辑命令 sed/","link":"","permalink":"https://simon-ace.github.io/2020/12/16/Linux/常用命令/Linux文本编辑命令 sed/","excerpt":"","text":"一、基本介绍二、常用操作2.1 替换用s命令替换 1sed \"s/替换前字符/替换后字符/\" xxx.txt # 仅替换每行第一个匹配的字符 1234567➜ cat NewFile.txthello tom1234hhh ➜ sed \"s/h/p/\" NewFile.txtpello tom1234phh 可以在后面添加g，作用于行内所有匹配的字符 1234➜ sed 's/h/p/g' NewFile.txtpello tom1234ppp 输出到文件 12345# 输出到新文件sed 's/h/p/g' NewFile.txt &gt; nn.txt# 原地替换（-i）sed -i 's/h/p/g' NewFile.txt 2.2 其他d命令：删除匹配行 1sed &apos;2,$d&apos; my.txt p命令：打印命令，类似grep功能 1sed -n &apos;1,/fish/p&apos; my.txt 三、正则匹配3.1 单个匹配在每一行最前面(^)加点东西： 1sed &quot;s/^/#/g&quot; pets.txt 在每一行最后面($)加点东西： 1sed &quot;s/$/ --- /g&quot; pets.txt 正则表达式的一些最基本的东西： ^ 表示一行的开头。如：/^#/ 以#开头的匹配。 $ 表示一行的结尾。如：/}$/ 以}结尾的匹配。 \\&lt; 表示词首。 如 \\&lt;abc 表示以 abc 为首的詞。 \\&gt; 表示词尾。 如 abc\\&gt; 表示以 abc 結尾的詞。 . 表示任何单个字符。 *表示某个字符出现了0次或多次。 [] 空格或字符集合。 如：[abc]表示匹配a或b或c，还有[a-zA-Z]表示匹配所有的26个字符。如果其中有^表示反，如[^a]表示非a的字符。 正规则表达式是一些很牛的事，比如我们要去掉某html中的tags： 如果你这样搞的话，就会有问题: 1sed &quot;s/&lt;.*&gt;//g&quot; html.txt 要解决上面的那个问题，就得像下面这样，其中的”[^&gt;]”指定了除了&gt;的字符重复0次或多次。 1sed &quot;s/&lt;[^&gt;]*&gt;//g&quot; html.txt 我们再来看看指定需要替换第3行的内容： 1sed &quot;3s/my/your/g&quot; pets.txt 下面的命令只替换第3到第6行的文本： 1sed &quot;3,6s/my/your/g&quot; pets.txt 只替换每一行的第一个s： 1sed &quot;s/s/S/1&quot; my.txt 只替换每一行的第二个s： 1sed &quot;s/s/S/2&quot; my.txt 只替换第一行的第3个以后的s： 1sed &quot;s/s/S/3g&quot; my.txt 3.2 多个匹配如果我们需要一次替换多个模式，可参看下面的示例：（第一个模式把第一行到第三行的my替换成your，第二个则把第3行以后的This替换成了That）。 1sed &quot;1,3s/my/your/g; 3,$s/This/That/g&quot; my.txt 上面的命令等价于：（注：下面使用的是sed的-e命令行参数） 1sed -e &quot;1,3s/my/your/g&quot; -e &quot;3,$s/This/That/g&quot; my.txt 我们可以使用&amp;来当做被匹配的变量，然后可以在基本左右加点东西。如下所示： 1sed &quot;s/my/[&amp;]/g&quot; my.txt 3.3 圆括号匹配使用圆括号匹配的示例：（圆括号括起来的正则表达式所匹配的字符串会可以当成变量来使用，sed中使用的是\\1,\\2…） 1sed &quot;s/This is my \\([^,]*\\),.*is \\(.*\\)/\\1:\\2/g&quot; my.txt 上面这个例子中的正则表达式有点复杂，解开如下（去掉转义字符）： 正则为：This is my ([^,]),.*is (.) 匹配为：This is my (cat),……….is (betty) 然后：\\1就是cat，\\2就是betty 相关链接https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html https://man.linuxde.net/sed https://coolshell.cn/articles/9104.html https://zhuanlan.zhihu.com/p/145661854 https://www.cnblogs.com/along21/p/10366886.html","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, sed","slug":"Linux-sed","permalink":"https://simon-ace.github.io/tags/Linux-sed/"}]},{"title":"Linux常用命令","slug":"Linux/Linux常用命令","date":"2020-11-26T16:00:00.000Z","updated":"2020-11-27T07:19:32.667Z","comments":true,"path":"2020/11/27/Linux/Linux常用命令/","link":"","permalink":"https://simon-ace.github.io/2020/11/27/Linux/Linux常用命令/","excerpt":"","text":"一、基础指令 xargs [Linux输出转换命令 xargs](./常用命令/Linux输出转换命令 xargs.md) xargs命令的作用，是将标准输入转为命令行参数。 原因：大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数 12$ echo \"hello world\" | xargs echohello world 二、指令组合输出中间结果 使用 tee 指令，将中间结果进行输出 1find ./ -i \"*.java\" | tee JavaList | grep Spring","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Linux文本文件处理程序 awk","slug":"Linux/常用命令/Linux文本文件处理程序 awk","date":"2020-11-26T16:00:00.000Z","updated":"2020-11-27T09:54:44.816Z","comments":true,"path":"2020/11/27/Linux/常用命令/Linux文本文件处理程序 awk/","link":"","permalink":"https://simon-ace.github.io/2020/11/27/Linux/常用命令/Linux文本文件处理程序 awk/","excerpt":"12","text":"12 一、基础用法awk是处理文本文件的一个应用程序，几乎所有 Linux 系统都自带这个程序。 它依次处理文件的每一行，并读取里面的每一个字段。对于日志、CSV 那样的每行格式相同的文本文件，awk可能是最方便的工具。 awk的基本用法就是下面的形式。 12345# 格式$ awk 动作 文件名# 示例$ awk '&#123;print $0&#125;' demo.txt 上面示例中，demo.txt是awk所要处理的文本文件。前面单引号内部有一个大括号，里面就是每一行的处理动作print $0。其中，print是打印命令，$0代表当前行。因此上面命令是把每一行原样打印出来。 awk会根据空格和制表符，将每一行分成若干字段，依次用$1、$2、$3代表第一个字段、第二个字段、第三个字段等等。 12$ echo 'this is a test' | awk '&#123;print $3&#125;'a 下面，为了便于举例，我们把/etc/passwd文件保存成demo.txt。 12345root:x:0:0:root:/root:/usr/bin/zshdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologinsync:x:4:65534:sync:/bin:/bin/sync 这个文件的字段分隔符是冒号（:），所以要用-F参数指定分隔符为冒号 123456$ awk -F ':' '&#123; print $1 &#125;' demo.txtrootdaemonbinsyssync 二、变量 数字：第几个字段（从1开始） NF：当前行有多少个字段，因此$NF就代表最后一个字段 NR：表示当前处理的是第几行。 FILENAME：当前文件名 FS：字段分隔符，默认是空格和制表符。 RS：行分隔符，用于分割每一行，默认是换行符。 OFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。 ORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。 OFMT：数字输出的格式，默认为％.6g。 变量NF表示当前行有多少个字段，因此$NF就代表最后一个字段；$(NF-1)代表倒数第二个字段。 123&gt; $ echo 'this is a test' | awk '&#123;print $NF, $(NF-1)&#125;' # print中的逗号是以空格分隔的意思&gt; test a&gt; 变量NR表示当前处理的是第几行。 1234567&gt; $ awk -F ':' '&#123;print NR \") \" $1&#125;' demo.txt&gt; 1) root&gt; 2) daemon&gt; 3) bin&gt; 4) sys&gt; 5) sync&gt; 上面代码中，print命令里面，如果原样输出字符，要放在双引号里面 三、函数常用函数如下 toupper()：字符转为大写 tolower()：字符转为小写 length()：返回字符串长度 substr()：返回子字符串 sin()：正弦 cos()：余弦 sqrt()：平方根 rand()：随机数 awk内置函数的完整列表，可以查看手册。 例：toupper 123456$ awk -F ':' '&#123; print toupper($1) &#125;' demo.txtROOTDAEMONBINSYSSYNC 四、条件awk允许指定输出条件，只输出符合条件的行。 输出条件要写在动作的前面。 1$ awk '条件 动作' 文件名 请看下面的例子。print命令前面是一个正则表达式，只输出包含usr的行。 12345$ awk -F ':' '/usr/ &#123;print $1&#125;' demo.txtrootdaemonbinsys 下面的例子只输出奇数行，以及输出第三行以后的行。 12345678910# 输出奇数行$ awk -F ':' 'NR % 2 == 1 &#123;print $1&#125;' demo.txtrootbinsync# 输出第三行以后的行$ awk -F ':' 'NR &gt;3 &#123;print $1&#125;' demo.txtsyssync 下面的例子输出第一个字段等于指定值的行。 123456$ awk -F ':' '$1 == \"root\" &#123;print $1&#125;' demo.txtroot$ awk -F ':' '$1 == \"root\" || $1 == \"bin\" &#123;print $1&#125;' demo.txtrootbin 五、if 语句awk提供了if结构，用于编写复杂的条件。 1234$ awk -F ':' '&#123;if ($1 &gt; \"m\") print $1&#125;' demo.txtrootsyssync 上面代码输出第一个字段的第一个字符大于m的行。 if结构还可以指定else部分。 123456$ awk -F ':' '&#123;if ($1 &gt; \"m\") print $1; else print \"---\"&#125;' demo.txtroot------syssync 六、相关连接 An Awk tutorial by Example, Greg Grothaus 30 Examples for Awk Command in Text Processing, Mokhtar Ebrahim","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Linux输出转换命令 xargs","slug":"Linux/常用命令/Linux输出转换命令 xargs","date":"2020-11-26T16:00:00.000Z","updated":"2020-11-27T07:23:47.334Z","comments":true,"path":"2020/11/27/Linux/常用命令/Linux输出转换命令 xargs/","link":"","permalink":"https://simon-ace.github.io/2020/11/27/Linux/常用命令/Linux输出转换命令 xargs/","excerpt":"1$ echo \"hello world\" | xargs echo","text":"1$ echo \"hello world\" | xargs echo 一、基本用法xargs命令的作用，是将标准输入转为命令行参数。 原因：大多数命令都不接受标准输入作为参数，只能直接在命令行输入参数，这导致无法用管道命令传递参数 如下面 echo 不接受标准输出做参数，可用 xargs 做转换： 12$ echo \"hello world\" | xargs echohello world 二、参数-d 指定分隔符默认情况下，xargs将换行符和空格作为分隔符，把标准输入分解成一个个命令行参数。 1$ echo \"one two three\" | xargs mkdir 上面代码中，mkdir会新建三个子目录，执行mkdir one two three。 -d参数可以更改分隔符 12$ echo -e \"a\\tb\\tc\" | xargs -d \"\\t\" echoa b c 上面的命令指定制表符\\t作为分隔符，所以a\\tb\\tc就转换成了三个命令行参数。echo命令的-e参数表示解释转义字符。 -p -t打印将要执行的命令-p参数打印出要执行的命令，询问用户是否要执行。 12$ echo 'one two three' | xargs -p touchtouch one two three ?... -t参数则是打印出最终要执行的命令，然后直接执行，不需要用户确认。 12$ echo 'one two three' | xargs -t rmrm one two three -I 传递参数起别名如果xargs要将命令行参数传给多个命令，可以使用-I参数。【貌似，会按空格或回车对参数进行分割，然后重复执行命令，而不是当成命令的多个参数】 -I指定每一项命令行参数的替代字符串。 123456789101112$ cat foo.txtonetwothree$ cat foo.txt | xargs -I file sh -c 'echo file; mkdir file'one twothree$ ls one two three 上面代码中，foo.txt是一个三行的文本文件。我们希望对每一项命令行参数，执行两个命令（echo和mkdir），使用-I file表示file是命令行参数的替代字符串。执行命令时，具体的参数会替代掉echo file; mkdir file里面的两个file。 -l -L 指定多少行作为一个命令行参数1234$ echo -e \"a\\nb\\nc\" | xargs -L 1 echoabc -n 指定一行内多项作为一个命令行参数123456$ echo &#123;0..9&#125; | xargs -n 2 echo0 12 34 56 78 9 --max-procs 多线程执行xargs默认只用一个进程执行命令。如果命令要执行多次，必须等上一次执行完，才能执行下一次。 --max-procs参数指定同时用多少个进程并行执行命令。--max-procs 2表示同时最多使用两个进程，--max-procs 0表示不限制进程数。 1$ docker ps -q | xargs -n 1 --max-procs 0 docker kill 上面命令表示，同时关闭尽可能多的 Docker 容器，这样运行速度会快很多","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Shell 编程","slug":"Linux/Shell 编程","date":"2020-11-18T16:00:00.000Z","updated":"2020-11-23T10:34:45.414Z","comments":true,"path":"2020/11/19/Linux/Shell 编程/","link":"","permalink":"https://simon-ace.github.io/2020/11/19/Linux/Shell 编程/","excerpt":"","text":"0 基础第一行 指明脚本应使用的解释器的名字 1#! /bin/bash 编程规范： 大写字母表示常量，小写字母表示变量 1 变量变量 1234567891011121314151617181920# 注意等号两边不能有空格foo=\"yes\"echo $foo# 有空格的字符串需要用引号包围b=\"abc efg\" # 使用其他变量的值c=\"hhh $b\"# 将执行命令的结果赋值d=$(ls -la) d1=`ls -la` # ``等价 $()# 算数扩展，注意是两个括号e=$((5*7)) # 使用&#123;&#125;限定变量名的范围f=aa.txtg=$&#123;f&#125;1 环境变量 123export xx=xxx # 设置环境变量source xxx_file # 让文件中的环境变量立即生效echo $xx # 输出变量的值 位置参数变量 1234$n # $0为命令本身；$1-$9代表第一到第九个参数；$&#123;10&#125;十以上的用大括号括起来$* # 代表命令行中所有参数，并把所有参数看成一个整体$@ # 代表命令行中所有参数，但把每个参数区分对待？$# # 参数个数 预定义变量 123$$ # 当前进程号$! # 后台运行的最后一个进程的进程号$? # 最后一次执行命令的返回状态，0代表成功，其他都是失败 可自定义 2 条件判断运算符 123$((m+n)) # $(()) 中间写运算式$[m+n] # 推荐这种方式expr m + n # 不推荐 条件判断 test 1234567891011# 写法一test expression# 写法二[ expression ]# 写法三[[ expression ]] # 推荐使用这种写法，包含前两种的用法，且还支持模式匹配# 数字判断(( expression )) 字符串判断 1234567[ -n string ] # 如果字符串string的长度大于零，则为真[ -z string ] # 如果字符串string的长度为零，则为真[ string1 = string2 ] # 如果string1和string2相同，则为真[ string1 == string2 ] # 等同于[ string1 = string2 ][ string1 != string2 ] # 如果string1和string2不相同，则为真[ string1 '&gt;' string2 ] # 如果按照字典顺序string1排列在string2之后，则为真[ string1 '&lt;' string2 ] # 如果按照字典顺序string1排列在string2之前，则为真 整数判断 1(( m &gt; n )) # 可以直接用 &gt; &lt; == &gt;= &lt;= !=，空格都要有！ 逻辑判断 1[[ expr1 &amp;&amp; expr2 ]] # &amp;&amp; || ! 3 流控制if 1234567if [[ $x = 5 ]]; then # 等号两边有空格，也可以用 == 代替 echo \"x=5\"elif [[ $x = 10 ]]; then echo \"x=10\"else echo \"no\"fi case 1234567891011121314case $变量名 in\"值 1\"）如果变量的值等于值 1，则执行程序 1;;\"值 2\"）如果变量的值等于值 2，则执行程序 2;;*）如果变量的值都不是以上的值，则执行此程序;;esac while 1234567#! /bin/basha=5while [[ $a&gt;0 ]]; do echo $a a=$(( a-1 ))doneecho finish for 12345678for (( i=0; i&lt;5; i++ )); do echo $idone# =========================for i in A B C D; do echo $idone","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"shell, linux, 教程","slug":"shell-linux-教程","permalink":"https://simon-ace.github.io/tags/shell-linux-教程/"}]},{"title":"Spark","slug":"Hadoop/Spark","date":"2020-11-04T16:00:00.000Z","updated":"2020-12-01T02:26:10.621Z","comments":true,"path":"2020/11/05/Hadoop/Spark/","link":"","permalink":"https://simon-ace.github.io/2020/11/05/Hadoop/Spark/","excerpt":"","text":"Spark一、SparkCoreRDD 创建 从集合中创建 12345val listRdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))listRdd.foreach(println)val arrayRDD: RDD[Int] = sc.parallelize(Array(1, 2, 3, 4))arrayRDD.foreach(println) 由外部存储系统的数据集创建 1val lines: RDD[String] = sc.textFile(\"in\") RDD 转换算子Value 类型map123val listRdd: RDD[Int] = sc.makeRDD(1 to 10)val mulRdd: RDD[Int] = listRdd.map(_ * 2)mulRdd.collect().foreach(println) mapPartitions对每一个分区中的数据批处理。相当于只给每个分区的数据，只发送一次计算；而 map 的实现会给每个数据发送一次计算，增加了网络传输消耗；但是 mapPartitions 由于以整个分区为单位，可能会造成 OOM 12345val listRdd: RDD[Int] = sc.makeRDD(1 to 10)val mapParRdd: RDD[Int] = listRdd.mapPartitions(datas =&gt; &#123; datas.map(_ * 2)&#125;)mapParRdd.collect().foreach(println) mapPartitionsWithIndex1234567val listRdd: RDD[Int] = sc.makeRDD(1 to 10,3)val tupleRdd: RDD[(Int, String)] = listRdd.mapPartitionsWithIndex &#123; case (num, datas) =&gt; &#123; datas.map((_, \"partition_num: \" + num)) &#125;&#125;tupleRdd.collect().foreach(println) flatMap扁平化，变成一个一个单独的元素 123val listRdd: RDD[List[Int]] = sc.makeRDD(Array(List(1, 2), List(3, 4)))val flatRdd: RDD[Int] = listRdd.flatMap(datas =&gt; datas)flatRdd.collect().foreach(println) glom将同一个分区的元素，放到一个数组里 12345val listRdd: RDD[Int] = sc.makeRDD(1 to 16, 4)val glomRdd: RDD[Array[Int]] = listRdd.glom()glomRdd.collect().foreach(array =&gt; &#123; println(array.mkString(\",\"))&#125;) groupBy同一个分区的放到一个迭代对象中。结果 tuple 中，第一个元素是 key，后面是 iterator 123456val listRdd: RDD[Int] = sc.makeRDD(1 to 9)val groupRdd: RDD[(Int, Iterable[Int])] = listRdd.groupBy(i =&gt; i % 2)groupRdd.collect().foreach(println)--------------------(0,CompactBuffer(2, 4, 6, 8))(1,CompactBuffer(1, 3, 5, 7, 9)) filter按条件筛选 123val listRdd: RDD[Int] = sc.makeRDD(1 to 9)val filterRdd: RDD[Int] = listRdd.filter(_ % 2 == 0)filterRdd.collect().foreach(println) sample抽样。 参数介绍：withReplacement，是否重复抽样（可重复，泊松抽样；不可重复，伯努利抽样） fraction，打分？（可重复下，需≥0，代表大概可重复的次数；不可重复下，需[0,1]，代表大概抽取比例） 1234val listRdd: RDD[Int] = sc.makeRDD(1 to 10)// val sampleRdd: RDD[Int] = listRdd.sample(false, 0.7, 333)val sampleRdd: RDD[Int] = listRdd.sample(true, 4, 333)sampleRdd.collect().foreach(println) distinct去重 注意：distinct 计算后，原数据分区会被打乱，是因为中间进行了 shuffle 操作。同时也因为 shuffle 导致必须等待所有分区都计算完成后才能进行下一个操作；而没有 shuffle 操作的算子，执行完一个分区的操作后就可以继续进行下一个操作。 1234val listRdd: RDD[Int] = sc.makeRDD(List(1, 2, 1, 1, 3, 4, 6, 4, 3))val disRdd: RDD[Int] = listRdd.distinct()val disRdd: RDD[Int] = listRdd.distinct(2) // 设置去重后的分区数disRdd.collect().foreach(println) coalease缩减分区。实际为合并分区，即将其中某几个分区合并；若要扩大分区，需要添加 shuffle 参数 1234val listRdd: RDD[Int] = sc.makeRDD(1 to 16, 4)println(\"before: \", listRdd.partitions.size)val coalRdd: RDD[Int] = listRdd.coalesce(3)println(\"after: \", coalRdd.partitions.size) repartition对 coalease 的封装，shuffle = true 1rdd.repartition(2) sortBy排序，可自己设置排序规则 1234val listRdd: RDD[Int] = sc.makeRDD(List(3, 5, 1, 7, 2))val sortRdd: RDD[Int] = listRdd.sortBy(x =&gt; x)// val sortRdd: RDD[Int] = listRdd.sortBy(x =&gt; x%3)sortRdd.collect().foreach(println) 双 Value 类型union合并两个 Rdd 1val rdd3 = rdd1.union(rdd2) subtract去除相同元素，不同的会保留 1val rdd3 = rdd1.subtract(rdd2) intersection求交集后返回 1val rdd3 = rdd1.intersection(rdd2) cartesian笛卡尔积 1val rdd3 = rdd1.cartesian(rdd2) zip将两个 rdd 对应元素组合在一起（tuple？key-value？）。两个 rdd 分区数量和元素数量必须都相同；会把分区中的拆成一个一个的元素，组合的元素还在原来的分区里。 12345val rdd1: RDD[Int] = sc.makeRDD(Array(1, 2, 3, 4), 2)val rdd2: RDD[String] = sc.makeRDD(Array(\"a\", \"b\", \"c\", \"d\"), 2)val zipRdd: RDD[(Int, String)] = rdd1.zip(rdd2)zipRdd.collect().foreach(println)zipRdd.saveAsTextFile(\"output\") Key-Value 类型partitionBy根据 key 进行重新分区（因此 rdd 需要是 kv 的形式），也可自定义分区类 123val arrayRdd: RDD[(Int, String)] = sc.makeRDD(Array((1, \"aaa\"), (2, \"bbb\"), (3, \"ccc\"), (4, \"ddd\")), 2)val parRdd: RDD[(Int, String)] = arrayRdd.partitionBy(new org.apache.spark.HashPartitioner(3))parRdd.saveAsTextFile(\"output\") Rdd Action 行动算子综合练习二、SparkSQLSpark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。 Rdd → DataFrame → DataSet DataFrame：在 Rdd 的基础上，装饰了表结构，让每一个字段包含意义 DataSet：在 DataFrame 基础上，装饰了读取操作，让数据的读取像操作对象一样简单 1bin/spark-shell DataFrame创建1234567&gt; val df = spark.read.json(\"/opt/module/spark-2.3.2-local/mydata/user.json\")&gt; df.show==========# user.json&#123;\"name\":\"123\", \"age\":20&#125;&#123;\"name\":\"456\", \"age\":20&#125;&#123;\"name\":\"789\", \"age\":20&#125; SQL 风格语法 单个 Session 内 View 可见 12&gt; df.createTempView(\"user\")&gt; spark.sql(\"select * from user\").show() 创建全局表 12&gt; df.createGlobalTempView(\"user_g\")&gt; spark.newSession().sql(\"SELECT * FROM global_temp.user_g\").show() DSL 风格语法以对象的方式来操作数据 1234&gt; df.select(\"name\").show()&gt; df.select($\"name\", $\"age\" + 1).show()&gt; df.filter($\"age\" &gt; 21).show()&gt; df.groupBy(\"age\").count().show() Rdd 转为 DataFrame12345&gt; case class People(name:String, age:Int)&gt; val rdd1 = sc.makeRDD(List((\"zhangsan\", 20), (\"lisi\", 14)))&gt; val peopleRdd = rdd1.map(t=&gt;&#123;People(t._1, t._2)&#125;)&gt; val df = peopleRdd.toDF&gt; df.show DataFrame 转为 Rdd注意这里面转换之后，并不会还原成 People 结构，而只是一个 Row 对象。这是因为 DataFrame 本身不存数据的类型 1&gt; df.rdd DataSetDataset是具有强类型的数据集合，需要提供对应的类型信息。 解决 DataFrame 中取数只能通过下标来取的问题（啥意思？？） 创建12&gt; case class People(name:String, age:Int)&gt; val caseClassDS = Seq(People(\"Andy\", 21)).toDS() Rdd 转换为 DataSetRdd + 结构 → DataFrame；DataFram + 类型 → DataSet Rdd + 结构 + 类型 → DataSet 1234&gt; case class Person(name: String, age: Long)&gt; val mapRdd = rdd.map(t=&gt;&#123;Person(t._1, t._2)&#125;)&gt; val ds = mapRdd.toDS&gt; ds.show DataSet 转换为 Rdd转换回来仍保留着类型 1&gt; ds.rdd 三、SparkStreaming","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Spark, 教程","slug":"Spark-教程","permalink":"https://simon-ace.github.io/tags/Spark-教程/"}]},{"title":"Hive 常用操作 & 练习","slug":"Hadoop/Hive 常用操作 & 练习","date":"2020-10-20T16:00:00.000Z","updated":"2020-10-21T02:51:56.681Z","comments":true,"path":"2020/10/21/Hadoop/Hive 常用操作 & 练习/","link":"","permalink":"https://simon-ace.github.io/2020/10/21/Hadoop/Hive 常用操作 & 练习/","excerpt":"","text":"一、常用操作二、练习","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"},{"name":"练习","slug":"练习","permalink":"https://simon-ace.github.io/tags/练习/"}]},{"title":"console 日志输出到文件","slug":"Linux/console 日志输出到文件","date":"2020-10-08T16:00:00.000Z","updated":"2020-11-22T03:56:40.104Z","comments":true,"path":"2020/10/09/Linux/console 日志输出到文件/","link":"","permalink":"https://simon-ace.github.io/2020/10/09/Linux/console 日志输出到文件/","excerpt":"1some_command 2&gt;&amp;1 | tee output.txt","text":"1some_command 2&gt;&amp;1 | tee output.txt 在Linux中，如果想将一个程序在控制台中的输出字符输出到文件中，不保留控制台内的文字，可以用下面命令： 1some_command &gt; output.txt 命令结果会输出到output.txt中，换成&gt;&gt;可以追加到文件末尾 但如果想输出到文件同时，保留控制台的内容，需要使用tee命令，示例如下： 1some_command | tee output.txt 有时会发现上述命令后屏幕有输出，但文件内容为空，此时可能是由于some_command输出的字符从std error文件描述符输出，需要先将std error的输出导向到std output： 1some_command 2&gt;&amp;1 | tee output.txt 其中，2代表std error，1代表std output，&gt;&amp;是linux中fd到fd的重定向操作符。","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"SpringBoot 集成 Prometheus","slug":"Spring/SpringBoot集成Prometheus","date":"2020-09-22T16:00:00.000Z","updated":"2020-09-25T12:14:05.812Z","comments":true,"path":"2020/09/23/Spring/SpringBoot集成Prometheus/","link":"","permalink":"https://simon-ace.github.io/2020/09/23/Spring/SpringBoot集成Prometheus/","excerpt":"","text":"一、添加依赖 Maven pom.xml 12345678910111213&lt;!-- 第一条必须加，否则会导致 Could not autowire. No beans of 'xxxx' type found 的错误 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt; Gradle build.gradle 123implementation &apos;org.springframework.boot:spring-boot-starter-actuator&apos;compile &apos;io.micrometer:micrometer-registry-prometheus&apos;compile &apos;io.micrometer:micrometer-core&apos; 打开 Prometheus 监控接口 application.properties 1234server.port=8088spring.application.name=springboot2-prometheusmanagement.endpoints.web.exposure.include=*management.metrics.tags.application=$&#123;spring.application.name&#125; 可以直接运行程序，访问http://localhost:8088/actuator/prometheus可以看到下面的内容： 12345678910111213141516171819202122# HELP jvm_buffer_total_capacity_bytes An estimate of the total capacity of the buffers in this pool# TYPE jvm_buffer_total_capacity_bytes gaugejvm_buffer_total_capacity_bytes&#123;id=&quot;direct&quot;,&#125; 90112.0jvm_buffer_total_capacity_bytes&#123;id=&quot;mapped&quot;,&#125; 0.0# HELP tomcat_sessions_expired_sessions_total # TYPE tomcat_sessions_expired_sessions_total countertomcat_sessions_expired_sessions_total 0.0# HELP jvm_classes_unloaded_classes_total The total number of classes unloaded since the Java virtual machine has started execution# TYPE jvm_classes_unloaded_classes_total counterjvm_classes_unloaded_classes_total 1.0# HELP jvm_buffer_count_buffers An estimate of the number of buffers in the pool# TYPE jvm_buffer_count_buffers gaugejvm_buffer_count_buffers&#123;id=&quot;direct&quot;,&#125; 11.0jvm_buffer_count_buffers&#123;id=&quot;mapped&quot;,&#125; 0.0# HELP system_cpu_usage The &quot;recent cpu usage&quot; for the whole system# TYPE system_cpu_usage gaugesystem_cpu_usage 0.0939447637893599# HELP jvm_gc_max_data_size_bytes Max size of old generation memory pool# TYPE jvm_gc_max_data_size_bytes gaugejvm_gc_max_data_size_bytes 2.841116672E9# 此处省略超多字... 二、Prometheus 安装与配置使用 docker 运行 Prometheus（仅初始测试） 1docker run --name prometheus -d -p 9090:9090 prom/prometheus:latest 写配置文件prometheus.yml 12345678910111213141516171819202122232425262728293031# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] # demo job - job_name: 'springboot-actuator-prometheus-test' # job name metrics_path: '/actuator/prometheus' # 指标获取路径 scrape_interval: 5s # 间隔 basic_auth: # Spring Security basic auth username: 'actuator' password: 'actuator' static_configs: - targets: ['docker.for.mac.localhost:18080'] # 实例的地址，默认的协议是http （这里开始有问题，直接写 localhost 是访问容器内的地址，而不是宿主机的。可通过在网页上方 status -&gt; targets 查看对应的服务情况 运行 docker 1docker run -d -p 9090:9090 -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml 访问 http://localhost:9090，可看到如下界面 点击 Insert metric at cursor ，即可选择监控指标；点击 Graph ，即可让指标以图表方式展示；点击Execute 按钮，即可看到指标图 三、Grafana 安装和配置1、启动 1$ docker run -d --name=grafana -p 3000:3000 grafana/grafana 2、登录 访问 http://localhost:3000/login ，初始账号/密码为：admin/admin 3、配置数据源 点击左侧齿轮Configuration中Add Data Source，会看到如下界面： 这里我们选择Prometheus 当做数据源，这里我们就配置一下Prometheus 的访问地址，点击 Save &amp; Test 4、创建监控 Dashboard 点击导航栏上的 + 按钮，并点击Dashboard，将会看到类似如下的界面 点击+ Add new panel 四、自定义监控指标1、创建 Prometheus 监控管理类PrometheusCustomMonitor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import io.micrometer.core.instrument.Counter;import io.micrometer.core.instrument.DistributionSummary;import io.micrometer.core.instrument.MeterRegistry;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;import java.util.concurrent.atomic.AtomicInteger;@Componentpublic class PrometheusCustomMonitor &#123; private Counter requestErrorCount; private Counter orderCount; private DistributionSummary amountSum; private AtomicInteger failCaseNum; private final MeterRegistry registry; @Autowired public PrometheusCustomMonitor(MeterRegistry registry) &#123; this.registry = registry; &#125; @PostConstruct private void init() &#123; requestErrorCount = registry.counter(\"requests_error_total\", \"status\", \"error\"); orderCount = registry.counter(\"order_request_count\", \"order\", \"test-svc\"); amountSum = registry.summary(\"order_amount_sum\", \"orderAmount\", \"test-svc\"); failCaseNum = registry.gauge(\"fail_case_num\", new AtomicInteger(0)); &#125; public Counter getRequestErrorCount() &#123; return requestErrorCount; &#125; public Counter getOrderCount() &#123; return orderCount; &#125; public DistributionSummary getAmountSum() &#123; return amountSum; &#125; public AtomicInteger getFailCaseNum() &#123; return failCaseNum; &#125;&#125; 2、新增/order接口 当 flag=&quot;1&quot;时，抛异常，模拟下单失败情况。在接口中统计order_request_count和order_amount_sum 12345678910111213141516171819202122232425262728import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;import java.util.Random;@RestControllerpublic class TestController &#123; @Resource private PrometheusCustomMonitor monitor; @RequestMapping(\"/order\") public String order(@RequestParam(defaultValue = \"0\") String flag) throws Exception &#123; // 统计下单次数 monitor.getOrderCount().increment(); if (\"1\".equals(flag)) &#123; throw new Exception(\"出错啦\"); &#125; Random random = new Random(); int amount = random.nextInt(100); // 统计金额 monitor.getAmountSum().record(amount); monitor.getFailCaseNum().set(amount); return \"下单成功, 金额: \" + amount; &#125;&#125; 3、新增全局异常处理器GlobalExceptionHandler 统计下单失败次数requests_error_total 123456789101112131415161718import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;@ControllerAdvicepublic class GlobalExceptionHandler &#123; @Resource private PrometheusCustomMonitor monitor; @ResponseBody @ExceptionHandler(value = Exception.class) public String handle(Exception e) &#123; monitor.getRequestErrorCount().increment(); return \"error, message: \" + e.getMessage(); &#125;&#125; 4、测试 启动项目，访问http://localhost:8080/order和http://localhost:8080/order?flag=1模拟下单成功和失败的情况，然后我们访问http://localhost:8080/actuator/prometheus，可以看到我们自定义指标已经被 /prometheus 端点暴露出来 12345678910# HELP requests_error_total # TYPE requests_error_total counterrequests_error_total&#123;application=&quot;springboot-actuator-prometheus-test&quot;,status=&quot;error&quot;,&#125; 41.0# HELP order_request_count_total # TYPE order_request_count_total counterorder_request_count_total&#123;application=&quot;springboot-actuator-prometheus-test&quot;,order=&quot;test-svc&quot;,&#125; 94.0# HELP order_amount_sum # TYPE order_amount_sum summaryorder_amount_sum_count&#123;application=&quot;springboot-actuator-prometheus-test&quot;,orderAmount=&quot;test-svc&quot;,&#125; 53.0order_amount_sum_sum&#123;application=&quot;springboot-actuator-prometheus-test&quot;,orderAmount=&quot;test-svc&quot;,&#125; 2701.0 5、使用 Prometheus 监控 重新运行 docker 1docker run -d -p 9090:9090 -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml 选择对应指标后可以看到数据变化 6、使用 Grafana 展示 在 Dashboard 界面选择对应的监控指标即可 参考资料： Metric types | Prometheus IntelliJ IDEA创建第一个Spring Boot项目_Study Notes-CSDN博客 Spring Boot 使用 Micrometer 集成 Prometheus 监控 Java 应用性能 【springboot 2.0】 Micrometer Application Monitoring【官方文档】 Spring Boot 微服务应用集成Prometheus + Grafana 实现监控告警 ★ Monitoring Java Spring Boot applications with Prometheus: Part 1 | by Arush Salil | Kubernauts 【放弃这个教程？】client java 不支持 springboot 2.x，最高支持 1.5 Spring Boot 参考指南（端点）_风继续吹 - SegmentFault 思否","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"SpringBoot, Prometheus","slug":"SpringBoot-Prometheus","permalink":"https://simon-ace.github.io/tags/SpringBoot-Prometheus/"}]},{"title":"虚拟机 Hadoop 环境配置","slug":"Hadoop/虚拟机Hadoop环境配置","date":"2020-09-16T16:00:00.000Z","updated":"2021-06-09T08:45:52.548Z","comments":true,"path":"2020/09/17/Hadoop/虚拟机Hadoop环境配置/","link":"","permalink":"https://simon-ace.github.io/2020/09/17/Hadoop/虚拟机Hadoop环境配置/","excerpt":"","text":"一、安装 CentOS 6 &amp; 基本配置 win10通过VMware安装CentOS6.5 - 简书https://www.jianshu.com/p/9d5b9757a1ef 1、关闭防火墙 12service iptables stopchkconfig iptables off 2、创建普通用户 12useradd Acepasswd 123 3、创建软件存储文件夹，并更改所有权 12mkdir /opt/software /opt/modulechown Ace:Ace /opt/software /opt/module 4、用户添加到 sudoers 12vi /etc/sudoersAce ALL=(ALL) NOPASSWD:ALL 5、改 Hosts 12345#!/bin/bashfor ((i=101;i&lt;105;i++))do echo \"192.168.87.$i hadoop$i\" &gt;&gt; /etc/hostsdone 6、改静态 ip vim /etc/sysconfig/network-scripts/ifcfg-eth0 123456789DEVICE=eth0TYPE=EthernetONBOOT=yesBOOTPROTO=staticIPADDR=192.168.87.100PREFIX=24GATEWAY=192.168.87.2DNS1=192.168.87.2NAME=\"System eth0\" 【创建新虚拟机，下面的都要做一遍，可以写脚本解决（看下面，推荐）】 6、改 ip 地址（同上6） vim /etc/sysconfig/network-scripts/ifcfg-eth0 1IPADDR=192.168.87.100 # 改成对应的 7、改主机名 vim /etc/sysconfig/network 1HOSTNAME=hadoopxxx 8、删除多余网卡 vim /etc/udev/rules.d/70-persistent-net.rules 12# 第一行删掉（只保留一个网卡就行，注释也删掉，手动删的话记得和后面脚本的行数要对应上）# 第二行最后 NAME=\"eth1\" 改为 NAME=\"eth0\" 9、拍快照，克隆 【脚本】 分发脚本 xsync vim xsync 123456789101112131415161718192021#!/bin/bash# 获取输入参数个数，如果没有参数，直接退出pcount=$#if ((pcount==0)); thenecho no args;exit;fip1=$1fname=`basename $p1`echo fname=$fnamedirname=`cd -P $(dirname $p1); pwd`echo dirname=$dirnameuser=`whoami`for((i=102;i&lt;105;i++)); do echo \"------------- hadoop$i ------------\" rsync -avlP $dirname/$fname hadoop$i:$dirnamedone 移动到bin目录下，sudo mv xsync /bin 安装rsync，sudo yum install -y rsync 改权限，chmod +x xsync 执行相同命令脚本 123456789101112#!/bin/bashpcount=$#if ((pcount==0)); thenecho no args;exit;fifor i in hadoop102 hadoop103 hadoop104do echo \"==== $i $1 ====\" ssh $i \"$1\"done 移动到/bin，改权限 自动配置网络脚本 123456789101112131415161718#!/bin/bashid=$1# sed -i \"s/before replace/after replace/\"sudo sed -i \"s/192.168.87.101/192.168.87.$id/\" /etc/sysconfig/network-scripts/ifcfg-eth0sudo sed -i \"s/hadoop101/hadoop$id/\" /etc/sysconfig/networkfile=/etc/udev/rules.d/70-persistent-net.rules# count \"SUBSYSTEM\" word numbernu=$(grep -c SUBSYSTEM $file)if(($nu &gt; 1));then # delete 8th line sed -i '8d' $filefised -i 's/eth1/eth0/' $filereboot 改权限，chmod +x change_network 二、安装 JAVA 和 Hadoop1、下载/上传 java 和 hadoop 的包到 /opt/software 2、解压 java 和 hadoop 到 /opt/module 3、安装（配置环境变量） 12345678910sudo vim /etc/profile# 在末尾添加# JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin# HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 4、测试安装情况 执行下面命令后，能出现版本号即为成功 12$ java -version$ hadoop version 5、使用 xsync 同步到多个机器上 三、Hadoop 配置Apache Hadoop 3.2.1 – Hadoop: Setting up a Single Node Cluster. 安装插件 1234567$ sudo yum install ssh$ sudo yum install pdsh # pdsh 可能默认找不到$ wget http://mirrors.mit.edu/epel/6/i386/epel-release-6-8.noarch.rpm$ rpm -Uvh epel-release-6-8.noarch.rpm$ yum install pdsh 环境配置 vim etc/hadoop/hadoop-env.sh 1export JAVA_HOME=/your-java-home-path 3.1 本地运行模式1234$ mkdir input$ cp etc/hadoop/*.xml input$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'$ cat output/* 3.2 伪分布式1、配置 vim etc/hadoop/core-site.xml 12345678910111213&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 暂时不配置 yarn 了 etc/hadoop/yarn-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 2、设置免密码登录 123$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys$ chmod 0600 ~/.ssh/authorized_keys 3、执行 Format the filesystem: 1$ bin/hdfs namenode -format Start NameNode daemon and DataNode daemon: 1$ sbin/start-dfs.sh The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs). Browse the web interface for the NameNode; by default it is available at: NameNode - http://localhost:9870/ Make the HDFS directories required to execute MapReduce jobs: 12$ bin/hdfs dfs -mkdir /user$ bin/hdfs dfs -mkdir /user/&lt;username&gt; Copy the input files into the distributed filesystem: 12$ bin/hdfs dfs -mkdir input$ bin/hdfs dfs -put etc/hadoop/*.xml input Run some of the examples provided: 1$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them: 12$ bin/hdfs dfs -get output output$ cat output/* or View the output files on the distributed filesystem: 1$ bin/hdfs dfs -cat output/* When you’re done, stop the daemons with: 1$ sbin/stop-dfs.sh 3.3 完全分布式同步两个软件，/etc/profile 3.3.1 集群配置 集群部署规划 NN 1个； 2NN 1个；RM 1个；DN 3个、NM 3个 —— 最少共需六台机器，但是开不起那么多个虚拟机，因此按下表进行合并配置 hadoop102 hadoop103 hadoop104 HDFS NameNode, DataNode DataNode SecondaryNameNode, DataNode YARN NodeManager ResourceManager, NodeManager NodeManager 配置集群 1）核心配置文件 配置etc/hadoop/core-site.xml 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 2）HDFS配置文件 配置``etc/hadoop/hadoop-env.sh` 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置etc/hadoop/hdfs-site.xml 12345678910&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; 3）YARN配置文件 配置etc/hadoop/yarn-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置etc/hadoop/yarn-site.xml 1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt; 4）MapReduce配置文件 配置etc/hadoop/mapred-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置etc/hadoop/mapred-site.xml 12345678910$ cp mapred-site.xml.template mapred-site.xml~~&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;~~ 在集群上分发配置好的Hadoop配置文件 1$ xsync /opt/module/hadoop-2.7.2/ 3.3.2 集群单点启动1）如果集群是第一次启动，需要格式化NameNode 1$ hadoop namenode -format 2）在 hadoop102 上启动 NameNode 1$ hadoop-daemon.sh start namenode 3）在 hadoop102、hadoop103、hadoop104 上分别启动DataNode 1$ hadoop-daemon.sh start datanode 4）在 hadoop104 上启动 secondarynamenode 1$ hadoop-daemon.sh start secondarynamenode 5）查看服务启动情况 jps 12345678910111213[hadoop102]$ jps4182 Jps2842 DataNode2747 NameNode[hadoop103]$ jps1712 DataNode2215 Jps[hadoop104]$ jps1680 DataNode2266 Jps2171 SecondaryNameNode 3.3.3 集群一键启动 配置机器间 ssh 无密登录 「方法1：共用一个秘钥」 123456# 生成秘钥$ ssh-keygen -t rsa# 将秘钥添加到本机的 authorized_keys 中，实现本机无密登录$ ssh-copy-id hadoop102# 共享同一个秘钥，实现集群机器间无密登录$ xsync ~/.ssh 「方法2：每个机器单独生成秘钥」 123456# 在每个机器上生成秘钥（每个机器执行一遍）$ ssh-keygen -t rsa# 把每个机器的秘钥分发到别的机器上（每个机器执行一遍）$ ssh-copy-id hadoop102$ ssh-copy-id hadoop103$ ssh-copy-id hadoop104 添加机器名，配置etc/hadoop/slaves，并同步到其他机器（xsync） 123hadoop102hadoop103hadoop104 启动 12[hadoop102]$ start-dfs.sh[hadoop103]$ start-yarn.sh # 应该在ResouceManager所在的机器上启动YARN 测试（单词计数） 1234567891011121314151617181920212223# 创建一个文件夹 wcinput，里面放一个文件，添加计数文件中的内容，如：qwedddsmo123qwe# 将这个文件夹上传到 hdfs$ hadoop fs -put wcinput/ /# 执行 MapReduce 程序$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /output# 查看执行结果[Ace@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /outputFound 2 items-rw-r--r-- 3 Ace supergroup 0 2020-09-27 10:15 /output/_SUCCESS-rw-r--r-- 3 Ace supergroup 24 2020-09-27 10:15 /output/part-r-00000[Ace@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /output/part-r-00000123 1ddd 1qwe 2smo 1 停止 12[hadoop102]$ stop-dfs.sh[hadoop103]$ stop-yarn.sh 3.3.4 配置历史服务器 &amp; 日志聚集 配置etc/hadoop/mapred-site.xml，添加： 12345678910&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop104:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop104:19888&lt;/value&gt;&lt;/property&gt; 配置etc/hadoop/yarn-site.xml 12345678910&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; [分发配置] 启动 1234[hadoop102]$ start-dfs.sh[hadoop103]$ start-yarn.sh[hadoop104]$ mr-jobhistory-daemon.sh start historyserver$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /wcinput /output/xxx 查看 打开 yarn web http://hadoop103:8088/ 打开 history，再点 log 就能看到任务具体的日志信息了 3.3.5 集群时间同步1）检查 ntp 是否安装 查看 ntp 包（切换到 root 用户） 123[root@hadoop102 ~]# rpm -qa | grep ntpntp-4.2.6p5-15.el6.centos.x86_64ntpdate-4.2.6p5-15.el6.centos.x86_64 如果没有这两个服务要安装一下 1yum install -y ntp 先停止 ntp 服务 1234567# 查看 ntpd 服务是否在运行$ service ntpd status# 如果在运行先关闭$ service ntpd stop$ chkconfig ntpd off# 再查看一下 ntpd 运行情况$ chkconfig --list ntpd 2）修改ntp配置文件/etc/ntp.conf 123456789101112# 修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# 修改2（集群在局域网中，不使用其他互联网上的时间），将下面四行注释掉server 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst# 添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）server 127.127.1.0fudge 127.127.1.0 stratum 10 3）修改/etc/sysconfig/ntpd文件 12增加内容如下（让硬件时间与系统时间一起同步）SYNC_HWCLOCK=yes 4）重新启动ntpd服务 12$ service ntpd status$ service ntpd start 5）设置ntpd服务开机启动 1[hadoop102]$ chkconfig ntpd on 6）其他机器配置（必须root用户） 在其他机器配置10分钟与时间服务器同步一次 12$ crontab -e*/10 * * * * /usr/sbin/ntpdate hadoop102 测试（修改任意机器时间），十分钟后查看机器是否与时间服务器同步 1$ date -s \"2017-9-11 11:11:11\" 7）若主机时间不对 123456$ service ntpd stop$ ntpdate us.pool.ntp.org$ service ntpd start# 或者直接$ sudo ntpdate -u pool.ntp.org 3.4 常用端口记录 Hadoop默认端口应用一览_在路上的学习者-CSDN博客 Hadoop常用端口号_baiBenny的博客-CSDN博客 组件 节点 默认端口 配置 用途说明 HDFS DataNode 50020 dfs.datanode.ipc.address ipc服务的端口 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口，可查看 HDFS 存储内容 HDFS NameNode 8020 fs.defaultFS 接收Client连接的RPC端口，用于获取文件系统metadata信息 YARN Resource Manager 8088 yarn.resourcemanager.webapp.address Yarn http服务的端口 HBase Master 16000 Master RPC Port（远程通信调用） Master 16010 Master Web Port Regionserver 16020 Regionserver RPC Port Regionserver 16030 Regionserver Web Port Spark 4040 查看 Spark Job 3.5 HA 配置两个 NameNode 配置 JournalNode 用于将 Active NN 的数据 同步到 Standby NN 上「解决元数据同步的问题」 配置 Zookeeper，解决主备 NN 切换的问题，防止脑裂 在 NN 上启动 failoverController（zkfc），作为 Zookeeper 的客户端，实现与 zk 集群的交互和监测 3.6 其他3.6.1 增加 Yarn 队列https://blog.csdn.net/lijingjingchn/article/details/84876193 修改 etc/hadoop/capacity-scheduler.xml 123456789101112131415161718&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt; &lt;value&gt;default, myqueue&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt; &lt;value&gt;40.9&lt;/value&gt; &lt;description&gt;Default queue target capacity.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.myqueue.capacity&lt;/name&gt; &lt;value&gt;59.1&lt;/value&gt; &lt;description&gt;Default queue target capacity.&lt;/description&gt; &lt;/property&gt; 刷新配置： 1bin/yarn rmadmin -refreshQueues 注意： 热更新只能增加队列，要删除队列只能重启 RM。 https://stackoverflow.com/questions/42589764/how-to-delete-a-queue-in-yarn 3.6.2 配置 node label 官方文档参考：https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/NodeLabel.html cloudera 文档参考（推荐）https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_yarn-resource-management/content/configuring_node_labels.html 1）先创建 node-label-conf 存放的路径 1hadoop fs -mkdir /node-labels-conf 2）配置yarn-site.xml ，增加 123456789101112131415 &lt;!--开启node label --&gt; &lt;property&gt; &lt;name&gt;yarn.node-labels.fs-store.root-dir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/node-labels-conf/&lt;/value&gt; &lt;!--9000端口号从core-site.xml中找--&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.node-labels.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--???????? 在 hadoop2.8.2 版本之前需要配置yarn.node-labels.configuration-type配置项。--&gt; &lt;property&gt; &lt;name&gt;yarn.node-labels.configuration-type&lt;/name&gt; &lt;value&gt;centralized or delegated-centralized or distributed&lt;/value&gt;&lt;/property&gt; 刷新 1bin/yarn rmadmin -refreshQueues 3）重启 RM 4）添加 label 1234567# 添加yarn rmadmin -addToClusterNodeLabels \"my_label_test\"# 查看[Ace@hadoop102 hadoop]$ yarn cluster --list-node-labels21/03/24 18:40:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.87.103:8032Node Labels: my_label_test 5）给 node 打 label 1234yarn rmadmin -replaceLabelsOnNode \"hadoop102=my_label_test\"# 如果想删除node上的标签yarn rmadmin -replaceLabelsOnNode \"hadoop102\" 6）将队列与 label 关联 123456789101112131415161718192021# 遵循层级配置# 1）root配置 &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.accessible-node-labels.my_label_test.capacity&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt;# 2) 子队列配置（给myqueue队列分配my_label_test的全部资源）# 如果不指定此字段，则将从其父字段继承。 ????????? &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.myqueue.accessible-node-labels&lt;/name&gt; &lt;value&gt;my_label_test&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.myqueue.accessible-node-labels.my_label_test.capacity&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt;# 当资源请求未指定节点标签时，应用将被提交到该值对应的分区。默认情况下，该值为空，即应用程序将被分配没有标签的节点上的容器。yarn.scheduler.capacity.&lt;queue-path&gt;.default-node-label-expression 12# 刷新队列yarn rmadmin -refreshQueues 7）查看效果 1234567891011121314151617181920212223[Ace@hadoop102 hadoop]$ yarn node -list21/03/24 18:50:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.87.103:8032Total Nodes:3 Node-Id Node-State Node-Http-Address Number-of-Running-Containers hadoop104:44284 RUNNING hadoop104:8042 0 hadoop103:41733 RUNNING hadoop103:8042 0 hadoop102:43484 RUNNING hadoop102:8042 0 [Ace@hadoop102 hadoop]$ yarn node -status hadoop102:4348421/03/24 19:01:17 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.87.103:8032Node Report : Node-Id : hadoop102:43484 Rack : /default-rack Node-State : RUNNING Node-Http-Address : hadoop102:8042 Last-Health-Update : 星期三 24/三月/21 06:59:23:634CST Health-Report : Containers : 0 Memory-Used : 0MB Memory-Capacity : 8192MB CPU-Used : 0 vcores CPU-Capacity : 8 vcores Node-Labels : my_label_test 8）测试 1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount -D mapreduce.job.queuename=myqueue -D node_label_expression=my_label_test /wcinput /output/xxx 2.7.2 版本有超多bug &amp; 不完善 a) 设置队列后 web ui 没有分组显示 b) 为了能在指定 label node 上运行，需要把默认队列的 capacity 和 max-capacity 都设为0。但是这样会导致只能起一个 AM（一个 job） 介绍label稳定版和非稳定版区别，以及自己开发的Yarn资源监控 3.6.3 添加 Prometheus 监控3.6.3.1 HDFS 监控 https://www.jesseyates.com/2019/03/03/hdfs-blocks-missing-vs-corrupt.html 配置 etc/hadoop/hadoop-env.sh，添加如下内容 12345# add prom# if 是必要的防止多次source 这个文件，导致端口被重复注册的问题（会无法启动namenode）if [[ $HADOOP_NAMENODE_OPTS != *\"javaagent\"* ]]; then export HADOOP_NAMENODE_OPTS=\"$HADOOP_NAMENODE_OPTS -javaagent:/opt/module/hadoop-2.7.2/jmx_prometheus_javaagent-0.13.0.jar=9300:/opt/module/hadoop-2.7.2/namenode.yml\"fi 创建 ${HADOOP_HOME}/namenode.yml 123lowercaseOutputName: truelowercaseOutputLabelNames: truewhitelistObjectNames: [\"Hadoop:*\", \"java.lang:*\"] 重启 HDFS（Namenode） 查看效果：http://hadoop102:9300 3.6.3.2 Yarn 监控 配置 etc/hadoop/hadoop-env.sh，添加如下内容 12# add promexport YARN_RESOURCEMANAGER_OPTS=\"$YARN_RESOURCEMANAGER_OPTS -javaagent:/opt/module/hadoop-2.7.2/jmx_prometheus_javaagent-0.13.0.jar=3333:/opt/module/hadoop-2.7.2/resourcemanager.yml\" 创建 ${HADOOP_HOME}/capacity-scheduler.xml（这里面有啥区别？？） 123lowercaseOutputName: truelowercaseOutputLabelNames: truewhitelistObjectNames: [\"Hadoop:*\", \"java.lang:*\"] 重启 Resource Manager 查看效果：http://hadoop104:8042/jmx 可能有用的参数： 12345678910111213141516171819&#125;, &#123; &quot;name&quot; : &quot;Hadoop:service=NodeManager,name=NodeManagerMetrics&quot;, &quot;modelerType&quot; : &quot;NodeManagerMetrics&quot;, &quot;tag.Context&quot; : &quot;yarn&quot;, &quot;tag.Hostname&quot; : &quot;hadoop103&quot;, &quot;ContainersLaunched&quot; : 15, &quot;ContainersCompleted&quot; : 1, &quot;ContainersFailed&quot; : 0, &quot;ContainersKilled&quot; : 11, &quot;ContainersIniting&quot; : 0, &quot;ContainersRunning&quot; : 3, &quot;AllocatedGB&quot; : 4, &quot;AllocatedContainers&quot; : 3, &quot;AvailableGB&quot; : 4, &quot;AllocatedVCores&quot; : 3, &quot;AvailableVCores&quot; : 5, &quot;ContainerLaunchDurationNumOps&quot; : 15, &quot;ContainerLaunchDurationAvgTime&quot; : 408.33333333333337&#125;, &#123; ==看到的内存比实际的多？== 1234567# HELP hadoop_nodemanager_availablegb AvailableGB (Hadoop&lt;service=NodeManager, name=NodeManagerMetrics&gt;&lt;&gt;AvailableGB)# TYPE hadoop_nodemanager_availablegb untypedhadoop_nodemanager_availablegb&#123;name=&quot;NodeManagerMetrics&quot;,&#125; 8.0# HELP hadoop_nodemanager_allocatedgb Current allocated memory in GB (Hadoop&lt;service=NodeManager, name=NodeManagerMetrics&gt;&lt;&gt;AllocatedGB)# TYPE hadoop_nodemanager_allocatedgb untypedhadoop_nodemanager_allocatedgb&#123;name=&quot;NodeManagerMetrics&quot;,&#125; 0.0 更细粒度监控 可以通过这个地址看到 json / xml 格式的监控信息 1http://&lt;rm http address:port&gt;/ws/v1/cluster/scheduler 3.6.4 NM 内存容量动态更新 NM内存配置参数推荐 YARN的Memory和CPU调优配置详解 yarn集群上内存和cpu调优和设置 YARN NodeManager 动态更新资源配置参数 Yarn patch User capacity has reached its maximum limit（用户容量已达到最大限制） 1yarn rmadmin -updateNodeResource $&#123;HOSTNAME&#125;:45454 53248 28 3.6.5 hdfs balancer Increasing HDFS Balancer Performance HDFS Administration - cloudera HDFS Balancers - cloudera HDFS balance策略详解 3.6.6 Nodemanager 重启恢复 YARN NodeManager Restart 特性 配置YARN集群重启时的作业自动恢复 Enabling NM Restart Step 1. To enable NM Restart functionality, set the following property in conf/yarn-site.xml to true. Property Value yarn.nodemanager.recovery.enabled true, (default value is set to false) Step 2. Configure a path to the local file-system directory where the NodeManager can save its run state.(本地文件目录) Property Description yarn.nodemanager.recovery.dir The local filesystem directory in which the node manager will store state when recovery is enabled. The default value is set to $hadoop.tmp.dir/yarn-nm-recovery. Step 3. Configure a valid RPC address for the NodeManager. Property Description yarn.nodemanager.address Ephemeral ports (port 0, which is default) cannot be used for the NodeManager’s RPC server specified via yarn.nodemanager.address as it can make NM use different ports before and after a restart. This will break any previously running clients that were communicating with the NM before restart. Explicitly setting yarn.nodemanager.address to an address with specific port number (for e.g 0.0.0.0:45454) is a precondition for enabling NM restart. 1234567891011121314&lt;!-- Yarn Restart --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.recovery.dir&lt;/name&gt; &lt;value&gt;/data1/eadop/hadoop-tmp&lt;/value&gt; &lt;!-- 会在最后自动添加 yarn-nm-recovery --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.address&lt;/name&gt; &lt;value&gt;0.0.0.0:45454&lt;/value&gt;&lt;/property&gt; 需要手动创建这个文件夹： 12mkdir /data1/eadop/hadoop-tmp/yarn-nm-state# chown 需要改权限 其他服务： 1234567org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: /data1/eadop/hadoop-tmp/nm-aux-services/mapreduce_shuffle/mapreduce_shuffle_state/LOCK: No such file or directory at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59) at org.apache.hadoop.service.AbstractService.start(AbstractService.java:204) at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceStart(AuxServices.java:159) at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193) at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120) at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:463) 12mkdir nm-aux-serviceschown yarn:yarn nm-aux-services 3.7 升级 Yarn3.7.1 升级版本到 2.8.5需要修改的文件 yarn-env.sh yarn-site.xml slaves core-site.xml hadoop-env.sh hdfs-site.xml 测试能否只升级 RM，不升级 NM（可以） hadoop各版本特性 Hadoop2.7.6 滚动升级Hadoop2.8.5 大数据的yarn类型系统分析与云储存 小米Hadoop YARN平滑升级3.1实践 Hadoop 2.7 不停服升级到 3.2 在滴滴的实践 3.8 Hadoop 进阶命令使用集群间数据平衡： 1&gt; nohup hdfs balancer -D &quot;dfs.balancer.movedWinWidth=300000000&quot; -D &quot;dfs.datanode.balance.bandwidthPerSec=2000m&quot; -threshold 1 &gt; hadoop-hadoop-balancer-hadoop-0018.log &amp; 节点内各个磁盘的数据平衡： 1&gt; hdfs diskbalancer -plan IP -bandwidth 1000 -v 2&gt; /dev/null | egrep ^/ | xargs hdfs diskbalancer -execute 接上，查看磁盘平衡情况/进度： 1&gt; hdfs diskbalancer -query IP YARN资源置空(这里多说一下，资源置空我们在生产环境是有些情况需要把这个node下线，但是此时此刻正有任务在运行，资源置空之后，UI上面会显示这个资源是负值，等正在运行的任务运行完成之后就不会再提交到这个node上了，就可以下线了) 注意这个PORT是UI页面上的Node Address，不是Node HTTP Address 1&gt; yarn rmadmin -updateNodeResource IP:PORT 0 0 HDFS高可用Namenode主从切换： nn1,nn2这两个是你集群配置文件配置高可用时指定的别名，需要用你自己的 1&gt; hdfs haadmin -failover nn2 nn1 HDFS退出安全模式 1&gt; hadoop dfsadmin -safemode leave HDFS动态生效datanode/namenode配置： status:查看动态生效配置状态 start:执行动态生效配置动作 properties:查看修改了哪些配置与正在运行的不一样 1&gt; hdfs dfsadmin -reconfig datanode IP:PORT status|start|properties 四、Zookeeper 配置4.1 本地模式1、安装前准备 安装Jdk 拷贝Zookeeper安装包到Linux系统下 解压到指定目录 1tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2、配置修改 修改conf/zoo_sample.cfg 1cp zoo_sample.cfg zoo.cfg 修改 zoo.cfg 文件 1dataDir=/opt/module/zookeeper-3.4.10/zkData 并创建 zkData 文件夹 3、操作Zookeeper 启动Zookeeper 1$ bin/zkServer.sh start 查看进程是否启动 123$ jps4020 Jps4001 QuorumPeerMain 查看状态： 1234$ bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: standalone 停止Zookeeper 1$ bin/zkServer.sh stop 4.2 集群模式1、配置 修改 zoo.cfg 文件 123456server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888# 2888 为集群间通信端口号# 3888 为选举端口号# server.2/3/4 为id，不相同即可 创建 zkData/myid，每个机器写不同的，要和前面的对应上 1234567$ vim zkData/myid# hadoop1022# hadoop1033# hadoop1044 配置 bin/zkEnv.sh 1234567# 替换前ZOO_LOG_DIR=\".\"# 替换后ZOO_LOG_DIR=\"/opt/module/zookeeper-3.4.10/logs\"# 添加 JAVA_HOME（远程启动的时候才是必要的，因为会丢失环境变量）export JAVA_HOME=/opt/module/jdk1.8.0_144 同步 xsync 2、启动 12# 每个机器都要单独启动./bin/zkServer.sh start 仅启动一台机器时： 1234$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgError contacting service. It is probably not running. 启动两台机器（超过半数）： 1234$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader / follower 3、集群脚本 12345678910111213141516171819202122232425262728#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== start $i zookeeper ====\" ssh $i \"/opt/module/zookeeper-3.4.10/bin/zkServer.sh start\" done&#125;;;\"stop\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== stop $i zookeeper ====\" ssh $i \"/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop\" done&#125;;;\"status\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== status $i zookeeper ====\" ssh $i \"/opt/module/zookeeper-3.4.10/bin/zkServer.sh status\" done&#125;;;esac 4.3 客户端操作 启动 1$ bin/zkCli.sh 执行 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建-s 含有序列-e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 五、Kakfa 配置5.1 环境准备 在 hadoop102 103 104 上均安装 Kafka jar 包下载 https://kafka.apache.org/downloads 命名中有两个版本号，第一个为 scala 版本，第二个是 kafka 版本 5.2 集群配置 修改 config/server.properties 1234567891011# broker的全局唯一编号，不能重复broker.id=0# 删除topic功能使能（kafka 2.x 版本 不需要配置）delete.topic.enable=true# kafka运行时数据存放的路径log.dirs=/opt/module/kafka_2.12-2.3.1/data# 配置连接Zookeeper集群地址zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 分发安装包到所有物理机上 xsync 将 hadoop103 104 上config/server.properties 中的 broker.id=x进行修改 单点启动 / 停止 依次在 hadoop102、hadoop103、hadoop104 节点上启动/停止 kafka，执行下面的命令 12345678# 启动# 前台运行$ bin/kafka-server-start.sh config/server.properties# 后台运行$ bin/kafka-server-start.sh -daemon config/server.properties# 停止$ bin/kafka-server-stop.sh 群起 / 群停 由于 Kafka 中没有给集群启动停止的脚本，需要自己写kk-all.sh 需要注意：要先在 ~/.bashrc 中配置 java 环境变量（xsync） 123# JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin 12345678910111213141516171819202122#!/bin/bashcase $1 in\"start\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== start $i kafka ====\" # ssh $i \"source /etc/profile\" ssh $i \"/opt/module/kafka_2.12-2.3.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.12-2.3.1/config/server.properties\" done&#125;;;\"stop\")&#123; for i in hadoop102 hadoop103 hadoop104 do echo \"==== stop $i kafka ====\" # ssh $i \"source /etc/profile\" ssh $i \"/opt/module/kafka_2.12-2.3.1/bin/kafka-server-stop.sh\" done&#125;;;esac 启动/停止 Kafka：（记得先启动 Zookeeper） 1234567$ ./kk-all.sh start$ jps1637 QuorumPeerMain6071 Kafka6538 Jps$ ./kk-all.sh stop 5.3 命令行操作 查看当前服务器中的所有 topic 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list 创建 topic 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first --replication-factor 定义副本数；--partitions 定义分区数 删除 topic 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first 查看某个 Topic 的详情 1$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first 发送消息 123$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first&gt;hello world&gt;atguigu atguigu 消费消息 1$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first --from-beginning 会把 first 主题中以往所有的数据都读取出来 六、Spark 配置6.1 环境准备下载地址：https://archive.apache.org/dist/spark/ 有两种版本，hadoop 版下载就能用，不依赖其他组价；without-hadoop 需要依赖已有的 hadoop 组件 spark-2.3.2-bin-hadoop2.7.tgzspark-2.3.2-bin-without-hadoop.tgz 6.2 本地模式 Local Jar 解压后进入 Spark 根目录执行（一个算 PI 的程序）： 123456bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master local[2] \\./examples/jars/spark-examples_2.11-2.3.2.jar 100bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.11-2.3.2.jar 100 输出结果： 12# 一大堆迭代过程 Pi is roughly 3.1424043142404314 可以通过访问 http://hadoop102:4040 查看任务运行情况 「问题」：运行结束这个页面就关闭了，不能查历史任务执行情况 「解决」：添加 Spark History Server Spark-shell 进入 Spark-shell，bin/spark-shell 1234567# 创建两个文件，里面输入几行单词$ vim 1.txt # xxxxx$ vim 2.txt # xxxxx# 进入 spark-shell 执行$ bin/spark-shell&gt; sc.textFile(\"input\").flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).collect 6.3 Standalone 模式构建一个由 Master + Slave 构成的 Spark 集群，Spark 运行在集群中。 这个要和 Hadoop 中的 Standalone 区别开来.这里的 Standalone 是指只用 Spark 来搭建一个集群, 不需要借助其他的框架.是相对于 Yarn 和 Mesos 来说的. 6.3.1 Spark server配置1、进入配置文件目录conf，配置spark-evn.sh 12cd conf/cp spark-env.sh.template spark-env.sh 在 spark-env.sh 文件中配置如下内容: 12SPARK_MASTER_HOST=hadoop102SPARK_MASTER_PORT=7077 # 默认端口就是7077, 可以省略不配 2、修改 slaves 文件，添加 worker 节点 12345cp slaves.template slaves# 在slaves文件中配置如下内容:hadoop201hadoop202hadoop203 3、修改 sbin/spark-config.sh，添加 JAVA_HOME （防止 JAVA_HOME is not set 报错） 1export JAVA_HOME=/opt/module/jdk1.8.0_144 4、分发spark-standalone 5、启动 Spark 集群 1sbin/start-all.sh 6、网页查看信息：http://hadoop102:8080/ 7、测试 1234567bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://hadoop102:7077 \\--executor-memory 1G \\--total-executor-cores 6 \\--executor-cores 2 \\./examples/jars/spark-examples_2.11-2.3.2.jar 100 6.3.2 spark-history-server在 Spark-shell 没有退出之前，看到正在执行的任务的日志情况:http://hadoop102:4040. 但是退出之后，执行的所有任务记录全部丢失 所以需要配置任务的历史服务器, 方便在任何需要的时候去查看日志。 配置spark-default.conf文件，开启 Log 1cp spark-defaults.conf.template spark-defaults.conf 在 spark-defaults.conf 文件中, 添加如下内容: 12spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop102:9000/spark-job-log 注意: hdfs://hadoop201:9000/spark-job-log 目录必须提前存在, 名字随意 修改spark-env.sh文件，添加如下配置 1export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://hadoop102:9000/spark-job-log\" 分发配置文件 启动历史服务 需要先启动 HDFS $HADOOP_HOME/sbin/start-dfs.sh 然后再启动: sbin/start-history-server.sh ui 地址: http://hadoop102:18080 6.4 Yarn 模式6.4.1 spark server 配置 修改 ${HADOOP_HOME}/etc/hadoop/yarn-site.xml（仅虚拟机中配置，防止内存不够） 12345678910&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默&gt;认是true --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默&gt;认是true --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 修改 conf/spark-env.sh，分发 1YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop 测试（注意 master、deploy-mode 参数的变化） 12345678$ start-dfs.sh$ start-yarn.shbin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\./examples/jars/spark-examples_2.11-2.3.2.jar 100 spark-shell 1$ bin/spark-shell --master yarn Yarn：http://hadoop103:8088 6.4.2 spark-history-server$HADOOP_HOME/etc/hadoop/yarn-site.xml 中添加 12345&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop104:19888/jobhistory/logs&lt;/value&gt; &lt;!-- 填 yarn historyserver 的物理机 --&gt;&lt;/property&gt; $SPARK_HOME/conf/spark-defaults.conf 12345spark.yarn.historyServer.address hadoop102:18080 # spark history Server 物理机spark.history.ui.port 18080spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop102:9000/spark-job-logspark.history.fs.logDirectory hdfs://hadoop102:9000/spark-job-log 相关服务 spark: sbin/start-all.sh HDFS: [hadoop102]$ start-dfs.sh Yarn: [hadoop103]$ start-yarn.sh Yarn-history: [hadoop104]$ mr-jobhistory-daemon.sh start historyserver Spark-history: [hadoop102] $ sbin/start-history-server.sh 【可以正常展示了】 相关文档解释：spark深入：配置文件与日志 - Super_Orco - 博客园 查看方式 通过 YARN 查询 http://hadoop103:8088/ 直接在 spark history server 中查询 http://hadoop102:18080/ 6.5 WordCount 程序略 七、Hive 配置7.1 单机默认配置下载地址：http://archive.apache.org/dist/hive/ 安装部署： 修改 conf/hive-env.sh.template 12345678$ mv hive-env.sh.template hive-env.sh$ vim hive-env.sh~~export HADOOP_HOME=/opt/module/hadoop-2.7.2export HIVE_CONF_DIR=/opt/module/hive-2.3.0-bin/conf~~ hadoop 相关配置 123456789# 启动 hdfs yarn$ start-dfs.sh$ start-yarn.sh# 创建 hive warehouse（存数据的地方）$ hadoop fs -mkdir /tmp$ hadoop fs -mkdir -p /user/hive/warehouse$ hadoop fs -chmod g+w /tmp$ hadoop fs -chmod g+w /user/hive/warehouse Hive 基本操作 1234567891011121314151617$ bin/hive# 查看数据库hive&gt; show databases;# 打开默认数据库 hive&gt; use default;# 显示 default 数据库中的表 hive&gt; show tables;# 创建一张表hive&gt; create table student(id int, name string);# 查看表的结构 hive&gt; desc student;# 向表中插入数据hive&gt; insert into student values(1000,\"ss\");# 查询表中数据hive&gt; select * from student;# 退出 hive hive&gt; quit; 7.2 修改默认数据库（derby -&gt; MySQL）derby 只支持单个客户端连接，仅适用于简单测试。更换成关系型数据库（如MySQL），可支持多客户端连接。 7.2.1 安装 MySQL 卸载原有的，安装新的 MySQL 12345678910111213141516171819# 卸载原有的$ su - # 切换到 root 用户$ rpm -qa | grep mysql mysql-libs-5.1.73-7.el6.x86_64$ rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64# 安装新的 MySQL-server$ rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpmcat /root/.mysql_secret # 记住默认的root登录密码OEXaQuS8IWkG19Xs$ service mysql status$ service mysql start# 安装 MySQL-client$ rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm# 修改 root 密码$ mysql -uroot -pOEXaQuS8IWkG19Xsmysql&gt;SET PASSWORD=PASSWORD('123456');mysql&gt;exit 配置 MySQL 远程登录 12345678910111213141516$ mysql -uroot -p123456mysql&gt; use mysql;mysql&gt; show tables;mysql&gt; select User, Host, Password from user;# 修改 user 表，把 Host 表内容修改为%mysql&gt; update user set host='%' where host='localhost';# 其他的都删掉mysql&gt; delete from user where host='hadoop102';mysql&gt; delete from user where host='127.0.0.1';mysql&gt; delete from user where host='::1';mysql&gt; flush privileges;mysql&gt; quit;# 都做完切换回原来的用户 7.2.2 修改 hive 元数据库 拷贝驱动 1$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive-2.3.0-bin/lib/ 配置 Metastore 到 MySQL 创建 conf/hive-site.xml 官方配置文档AdminManual Metastore Administration - Apache Hive - Apache Software Foundation 123456789101112131415161718192021222324&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt; &lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 用于启动 hive-metastore 的配置 123456789101112131415161718192021222324252627 &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt;&lt;!-- 可以显示表头列名 --&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;!-- 显示当前数据库名 --&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://hadoop102:9083&lt;/value&gt; &lt;/property&gt; 启动 12345678# 初试化hive库$ bin/schematool -initSchema -dbType mysql# 启动metastore节点$ nohup bin/hive --service metastore &amp;# 启动hiveserver2（可选？） $ nohup bin/hive --service hiveserver2 &amp;# 命令行启动$ bin/hive 7.3 Beeline 连接 Hive学习之路 （四）Hive的连接3种连接方式 - 扎心了，老铁 - 博客园https://www.cnblogs.com/qingyunzong/p/8715925.html 7.4 常用交互命令1、-e不进入 hive 的交互窗口执行 sql 语句 1$ bin/hive -e \"select id from student;\" 2、-f执行脚本中 sql 语句 创建 hivef.sql 文件 123$ touch hivef.sqlselect *from student;$ bin/hive -f xxx/hivef.sql &gt; xxx/result.txt 7.5 集成 Tez 引擎 解压 tez：/opt/module/tez-0.9.1-bin 同时上传一个 tez 包到 hdfs，用于给集群中其他节点用 1hadoop fs -put /opt/software/apache-tez-0.9.1-bin.tar.gz/ /tez 在 HIVE_HOME/conf 下创建 tez-site.xml 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;$&#123;fs.defaultFS&#125;/tez/apache-tez-0.9.1-bin.tar.gz&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.use.cluster.hadoop-libs&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;tez.history.logging.service.class&lt;/name&gt; &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hive-env.sh 1234567891011# Folder containing extra libraries required for hive compilation/execution can be controlled by:export TEZ_HOME=/opt/module/tez-0.9.1-bin #是你的tez的解压目录export TEZ_JARS=\"\"for jar in `ls $TEZ_HOME |grep jar`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jardonefor jar in `ls $TEZ_HOME/lib`; do export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jardoneexport HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS 修改 hive-site.xml 1234&lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;tez&lt;/value&gt;&lt;/property&gt; 测试 1234567891011121314# 启动Hive$ bin/hive# 创建表hive (default)&gt; create table student(id int,name string);# 向表中插入数据hive (default)&gt; insert into student values(1,\"zhangsan\");# 如果没有报错就表示成功了hive (default)&gt; select * from student;1 zhangsan 八、HBase 配置8.1 集群配置 conf/hbase-env.sh 1234567# 1、注释掉下面两行# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+export HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m\"export HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSiz e=128m\"# 2、使用单独的 Zookeeperexport HBASE_MANAGES_ZK=false conf/hbase-site.sh 1234567891011121314151617181920&lt;!-- 都要根据自己的机器进行配置 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt; 单独启动 123456789101112# 启动 HDFS、Zookeeper$ start-dfs.sh$ $&#123;Zookeeper_BASE&#125;/bin/zkServer.sh start # 每个机器都要单独启动# HBase$ bin/hbase-daemon.sh start master # 在其中一台启动$ bin/hbase-daemon.sh start regionserver # 都要启动$ bin/hbase-daemon.sh stop master # 在其中一台启动$ bin/hbase-daemon.sh stop regionserver # 都要启动# 可以在 hadoop102:16010 查看ui界面 群起 / 群停 12345678910111213141516# 启动 HDFS、Zookeeper$ start-dfs.sh$ $&#123;Zookeeper_BASE&#125;/bin/zkServer.sh start # 每个机器都要单独启动# 配置 conf/regionservers，添加所有 regionserver 的 hosthadoop102hadoop103hadoop104# 群起 / 群停$ bin/start-hbase.sh$ bin/stop-hbase.sh# regionservers$ bin/hbase-daemons.sh start regionserver$ bin/hbase-daemons.sh stop regionserver 进入 shell 12# 如果有 kerberos 认证，需要先 kinit 一下$ bin/hbase shell 8.2 常用操作使用 help 查看各种命令使用方式 12345678# 列出所有指令&gt; help# 查看某一组命令的帮助&gt; help 'COMMAND_GROUP'# 查看单个命令帮助&gt; help 'COMMAND' 8.2.1 namespaceCommands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables list_namespace 12# 查看所有库名&gt; list_namespace create_namespace 1&gt; create_namespace 'school' delete_namespace 12# 注意：只能删除空库&gt; delete_namespace 'school' list_namespace_tables 12# 列出库中所有表&gt; list_namespace_tables 'school' 8.2.2 DDLCommands: alter, alter_async, alter_status, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, locate_region, show_filters list 12# list 列出所有表&gt; list create 12345678# create '库名:表名', &#123; NAME =&gt; '列族名1', 属性名 =&gt; 属性值&#125;, &#123;NAME =&gt; '列族名2', 属性名 =&gt; 属性值&#125;, …&gt; create 'school:student', &#123;NAME=&gt;'info'&#125;&gt; create 'school:student', &#123;NAME=&gt;'info', VERSIONS=&gt;5&#125;# 如果你只需要创建列族，而不需要定义列族属性，那么可以采用以下快捷写法：# create'表名','列族名1' ,'列族名2', …# 不写库名，默认 namespace 为 default&gt; create 'student','info' desc 1&gt; desc 'student' disable 123# 停用表，防止对表进行写数据；在修改或删除表之前要 disable&gt; disable 'student'&gt; is_disable 'student' enable 123# 启用表&gt; enable 'student'&gt; is_enable 'student' alter 12# 需要先 disable&gt; alter 'student', &#123;NAME =&gt; 'info', VERSIONS =&gt; '5'&#125; drop 12# 需要先 disable&gt; drop 'student' count 12# 查看行数&gt; count 'student' truncate 12# 删除表数据&gt; truncate 'student' 8.2.3 DMLCommands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve scan 1234# 查看数据&gt; scan 'student', &#123;limit =&gt; 5&#125;# 查看每行最近十次修改的数据&gt; scan 'student', &#123;RAW =&gt; true, VERSIONS =&gt; 10&#125; put 12# put '表名', '行键', '列族:列名', '值'&gt; put 'student', '1001', 'info:name', 'Nick' get 1&gt; get 'student','1001' delete 1234# 删除某rowkey的全部数据：&gt; deleteall 'student', '1001'# 删除某rowkey的某一列数据：&gt; delete 'student', '1002', 'info:sex' 8.2.4 其他操作 flush 12# 将内存数据落盘&gt; flush 'student' 九、Flink 配置9.1 Standalone 模式1、准备安装包 1flink-1.10.1-bin-scala_2.12.tgz 2、修改 conf/flink-conf.yaml 文件 1jobmanager.rpc.address: hadoop102 3、修改 conf/slaves 文件 12hadoop103hadoop104 4、分发 5、启动 1./bin/start-cluster.sh 6、访问 http://localhost:8081 可以对 flink 集群和任务进行监控管理 7、提交任务 web 模式 命令行 1./flink run -c com.atguigu.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hadoop, 教程","slug":"Hadoop-教程","permalink":"https://simon-ace.github.io/tags/Hadoop-教程/"}]},{"title":"vim 常用操作","slug":"Linux/vim 常用操作","date":"2020-09-16T16:00:00.000Z","updated":"2021-02-23T02:48:20.764Z","comments":true,"path":"2020/09/17/Linux/vim 常用操作/","link":"","permalink":"https://simon-ace.github.io/2020/09/17/Linux/vim 常用操作/","excerpt":"12# 取消高亮:noh","text":"12# 取消高亮:noh 取消搜索后高亮1234# no high light search:nohlsearch# 简写:noh 移动12w # 移至下一单词b # 移至上一单词 剪切、复制、粘贴、删除1234567dd # 删除当前行5dd # 删除5行yy # 复制当前行5yy # 复制5行p # 粘贴 设置 tab 键长度1:set tabstop=4 开启自动缩进12:set autoindentctrl+d # 停止自动缩进 字符串替换:s（substitute）命令用来查找和替换字符串。语法如下： 1:&#123;作用范围&#125;s/&#123;目标&#125;/&#123;替换&#125;/&#123;替换标志&#125; 例如:%s/foo/bar/g会在全局范围(%)查找foo并替换为bar，所有出现都会被替换（g）。 作用范围 当前行： 1:s/foo/bar/g 全文： 1:%s/foo/bar/g 2-11行： 1:5,12s/foo/bar/g 当前行.与接下来两行+2： 1:.,+2s/foo/bar/g 替换标志 上文中命令结尾的g即是替换标志之一，表示全局global替换（即替换目标的所有出现）。 还有很多其他有用的替换标志： 空替换标志表示只替换从光标位置开始，目标的第一次出现： 1:%s/foo/bar i表示大小写不敏感查找，I表示大小写敏感： 123:%s/foo/bar/i# 等效于模式中的\\c（不敏感）或\\C（敏感）:%s/foo\\c/bar c表示需要确认，例如全局查找&quot;foo&quot;替换为&quot;bar&quot;并且需要确认： 1:%s/foo/bar/gc 回车后Vim会将光标移动到每一次&quot;foo&quot;出现的位置，并提示 1replace with bar (y/n/a/q/l/^E/^Y)? 按下y表示替换，n表示不替换，a表示替换所有，q表示退出查找模式， l表示替换当前位置并退出。^E与^Y是光标移动快捷键","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"vim, 教程","slug":"vim-教程","permalink":"https://simon-ace.github.io/tags/vim-教程/"}]},{"title":"Linux压缩命令 tar","slug":"Linux压缩命令 tar 3","date":"2020-08-31T16:00:00.000Z","updated":"2021-01-12T07:18:38.035Z","comments":true,"path":"2020/09/01/Linux压缩命令 tar 3/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Linux压缩命令 tar 3/","excerpt":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx","text":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx 一、常用压缩参数必选参数，压缩解压都要用到其中一个： -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 可选参数： -z：有gzip属性的 -j： 有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 -C：解压时指定文件夹 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 二、举个栗子压缩 123456789101112131415# 将目录里所有jpg文件打包成tar.jpgtar -cvf jpg.tar *.jpg # 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gztar -czvf jpg.tar.gz *.jpg# 打包文件夹tar -czvf xxx.tar.gz /etc/folder# rar格式的压缩，需要先下载 rar for linuxrar a jpg.rar *.jpg # zip格式的压缩，需要先下载 zip for linuxzip jpg.zip *.jpg# zip 文件夹zip -r folder.zip folder 解压 12345678910111213141516# 解压 tar包tar -xvf file.tar # 指定文件夹tar -xvf file.tar -C /home/xxx# 解压tar.gztar -xzvf file.tar.gz # 解压 tar.bz2tar -xjvf file.tar.bz2 # 解压rarunrar e file.rar # 解压zipunzip file.zip 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压 1234tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Linux压缩命令 tar","slug":"Linux压缩命令 tar","date":"2020-08-31T16:00:00.000Z","updated":"2021-01-12T07:18:38.035Z","comments":true,"path":"2020/09/01/Linux压缩命令 tar/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Linux压缩命令 tar/","excerpt":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx","text":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx 一、常用压缩参数必选参数，压缩解压都要用到其中一个： -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 可选参数： -z：有gzip属性的 -j： 有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 -C：解压时指定文件夹 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 二、举个栗子压缩 123456789101112131415# 将目录里所有jpg文件打包成tar.jpgtar -cvf jpg.tar *.jpg # 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gztar -czvf jpg.tar.gz *.jpg# 打包文件夹tar -czvf xxx.tar.gz /etc/folder# rar格式的压缩，需要先下载 rar for linuxrar a jpg.rar *.jpg # zip格式的压缩，需要先下载 zip for linuxzip jpg.zip *.jpg# zip 文件夹zip -r folder.zip folder 解压 12345678910111213141516# 解压 tar包tar -xvf file.tar # 指定文件夹tar -xvf file.tar -C /home/xxx# 解压tar.gztar -xzvf file.tar.gz # 解压 tar.bz2tar -xjvf file.tar.bz2 # 解压rarunrar e file.rar # 解压zipunzip file.zip 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压 1234tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Linux压缩命令 tar","slug":"Linux压缩命令 tar 2","date":"2020-08-31T16:00:00.000Z","updated":"2021-01-12T07:18:38.035Z","comments":true,"path":"2020/09/01/Linux压缩命令 tar 2/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Linux压缩命令 tar 2/","excerpt":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx","text":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx 一、常用压缩参数必选参数，压缩解压都要用到其中一个： -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 可选参数： -z：有gzip属性的 -j： 有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 -C：解压时指定文件夹 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 二、举个栗子压缩 123456789101112131415# 将目录里所有jpg文件打包成tar.jpgtar -cvf jpg.tar *.jpg # 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gztar -czvf jpg.tar.gz *.jpg# 打包文件夹tar -czvf xxx.tar.gz /etc/folder# rar格式的压缩，需要先下载 rar for linuxrar a jpg.rar *.jpg # zip格式的压缩，需要先下载 zip for linuxzip jpg.zip *.jpg# zip 文件夹zip -r folder.zip folder 解压 12345678910111213141516# 解压 tar包tar -xvf file.tar # 指定文件夹tar -xvf file.tar -C /home/xxx# 解压tar.gztar -xzvf file.tar.gz # 解压 tar.bz2tar -xjvf file.tar.bz2 # 解压rarunrar e file.rar # 解压zipunzip file.zip 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压 1234tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"构建Hadoop的Docker编译环境","slug":"Hadoop/构建 Hadoop 的 Docker 编译环境","date":"2020-08-31T16:00:00.000Z","updated":"2020-09-02T02:47:19.693Z","comments":true,"path":"2020/09/01/Hadoop/构建 Hadoop 的 Docker 编译环境/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Hadoop/构建 Hadoop 的 Docker 编译环境/","excerpt":"","text":"一、配置 docker 环境 参考链接：Hadoop安装之一：使用Docker编译64位的Hadoop - 简书 1. 制作 CentOS 7 基础镜像（可选）Docker Hub上已经提供了CentOS7的官方镜像，但并未激活 Systemd（用来启动守护进程），制作一个启动 Systemd 的镜像。（这里编译Hadoop其实用不到systemd） Dockerfile 1234567891011121314151617181920212223242526# 镜像来源FROM centos:7# 镜像创建者MAINTAINER \"you\" &lt;your@email.here&gt;# 设置一个环境变量ENV container docker# 运行命令# 设置systemdRUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == \\systemd-tmpfiles-setup.service ] || rm -f $i; done); \\rm -f /lib/systemd/system/multi-user.target.wants/*;\\rm -f /etc/systemd/system/*.wants/*;\\rm -f /lib/systemd/system/local-fs.target.wants/*; \\rm -f /lib/systemd/system/sockets.target.wants/*udev*; \\rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \\rm -f /lib/systemd/system/basic.target.wants/*;\\rm -f /lib/systemd/system/anaconda.target.wants/*;# 挂载一个本地文件夹VOLUME [ \"/sys/fs/cgroup\" ]# 设置容器启动时的执行命令CMD [\"/usr/sbin/init\"] 生成镜像 1docker build -t centos7-systemd . 2. 安装 Oracle Java 参考链接使用yum卸载、安装jdk_不做小白的博客-CSDN博客 注意不要使用 openjdk，会导致编译 hive 时出现问题 启动刚刚生成的镜像 从官网下载 oracle java jdk-8u202-linux-x64.tar.gz 安装 Java，配置环境变量 12345678910111213141516171819mkdir /usr/local/javacp jdk-8u202-linux-x64.tar.gz /usr/local/javacd /usr/local/javatar -xzvf jdk-8u202-linux-x64.tar.gz# 配置环境变量vim /etc/profile~~export JAVA_HOME=/usr/local/java/jdk1.8.0_202export JRE_HOME=/usr/local/java/jdk1.8.0_202/jre export PATH=$PATH:/usr/local/java/jdk1.8.0_202/bin export CLASSPATH=./:/usr/local/java/jdk1.8.0_202/lib:/usr/local/java/jdk1.8.0_202/jre/lib~~source /etc/profile# 检查 JAVA 是否安装成功java -version 保存镜像 1docker commit 容器id 镜像名 3. 制作编译镜像 编译脚本 1234567891011121314151617181920$ vi compile.sh#!/bin/bash# 设置默认编译版本(支持传参)version=$&#123;1:-2.7.3&#125;# 进入源代码目录cd /hadoop-$version-src# 开始编译echo -e \"\\n\\ncompile hadoop $version...\"mvn clean package -Pdist,native -DskipTests -Dtar# 输出结果if [[ $? -eq 0]]; then echo -e \"\\n\\ncompile hadoop $version success!\\n\\n\"else echo -e \"\\n\\ncompile hadoop $version fail!\\n\\n\"fi Dockerfile（其中有不少安装包不是必要的） 123456789101112131415161718192021222324252627# 镜像来源(第二步生成的本地镜像)FROM centos7-systemd-java# 镜像创建者MAINTAINER \"you\" &lt;your@email.here&gt;# 运行命令安装环境依赖# 使用 -y 同意全部询问RUN yum update -y &amp;&amp; \\ yum groupinstall -y \"Development Tools\" &amp;&amp; \\ yum install -y wget \\ protobuf-devel \\ protobuf-compiler \\ maven \\ cmake \\ pkgconfig \\ openssl-devel \\ zlib-devel \\ gcc \\ automake \\ autoconf \\ make # 复制编辑脚本文件到镜像中COPY compile.sh /root/compile.sh# 设置脚本文件的可运行权限RUN chmod +x /root/compile.sh 生成镜像 1sudo docker build -t centos7-hadoop-compiler . 二、编译源码 hive（大概10分钟） 1mvn clean package -Pdist -DskipTests hadoop（大概15分钟） 1mvn clean package -Pdist,native -DskipTests -Dtar 也可以使用 docker image 中的脚本编译 12$ export VERSION=2.7.3$ sudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src --privileged=true centos7-hadoop-complier /root/compile.sh $VERSION 要添加 privileged 参数！ [docker]privileged参数_追寻神迹-CSDN博客 使用该参数，container内的root拥有真正的root权限。否则，container内的root只是外部的一个普通用户权限。 总结 1234567891011121314151617181920212223242526272829# pull docker imagedocker pull shuofxz/hadoop-compiler:1.0# === HADOOP ===# hadoop download link# new versionhttps://hadoop.apache.org/releases.html# old versionhttps://archive.apache.org/dist/hadoop/common/# compile command (about 15 minutes to complete)mvn package -Pdist,native -DskipTests -Dtar# compile with script file$ export VERSION=2.7.3$ sudo docker run -v $(pwd)/hadoop-$VERSION-src:/hadoop-$VERSION-src --privileged=true shuofxz/hadoop-compiler:1.0 /root/hadoop-compile.sh $VERSION# === HIVE ===# hive download link# select corresponding branch src file to downloadhttps://github.com/apache/hive# compile command (about 10 minutes to complete)mvn clean package -Pdist -DskipTests# compile with script file$ export VERSION=2.3.0$ sudo docker run -v $(pwd)/hive-rel-release-$VERSION:/hive-rel-release-$VERSION --privileged=true shuofxz/hadoop-compiler:1.0 /root/hive-compile.sh $VERSION hadoop-2.7.0-src.tar.gz release-2.3.0.tar.gz 已包括各种库的 image，可以直接编译 hadoop（不好用） GitHub - kiwenlau/compile-hadoop: Compile Hadoop in Docker containerhttps://github.com/kiwenlau/compile-hadoop","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"docker,教程","slug":"docker-教程","permalink":"https://simon-ace.github.io/tags/docker-教程/"}]},{"title":"Linux压缩命令 tar","slug":"Linux/常用命令/Linux","date":"2020-08-31T16:00:00.000Z","updated":"2021-01-12T07:18:38.035Z","comments":true,"path":"2020/09/01/Linux/常用命令/Linux/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Linux/常用命令/Linux/","excerpt":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx","text":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx 一、常用压缩参数必选参数，压缩解压都要用到其中一个： -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 可选参数： -z：有gzip属性的 -j： 有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 -C：解压时指定文件夹 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 二、举个栗子压缩 123456789101112131415# 将目录里所有jpg文件打包成tar.jpgtar -cvf jpg.tar *.jpg # 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gztar -czvf jpg.tar.gz *.jpg# 打包文件夹tar -czvf xxx.tar.gz /etc/folder# rar格式的压缩，需要先下载 rar for linuxrar a jpg.rar *.jpg # zip格式的压缩，需要先下载 zip for linuxzip jpg.zip *.jpg# zip 文件夹zip -r folder.zip folder 解压 12345678910111213141516# 解压 tar包tar -xvf file.tar # 指定文件夹tar -xvf file.tar -C /home/xxx# 解压tar.gztar -xzvf file.tar.gz # 解压 tar.bz2tar -xjvf file.tar.bz2 # 解压rarunrar e file.rar # 解压zipunzip file.zip 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压 1234tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Linux压缩命令 tar","slug":"Linux/常用命令/Linux压缩命令 tar","date":"2020-08-31T16:00:00.000Z","updated":"2021-01-12T07:18:38.035Z","comments":true,"path":"2020/09/01/Linux/常用命令/Linux压缩命令 tar/","link":"","permalink":"https://simon-ace.github.io/2020/09/01/Linux/常用命令/Linux压缩命令 tar/","excerpt":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx","text":"123456# 压缩tar -czvf xxx.tar.gz /etc/folder# 解压（1.15版本后 tar 自动识别压缩方式）tar -xvf filename.tar.gztar -xvf filename.tar.gz -C /home/xxx 一、常用压缩参数必选参数，压缩解压都要用到其中一个： -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 可选参数： -z：有gzip属性的 -j： 有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 -C：解压时指定文件夹 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 二、举个栗子压缩 123456789101112131415# 将目录里所有jpg文件打包成tar.jpgtar -cvf jpg.tar *.jpg # 将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gztar -czvf jpg.tar.gz *.jpg# 打包文件夹tar -czvf xxx.tar.gz /etc/folder# rar格式的压缩，需要先下载 rar for linuxrar a jpg.rar *.jpg # zip格式的压缩，需要先下载 zip for linuxzip jpg.zip *.jpg# zip 文件夹zip -r folder.zip folder 解压 12345678910111213141516# 解压 tar包tar -xvf file.tar # 指定文件夹tar -xvf file.tar -C /home/xxx# 解压tar.gztar -xzvf file.tar.gz # 解压 tar.bz2tar -xjvf file.tar.bz2 # 解压rarunrar e file.rar # 解压zipunzip file.zip 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压 1234tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"Linux, 压缩","slug":"Linux-压缩","permalink":"https://simon-ace.github.io/tags/Linux-压缩/"}]},{"title":"Docker 批量操作","slug":"Docker/Docker 批量操作","date":"2020-08-27T16:00:00.000Z","updated":"2020-09-01T10:04:59.753Z","comments":true,"path":"2020/08/28/Docker/Docker 批量操作/","link":"","permalink":"https://simon-ace.github.io/2020/08/28/Docker/Docker 批量操作/","excerpt":"1docker rmi $(docker images | grep \"none\" | awk '&#123;print $3&#125;')","text":"1docker rmi $(docker images | grep \"none\" | awk '&#123;print $3&#125;') 列出所有的容器 ID 1docker ps -aq 停止所有的容器 1docker stop $(docker ps -aq) 删除所有的容器 1docker rm $(docker ps -aq) 删除所有的镜像 1docker rmi $(docker images -q) 删除指定名称镜像 1docker rmi $(docker images | grep \"none\" | awk '&#123;print $3&#125;') 复制文件 12docker cp mycontainer:/opt/file.txt /opt/local/docker cp /opt/local/file.txt mycontainer:/opt/ 现在的docker有了专门清理资源(container、image、网络)的命令。 docker 1.13 中增加了 docker system prune的命令，针对container、image可以使用docker container prune、docker image prune命令。 删除所有不使用的镜像 123docker image prune --force --all# ordocker image prune -f -a 删除所有停止的容器 1docker container prune -f","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://simon-ace.github.io/tags/docker/"}]},{"title":"Cron语法","slug":"Linux/cron语法","date":"2020-08-16T16:00:00.000Z","updated":"2020-11-26T04:22:48.797Z","comments":true,"path":"2020/08/17/Linux/cron语法/","link":"","permalink":"https://simon-ace.github.io/2020/08/17/Linux/cron语法/","excerpt":"1234567* * * * * * *秒 分钟 小时 天 月 星期 年# -------------------# linux crontab 只有5个 * * * * * 分钟 小时 天 月 星期","text":"1234567* * * * * * *秒 分钟 小时 天 月 星期 年# -------------------# linux crontab 只有5个 * * * * * 分钟 小时 天 月 星期 1 表达式详解一个cron表达式有至少6个（也可能7个）有空格分隔的时间元素。 按顺序依次为 1 秒（0~59） 2 分钟（0~59） 3 小时（0~23） 4 天（0~31） 5 月（0~11） 6 星期（1~7 1=SUN 或 SUN，MON，TUE，WED，THU，FRI，SAT） 7 年份（1970－2099） 每个元素格式： 一个具体值（如6） 一个连续区间（9-12） 一个列表(1,3,5) 特殊字符 通配符（*），所有可能的值 空符号（？），表示不指定值 由于”月份中的日期”和”星期中的日期”这两个元素互斥的,必须要对其中一个设置? 增量符（/） 如第二位12/10 表示从第12分钟开始，每10分钟（它和“12，22，32…”） 最后（L） 仅被用于天（月）和天（星期）两个子表达式，它是单词“last”的缩写 “6L”表示这个月的倒数第６天 平日（W） 仅能用于日域中，它用来指定离指定日的最近的一个工作日（1-5） 日域中的 15W 意味着 “离该月15号的最近一个平日 2 例子123456789101112130 0 10,14,16 * * ? 每天上午10点，下午2点，4点0 0/30 9-17 * * ? 朝九晚五工作时间内每半小时0 0 12 ? * WED 表示每个星期三中午12点0 0 12 * * ? 每天中午12点触发0 15 10 ? * * 每天上午10:15触发0 15 10 * * ? 2005 2005年的每天上午10:15触发0 * 14 * * ? 在每天下午2点到下午2:59期间的每1分钟触发0 0/5 14,18 * * ? 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发0 0-5 14 * * ? 在每天下午2点到下午2:05期间的每1分钟触发0 10,44 14 ? 3 WED 每年三月的星期三的下午2:10和2:44触发0 15 10 ? * MON-FRI 周一至周五的上午10:15触发0 15 10 L * ? 每月最后一日的上午10:15触发0 15 10 ? * 6L 每月的最后一个星期五上午10:15触发","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"cron","slug":"cron","permalink":"https://simon-ace.github.io/tags/cron/"}]},{"title":"Hexo常用命令","slug":"配置/Hexo常用命令","date":"2020-08-12T16:00:00.000Z","updated":"2020-08-13T10:18:27.861Z","comments":true,"path":"2020/08/13/配置/Hexo常用命令/","link":"","permalink":"https://simon-ace.github.io/2020/08/13/配置/Hexo常用命令/","excerpt":"123456# 生成静态文件hexo g# 启动服务hexo s# 部署hexo d","text":"123456# 生成静态文件hexo g# 启动服务hexo s# 部署hexo d 常用命令 生成静态文件 123hexo generate# 简写hexo g 启动服务预览文章 12345hexo server# 简写hexo s# 指定端口hexo server -p 5000 一键部署 123hexo deploy# 简写hexo d","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"","slug":"Docker/Docker部署Vue项目","date":"2020-08-12T07:02:40.098Z","updated":"2020-08-12T07:02:43.859Z","comments":true,"path":"2020/08/12/Docker/Docker部署Vue项目/","link":"","permalink":"https://simon-ace.github.io/2020/08/12/Docker/Docker部署Vue项目/","excerpt":"","text":"[手把手系列之]Docker 部署 vue 项目 - 掘金https://juejin.im/post/6844903837774397447","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-07-10T03:23:10.782Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2020/07/10/hello-world/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Mac安装node","slug":"配置/Mac安装node","date":"2020-07-09T16:00:00.000Z","updated":"2020-07-10T06:24:18.048Z","comments":true,"path":"2020/07/10/配置/Mac安装node/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/配置/Mac安装node/","excerpt":"Mac安装及降级node版本","text":"Mac安装及降级node版本 1 安装最新版Node 安装HomeBrew 1/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 安装Node 1brew install node 验证Node是否安装成功 输入下面两条指令看是否可以都输出版本号 12node -vnpm -v 2 降级Node由于开发需要或版本兼容性，需要安装低版本的Node，按下面的方式操作 卸载Node 如果你是按前面的方法安装的Node，则用下面的命令卸载 1brew uninstall node 查看可用的Node版本 1brew search node 输出结果： 12==&gt; Formulaelibbitcoin-node node node-sass node@12 nodebrew nodenv llnode node-build node@10 node_exporter nodeenv 安装你需要的版本 12# 这里安装v12版本brew install node@12 连接Node 12brew link node@12# 这一步可能会报错, 按照提示执行命令就ok了, 比如我最后执行的是brew link --overwrite --force node@12 检查Node是否安装成功 1node -v","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"Mac使用代理ssh远程连接服务器 & keep alive","slug":"配置/Mac使用代理ssh远程连接服务器 & keep alive","date":"2020-07-09T16:00:00.000Z","updated":"2020-11-25T09:52:52.298Z","comments":true,"path":"2020/07/10/配置/Mac使用代理ssh远程连接服务器 & keep alive/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/配置/Mac使用代理ssh远程连接服务器 & keep alive/","excerpt":"123456# 直接连接ssh -p 端口号 服务器用户名@ip地址# eg: ssh -p 22 userkunyu@119.29.37.63# 通过代理连接ssh -o ProxyCommand=\"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p\" 需要访问的服务器的用户名@需要访问的服务器ip","text":"123456# 直接连接ssh -p 端口号 服务器用户名@ip地址# eg: ssh -p 22 userkunyu@119.29.37.63# 通过代理连接ssh -o ProxyCommand=\"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p\" 需要访问的服务器的用户名@需要访问的服务器ip 1 直接连接12# ssh -p 端口号 服务器用户名@ip地址ssh -p 22 userkunyu@119.29.37.63 2 通过代理连接 直接连接 12# ssh -o ProxyCommand=\"nc -X 5 -x 代理服务器ip:代理服务器端口 %h %p\" 需要访问的服务器的用户名@需要访问的服务器ipssh -o ProxyCommand=\"nc -X 5 -x 192.168.0.255:9999 %h %p\" user_name@192.168.77.200 使用SSH配置文件 1sudo vi ~/.ssh/config 12Host * ProxyCommand nc -X 5 -x 192.168.0.255:9999 %h %p 配置好了之后就可以和直接连接一样使用 1ssh uesr@ip Mac下SSH跳点连接及代理连接_Dawnworld-CSDN博客_mac ssh 代理https://blog.csdn.net/thundon/article/details/46858957 3 Keep alive http://bluebiu.com/blog/iterm2-ssh-session-idle.html 方案一： 在本机 vim ~/.ssh/config 123# 在开头添加Host * ServerAliveInterval 60 我觉得60秒就好了，而且基本去连的机器都保持，所以配置了*，如果有需要针对某个机器，可以自行配置为需要的serverHostName。 方案二： 单次连接 添加下面的参数 1ssh -o ServerAliveInterval=30 user@host","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"iTerm2配置","slug":"配置/iTerm2配置","date":"2020-07-09T16:00:00.000Z","updated":"2020-07-10T10:23:28.607Z","comments":true,"path":"2020/07/10/配置/iTerm2配置/","link":"","permalink":"https://simon-ace.github.io/2020/07/10/配置/iTerm2配置/","excerpt":"iTerm2配置 Oh-my-zsh安装，主题配置","text":"iTerm2配置 Oh-my-zsh安装，主题配置 1 安装iTerm2iTerm2 是一款完全免费的，专为 Mac OS 用户打造的命令行应用。直接在官网上 http://iterm2.com/ 下载并安装即可。 设置为默认终端 2 安装 oh-my-zshbash是mac中terminal自带的shell，把它换成oh-my-zsh，这个的功能要多得多。拥有语法高亮，命令行tab补全，自动提示符，显示Git仓库状态等功能。 1sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 解决权限问题 如果安装完重启iterm之后，出现下面的提示： 12345[oh-my-zsh] Insecure completion-dependent directories detected:drwxrwxrwx 7 hans admin 238 2 9 10:13 /usr/local/share/zshdrwxrwxrwx 6 hans admin 204 10 1 2017 /usr/local/share/zsh/site-functions...... 解决方法： 12chmod 755 /usr/local/share/zshchmod 755 /usr/local/share/zsh/site-functions 3 配置主题4 Vim配置设置鼠标滚动","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/tags/教程/"}]},{"title":"Linux后台执行命令 nohup","slug":"Linux/常用命令/Linux后台执行命令 nohup","date":"2019-10-21T16:00:00.000Z","updated":"2020-11-27T06:48:34.581Z","comments":true,"path":"2019/10/22/Linux/常用命令/Linux后台执行命令 nohup/","link":"","permalink":"https://simon-ace.github.io/2019/10/22/Linux/常用命令/Linux后台执行命令 nohup/","excerpt":"当在终端工作时，可能一个持续运行的作业占住屏幕输出，或终端退出时导致命令结束。为了避免这些问题，可以将这些进程放到后台运行，且不受终端关闭的影响，可使用下面的方法： 1nohup command &gt; myout.file 2&gt;&amp;1 &amp;","text":"当在终端工作时，可能一个持续运行的作业占住屏幕输出，或终端退出时导致命令结束。为了避免这些问题，可以将这些进程放到后台运行，且不受终端关闭的影响，可使用下面的方法： 1nohup command &gt; myout.file 2&gt;&amp;1 &amp; 1 后台执行命令1.1 命令&amp;在命令后面加上&amp;实现后台运行（控制台关掉(退出帐户时)，作业就会停止运行） 1command &amp; 例：python run.py &amp; 1.2 命令nohupnohup命令可以在你退出帐户之后继续运行相应的进程。nohup就是不挂起的意思( no hang up) 1nohup command &amp; 例：nohup run.py &amp; 2 kill进程执行后台任务命令后，会返回一个进程号，可通过这个进程号kill掉进程。 1kill -9 进程号 3 输出重定向由于使用前面的命令将任务放到后台运行，因此任务的输出也不打印到屏幕上了，所以需要将输出重定向到文件中，以方便查看输出内容。 将输出重定向到 file（覆盖） 1command1 &gt; file1 将输出重定向到 file（追加） 1command1 &gt;&gt; file1 将 stdout 和 stderr 合并后重定向到 file 2&gt;1代表什么，2与&gt;结合代表错误重定向，而1则代表错误重定向到一个文件1，而不代表标准输出；换成2&gt;&amp;1，&amp;与1结合就代表标准输出了，就变成错误重定向到标准输出. 1command1 &gt; file1 2&gt;&amp;1 完整写法： 1nohup command &gt;out.file 2&gt;&amp;1 &amp; 4 其他 nohup执行python程序时，print无法输出 这是因为python的输出有缓冲，导致nohup.out并不能够马上看到输出 python 有个-u参数，使得python不启用缓冲 nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Linux统计文件数目 wc","slug":"Linux/常用命令/Linux统计文件夹下的文件数目 wc","date":"2019-10-21T16:00:00.000Z","updated":"2020-11-27T06:49:00.812Z","comments":true,"path":"2019/10/22/Linux/常用命令/Linux统计文件夹下的文件数目 wc/","link":"","permalink":"https://simon-ace.github.io/2019/10/22/Linux/常用命令/Linux统计文件夹下的文件数目 wc/","excerpt":"1$ ls -l | grep &quot;^-&quot; | wc -l","text":"1$ ls -l | grep &quot;^-&quot; | wc -l 1 统计文件夹下的文件数目 统计当前目录下文件的个数（不包括目录） 1$ ls -l | grep &quot;^-&quot; | wc -l 统计当前目录下文件的个数（包括子目录） 1$ ls -lR| grep &quot;^-&quot; | wc -l 查看某目录下文件夹(目录)的个数（包括子目录） 1$ ls -lR | grep &quot;^d&quot; | wc -l 命令原理： ls -l 详细输出该文件夹下文件信息 ls -lR是列出所有文件，包括子目录 grep &quot;^-&quot; 过滤ls的输出信息，只保留一般文件；只保留目录是grep &quot;^d&quot; wc -l 统计输出信息的行数","categories":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://simon-ace.github.io/tags/Linux/"}]},{"title":"Python打印更详细的异常信息","slug":"Python/Python打印更详细的异常信息","date":"2019-10-20T16:00:00.000Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2019/10/21/Python/Python打印更详细的异常信息/","link":"","permalink":"https://simon-ace.github.io/2019/10/21/Python/Python打印更详细的异常信息/","excerpt":"打印Python异常信息的几种方式","text":"打印Python异常信息的几种方式 1 简单的异常信息1234try: a = 1/0except Exception as e: print(e) 打印最简单的message信息： 1division by zero 2 更完整的信息12345678910import tracebacktry: a = 1/0except Exception as e: print('str(e):\\t', e) print('repr(e):\\t', repr(e)) print('traceback.format_exc():\\n%s' % traceback.format_exc()) #字符串 traceback.print_exc() #执行函数 输出： 123456789101112str(e): division by zerorepr(e): ZeroDivisionError(&apos;division by zero&apos;)traceback.format_exc():Traceback (most recent call last): File &quot;/Users/ace/Play/test/异常信息.py&quot;, line 4, in &lt;module&gt; a = 1/0ZeroDivisionError: division by zeroTraceback (most recent call last): File &quot;/Users/ace/Play/test/异常信息.py&quot;, line 4, in &lt;module&gt; a = 1/0ZeroDivisionError: division by zero traceback.format_exc()和traceback.print_exc()都可以打印完整的错误信息 traceback.format_exc()返回值为字符串 traceback.print_exc()是一个执行函数，直接在控制台打印错误信息","categories":[{"name":"Python","slug":"Python","permalink":"https://simon-ace.github.io/categories/Python/"}],"tags":[{"name":"Python, 异常","slug":"Python-异常","permalink":"https://simon-ace.github.io/tags/Python-异常/"}]},{"title":"【转】持续集成 Continuous Integration","slug":"配置/持续集成 Continuous Integration","date":"2019-10-17T16:00:00.000Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2019/10/18/配置/持续集成 Continuous Integration/","link":"","permalink":"https://simon-ace.github.io/2019/10/18/配置/持续集成 Continuous Integration/","excerpt":"持续集成 Continuous Integration","text":"持续集成 Continuous Integration","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"Python-加快pip安装速度","slug":"Python/Python-加快pip安装速度","date":"2019-10-15T16:00:00.000Z","updated":"2020-07-10T03:23:10.782Z","comments":true,"path":"2019/10/16/Python/Python-加快pip安装速度/","link":"","permalink":"https://simon-ace.github.io/2019/10/16/Python/Python-加快pip安装速度/","excerpt":"PIP安装时使用国内镜像，加快下载速度","text":"PIP安装时使用国内镜像，加快下载速度 0 国内源清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/ 1 临时使用 可以在使用pip的时候加参数-i https://pypi.tuna.tsinghua.edu.cn/simple 例如： pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy 2 永久修改这样就不用每次都添加国内镜像源地址了 Linux下，修改~/.pip/pip.conf（没有就创建一个文件夹及文件） 打开文件，添加内容： 1234[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.com windows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini， 内容同上","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"Hexo多台电脑同步","slug":"配置/Hexo多台电脑同步","date":"2019-10-15T16:00:00.000Z","updated":"2020-08-13T09:06:57.490Z","comments":true,"path":"2019/10/16/配置/Hexo多台电脑同步/","link":"","permalink":"https://simon-ace.github.io/2019/10/16/配置/Hexo多台电脑同步/","excerpt":"如果换了电脑该如何同步Hexo的源文件？把hexo文件从一个电脑cope到另外一个电脑吗？答案肯定不是这样的，因为这里面有好多依赖包，好几万个文件呢，这样显然不合理。 本文提供一种多台电脑同步源文件的方法。","text":"如果换了电脑该如何同步Hexo的源文件？把hexo文件从一个电脑cope到另外一个电脑吗？答案肯定不是这样的，因为这里面有好多依赖包，好几万个文件呢，这样显然不合理。 本文提供一种多台电脑同步源文件的方法。 0 解决思路使用GitHub的分支！在博客对应的仓库中新建一个分支。一个分支用来存放Hexo生成的网站原始的文件，另一个分支用来存放生成的静态网页。 1 创建分支1.1 创建新分支命令行操作： GitHub操作： 点击branch按钮，输入新的分支名source，点创建。 1.2 设置默认分支准备在source分支中存放源文件，master中存放生成的网页，因此将source设置为默认分支，方便同步文件。 在仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为source，save保存 2 源文件上传到GitHub 选好一个本地文件夹，执行 git clone git@github.com:Simon-Ace/Simon-Ace.github.io.git(替换成你的仓库) 在克隆到本地的Simon-Ace.github.io中，把除了.git 文件夹外的所有文件都删掉 把之前我们写的博客源文件全部复制过来，除了.deploy_git 复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的.git文件夹删掉，因为git不能嵌套上传。 提交更改 123git add .git commit –m \"add branch\"git push 参考文章： https://juejin.im/post/5acf22e6f265da23994eeac9 https://www.zhihu.com/question/21193762","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://simon-ace.github.io/tags/Hexo/"}]},{"title":"一台电脑配置多个git账号","slug":"Linux/配置多个git账号","date":"2019-10-13T16:00:00.000Z","updated":"2020-11-22T03:56:40.106Z","comments":true,"path":"2019/10/14/Linux/配置多个git账号/","link":"","permalink":"https://simon-ace.github.io/2019/10/14/Linux/配置多个git账号/","excerpt":"1 清除git全局设置如果配置第一个账号的时候使用git config --global设置过，就先要取消掉，否则两个账号肯定会冲突 123# 取消globalgit config --global --unset user.namegit config --global --unset user.email","text":"1 清除git全局设置如果配置第一个账号的时候使用git config --global设置过，就先要取消掉，否则两个账号肯定会冲突 123# 取消globalgit config --global --unset user.namegit config --global --unset user.email 2 生成新账号的SSH keys2.1 用 ssh-keygen 命令生成密钥1$ ssh-keygen -t rsa -C \"new email\" 平时都是直接回车，默认生成 id_rsa 和 id_rsa.pub。这里特别需要注意，出现提示输入文件名的时候(Enter file in which to save the key (~/.ssh/id_rsa): id_rsa_new)要输入与默认配置不一样的文件名，比如：我这里填的是 id_rsa和id_rsa_me。 如果之前没配置过ssh key，这里用不同邮箱生成两遍即可，注意用不同的文件名 成功后会出现： 12Your identification has been saved in xxx.Your public key has been saved in xxx. 2.2 添加到ssh-agent中使用ssh-add将 IdentityFile 添加到 ssh-agent中 12ssh-add ~/.ssh/id_rsassh-add ~/.ssh/id_rsa_me 2.3 配置 ~/.ssh/config 文件在~/.ssh/下新建config文件 1234567891011# The git info for companyHost git.XXX.com # git别名，写公司的git名字即可HostName git.XXX.com # git名字，同样写公司的git名字User git # 写 git 即可IdentityFile ~/.ssh/id_rsa #私钥路径，若写错会连接失败# The git info for github Host github.com # git别名，写github的git名字即可HostName github.com # git名字，同样写github的git名字User git # 写 git 即可IdentityFile ~/.ssh/id_rsa_me #私钥路径，若写错会连接失败 3 与GitHub链接复制刚刚生成的两个ssh公钥到对应的账号中 文件id_rsa.pub中保存的就是 ssh 公钥 12pbcopy &lt; ~/.ssh/id_rsa.pubpbcopy &lt; ~/.ssh/id_rsa_me.pub 在 github 网站中添加该 ssh 公钥 验证是否配置成功，以 github 为例，输入 ssh -T git@github.com，若出现 1Hi xxx! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 这样的字段，即说明配置成功。另一个同理。 参考链接： 配置多个git账号的ssh密钥 - 掘金https://juejin.im/post/5befe84d51882557795cc8f9 同一台电脑配置多个git账号 · Issue #2 · jawil/noteshttps://github.com/jawil/notes/issues/2","categories":[{"name":"教程","slug":"教程","permalink":"https://simon-ace.github.io/categories/教程/"}],"tags":[{"name":"git","slug":"git","permalink":"https://simon-ace.github.io/tags/git/"}]},{"title":"模板项目","slug":"模板项目","date":"1999-12-31T16:00:00.000Z","updated":"2021-02-23T08:07:27.858Z","comments":true,"path":"2000/01/01/模板项目/","link":"","permalink":"https://simon-ace.github.io/2000/01/01/模板项目/","excerpt":"模板～","text":"模板～ 正文","categories":[{"name":"技术","slug":"技术","permalink":"https://simon-ace.github.io/categories/技术/"}],"tags":[{"name":"模板","slug":"模板","permalink":"https://simon-ace.github.io/tags/模板/"}]}]}